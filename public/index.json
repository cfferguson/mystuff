[{"authors":["admin"],"categories":null,"content":"I hold a Ph.D. in Program Evaluation and am an Assistant Professor of Educational Psychology within the Department of Counseling and Learning Sciences in the College of Education and Human Services at West Virginia University. I currently perform research on the teaching of developmental evaluation, modeling of social science concepts using machine learning, and in the construction of predictive social networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0ebaba4b5b8aaba6189e8c7aaa6409a6","permalink":"/author/dr.-abhik-roy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-abhik-roy/","section":"authors","summary":"I hold a Ph.D. in Program Evaluation and am an Assistant Professor of Educational Psychology within the Department of Counseling and Learning Sciences in the College of Education and Human Services at West Virginia University.","tags":null,"title":"Dr. Abhik Roy","type":"authors"},{"authors":["admin"],"categories":null,"content":"Abhik Roy holds a Ph.D. in Program Evaluation and is an Assistant Professor of Educational Psychology within the Department of Learning Sciences and Human Development in the College of Education and Human Services at West Virginia University. He currently performs research on the teaching of developmental evaluation, modeling of social science concepts using machine learning, and in the construction of predictive social networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/dr.-abhik-roy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-abhik-roy/","section":"authors","summary":"Abhik Roy holds a Ph.D. in Program Evaluation and is an Assistant Professor of Educational Psychology within the Department of Learning Sciences and Human Development in the College of Education and Human Services at West Virginia University.","tags":null,"title":"Dr. Abhik Roy","type":"authors"},{"authors":null,"categories":null,"content":"  Here are multiple resources and guides related to data, R and other relevant topics. None of these are required but could be helpful in or beyond the course!\n","date":1594771200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1594771200,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"Here are multiple resources and guides related to data, R and other relevant topics. None of these are required but could be helpful in or beyond the course!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"   Lessons via Data Camp Homeworks Data Tasks Quizzes and Exams A Note About Discussion Board Postings   You will get the most of out this class if you:\nengage with the readings and materials, use R to complete any data wrangling or analysis, ask for help immediately after giving it (whatever it is) a good faith effort, and keep an open and honest line of communication with me and your peers if possible.  Each type of assignment in this class helps with at least one one of these criteria.\nLessons via Data Camp Most class sessions have interactive lessons via Data Camp. Your task is simply to go through these.\nI will grade these short exercises using a check system:\n ✔+: (11.5 points (115%) in gradebook) Modules are 100% completed. Every task was attempted and answered, and most answers are correct. These are not earned often. ✔: (10 points (100%) in gradebook) Modules are 70–99% complete and most answers are correct or on point. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Modules are less than 70% complete and/or most answers are incorrect or off-point. This indicates that you need to improve next time. Hopefully people will not earn this often.  Otherwise 0 points (0%) in gradebook.\n Homeworks On most weeks, you will be asked to submit a set of problems assigned after the prior week’s class. These must be submitted as a single document in the correct numerical order and submitted on the Submission Portal within ecampus as a single cohesive document (in either in docx or pdf format). If you learn how to type your homework in Rmarkdown, you will earn an extra 5% on each submission where this is done.\n Data Tasks Some classes also have fully annotated examples of performing tasks in R code that teach and demonstrate how to do specific concepots we cover here or elsewhere and on Data Camp.\nI will grade these short exercises using a check system:\n ✔+: (11.5 points (115%) in gradebook) Exercises are 100% completed. Every task was attempted and answered, and most answers are correct. Knitted document are clean and easy to follow. This indicates that your work is exceptional. These are not earned often. ✔: (10 points (100%) in gradebook) Exercises are 70–99% complete and most answers are correct or on point. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect or off-point. This indicates that you need to improve next time. Hopefully people will not earn this often.  Otherwise 0 points (0%) in gradebook.\nAgain, I am not grading your coding ability, checking each line of code or syntax to make sure it produces the exact output, and most importantly, I am not looking for perfection! Try hard, get frustrated, do good work and you’ll get a ✓ or even a ✔+.\nI encourage you to work together on the courses if possible, but you must turn in your own work on eCampus.\n Quizzes and Exams Exact dates will be posted here soon but you will have a announced quizzes, three exams and a final. Please note if you have an A going into the final exam, you do not need to take it!\n A Note About Discussion Board Postings While you may like or even love them, I hate discussion board posts. I hated them as a graduate student and still hate them as a professor. I recently made the decision to never use discussion board posts again for many reasons which are not useful to discuss here.\nI always prefer face to face or at least conferencing if needed. With that said, we have Zoom and a Slack channel so please use both!\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Lessons via Data Camp Homeworks Data Tasks Quizzes and Exams A Note About Discussion Board Postings   You will get the most of out this class if you:","tags":null,"title":"About the Assignments","type":"docs"},{"authors":null,"categories":null,"content":"  This section contains extra material to help you understand a concepts covered for that week. Sometimes this is just a few links for you to look at and other times its fully annotated R code that you can use as a reference for helping you along with a task. Please access the material on this site this section after you have finished the reading(s).\nFrom time to time, I may also contains video of coding some examples pieces so you can see what it looks like to work with R in real time. You’ll definitely notice that I make all sorts of errors, which is absolutely normal and happens to everyone!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains extra material to help you understand a concepts covered for that week. Sometimes this is just a few links for you to look at and other times its fully annotated R code that you can use as a reference for helping you along with a task.","tags":null,"title":"About the Examples","type":"docs"},{"authors":null,"categories":null,"content":"  Each class session has an interactive lesson that you will work through after doing the readings and watching a lecture (if applicable). These lessons are a central part of the class—they will teach you how to use R and other packages eventually leading to the tidyverse family.\nInteractive training sections are provided on Data Camp.\nHere is some advice. Carve out some time everyday to go through these. If you try to complete everything in one sitting, it will probably be overwhelming! However if you have familiarity with some modules, please feel free to work ahead.\nPlease note that if you have (1) used Data Camp before and (2) are logged in with the same username, then any module that was successfully completed will not have to be done again.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"45e63e789e3cb381d68e4ca47e0a453c","permalink":"/lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/","section":"lesson","summary":"Each class session has an interactive lesson that you will work through after doing the readings and watching a lecture (if applicable). These lessons are a central part of the class—they will teach you how to use R and other packages eventually leading to the tidyverse family.","tags":null,"title":"About the Lessons","type":"docs"},{"authors":null,"categories":null,"content":"  The Course Text Each class session has a set of required readings that you should complete before doing anything else. It also allows you to ask questions prior to moving on! With that said, think of the text as a reference, in that\n it contains a lot of stuff that you may use but probably won’t right now, and it gives you a basis to ask intelligent questions.  So you should absolutely read it to get a basis but in the long run, you will learn more from this course by doing.\n The Course Nontext Many of us are visual learners with me included, so a lot of what I provide outside of the text is visually based. Resources such as presentations, videos, visualizations, walkthroughs, etc. take a lot of time to construct so please take some time to go through them. Are they Hollywood? No because I’m a professor at a public university, have three children and drive a car with duct tape on it BUT they’ll be decent. I am always open to suggestions so if you have any, send them along! Pictures are better than words because some words are big and hard to understand. - Peter Griffin\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0aa019bdc1e0c98e24563159b8fa2f91","permalink":"/readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/","section":"readings","summary":"The Course Text Each class session has a set of required readings that you should complete before doing anything else. It also allows you to ask questions prior to moving on!","tags":null,"title":"About the Readings","type":"docs"},{"authors":null,"categories":null,"content":"  This section will let you know what must be completed for the purposes of assessment1 or preparation2. You should still do everything else because those items are still expected and you will be scored on those skills eventually on some other course related task.\n What you have to do after a current class ends.↩︎\n What you have to do before the next class begins.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"140e53d0243061ae64e96f1d35188d6a","permalink":"/due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/","section":"due","summary":"This section will let you know what must be completed for the purposes of assessment1 or preparation2. You should still do everything else because those items are still expected and you will be scored on those skills eventually on some other course related task.","tags":null,"title":"So What's Due?","type":"docs"},{"authors":null,"categories":null,"content":"  You can download a BibTeX file of all the readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows/macOS, or Zotero or Mendeley online.\nIf you are comfortable with the terminal in a Mac, then the bib2ris may be easy to implement.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"68be32a8da6a38dd54a9e724ab3904a0","permalink":"/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows/macOS, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 9 after class ends What to do for Week 10 before coming to class Extra Credit   What to do for Week 9 after class ends1   eCampus?  Description  Location         Complete problems 1, 2, 3, 4, 5, and 10 in Chapter 7.  Link       Complete problems 1, 2, 3, 4, and 5 in Chapter 8.  Link       Take a look at the R Walkthrough on t-tests.  Link     \n What to do for Week 10 before coming to class2   Area  Description  Location         Read chapters 7 and 8 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch videos on Hypothesis Testing and Confidence Intervals.  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes is still available.  Link       For grading. Due in two weeks.↩︎\n For preparation.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56b7ec6b6e53d3915f62daa80252c917","permalink":"/due/10-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/10-due/","section":"due","summary":"What to do for Week 9 after class ends What to do for Week 10 before coming to class Extra Credit   What to do for Week 9 after class ends1   eCampus?","tags":null,"title":"Testing Hypotheses","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we are looking ahead a bit and prepping for testing by first looking over the logic of the p-value and then transitioning to the almighty (statistical) power.\n  Remember you can always access a Crash Course Statistics playlist using the following link.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d7091da7131dcaeb0a7a2758ca2db8e","permalink":"/example/10-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/10-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we are looking ahead a bit and prepping for testing by first looking over the logic of the p-value and then transitioning to the almighty (statistical) power.","tags":null,"title":"Testing Hypotheses","type":"docs"},{"authors":null,"categories":null,"content":"   In Class Notes Textbook Class Notes R Walkthrough: Testing   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation.   Textbook Class Notes Download the textbook\n Ch 7 PowerPoint, Ch 7 Lecture Notes, Ch 8 PowerPoint, and Ch 8 Lecture Notes.   R Walkthrough: Testing Take a look at a walkthrough about t-tests.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"86a399491d0977aee8295e2950c3a1b2","permalink":"/lesson/10-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/10-lesson/","section":"lesson","summary":"In Class Notes Textbook Class Notes R Walkthrough: Testing   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may","tags":null,"title":"Testing Hypotheses","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff  Read Chapter 7: Estimation Read Chapter 8: Testing Hypothesis   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d9d98a7fb29dd2567fe38882b5fca7f","permalink":"/readings/10-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/10-readings/","section":"readings","summary":"   Book Stuff   Book Stuff  Read Chapter 7: Estimation Read Chapter 8: Testing Hypothesis   ","tags":null,"title":"Testing Hypotheses","type":"docs"},{"authors":null,"categories":null,"content":"    Preparation Purpose Objectives Packages  The t.test() Command It’s All About the Assumptions Example Midwest Data Set from ggplot2 One-sample t-test Two-sample t-test The Paired t-test    Preparation Download a script file of just the R chunks used in this walkthrough.\n Purpose One of the most common tests in statistics, the t-test, is used to determine whether the means of two groups are equal to each other. The assumption for the test is that both groups are sampled from normal distributions with equal variances. The null hypothesis is that the two means are equal, and the alternative is that they are not. It is known that under the null hypothesis, we can calculate a t-statistic that will follow a t-distribution with \\(n_1+n_2 - 2\\) degrees of freedom. There is also a widely used modification of the t-test, known as Welch’s t-test that adjusts the number of degrees of freedom when the variances are thought not to be equal to each other. This tutorial covers the basics of performing t-tests in R.\nObjectives This guide serves as an introduction to performing t-tests to compare two groups. Here’s what we’ll\n introduce the midwest example data set use the patchwork package associate the t-test in R by using the overarching command t.test() provide some familiarity with the variants of the t-test:  One-sample t-tests: Compare the sample mean with a known value, when the variance of the population is unknown Two-sample t-tests: Compare the means of two groups under the assumption that both samples are random, independent, and normally distributed with unknown but equal variances Paired t-tests: Compare the means of two sets of paired samples, taken from two populations with unknown variance    Packages There is one package we’ll be using that you probably do not have: the patchwork package which is a tidy update to the base R grid and gridExtra packages. If you do not have it yet, make sure to install it first using\ninstall.packages(\u0026#39;patchwork\u0026#39;) Now please load up the following packages\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::group_rows() masks kableExtra::group_rows() ## x dplyr::lag() masks stats::lag() library(patchwork)   The t.test() Command The t.test() function can be used to perform both one and two sample t-tests on vectors of data. The function contains a variety of arguments and is called as follows:\nt.test(x, y = NULL, alternative = c(\u0026quot;two.sided\u0026quot;, \u0026quot;less\u0026quot;, \u0026quot;greater\u0026quot;), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95) Each of the letters - aka arguments - inside of the command are specifically defined:\n   Options that you can use in t.test()  What it is…      x  a numeric vector from a data set    y  an optional numeric vector from a data set    mu  a number indicating the true value of the mean    alternative  preference on type of test you wish to run    paired  preference on whether you wish to perform a paired t-test    var.equal  indicates whether or not to assume equal variances when performing a two-sample t-test    conf.level  the confidence level of the reported confidence interval      Below are a few things to note about the t.test() command before we move on that can drastically affect your outcomes if you are not aware of them.\n If the variable y is  excluded, t.test() will run as a one-sample t-test included, t.test() will run as a two-sample t-test  The default t.test() command will run as a two-sided t-test. However you can perform an alternative hypothesis by changing the alternative argument to:  “greater”, or “less”. For example    t.test(x, alternative = \u0026quot;greater\u0026quot;, mu = 47) performs a one-sample t-test on the data contained in x where the - null hypothesis is \\(\\mu = 47\\), and the - alternative hypothesis is \\(\\mu \u0026gt; 47\\).\n The paired argument has a default setting of FALSE indicating that a paired t-test should not be run. If you wish to run a paired t-test, this can be done by changing the setting to TRUE.\n The var.equals argument has a default setting of FALSE indicating unequal variances ans applies the Welsch approximation to the degrees of freedom. If you wish to have equal variances, this can be done by changing the setting to TRUE.\n The conf.level argument is set to 95%, or where \\(\\alpha = 0.05\\). The confidence interval is determined by\n \\(\\mu\\) for the one-sample t-test, and \\(\\mu_1-\\mu_2\\) for the one-sample t-test.   On a final note, the wilcox.test() option provides the same basic functionality and arguments, but with the idea that we do not want to assume the data to follow a normal distribution. This is often the case for samples but at this time, we will assume that the data that is normally distributed. Understanding the wilcox.test() function is beyond the scope of this course.\n It’s All About the Assumptions Before we can move forward in the exploration of the t-test, we must first make sure that any data that we use adheres to the test assumptions.\n Assumption #1: Random sampling: Data is used from random sampling. Assumption #2: Independent observations: Observations are independent from one another. Assumption #3: Normality: Observations are from a normally distributed population. Assumption #4: Homogeneity: If more than one population is sampled from, then the populations have equal variances (also known as homogeneity of variances)   Example Midwest Data Set from ggplot2 The ggplot2 package has some test data built in, though you may not find them that exciting which is the reason we tend to use real-world data that is more connected to the social sciences and education. However, in this walkthrough, we will be using one of the ggplot2 data sets named midwest. This data set contains contains county-level data for 5 states: IL, IN, MI, OH, \u0026amp; WI derived from the 2010 U.S. Census. I’ll leave the level of excitement up to you. When you lead up tidyverse, or more specifically ggplot2, the midwest data sets is also loaded automatically. Let’s take a look at it:\nhead(midwest) ## # A tibble: 6 x 28 ## PID county state area poptotal popdensity popwhite popblack popamerindian ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 561 ADAMS IL 0.052 66090 1271. 63917 1702 98 ## 2 562 ALEXA… IL 0.014 10626 759 7054 3496 19 ## 3 563 BOND IL 0.022 14991 681. 14477 429 35 ## 4 564 BOONE IL 0.017 30806 1812. 29344 127 46 ## 5 565 BROWN IL 0.018 5836 324. 5264 547 14 ## 6 566 BUREAU IL 0.05 35688 714. 35157 50 65 ## # … with 19 more variables: popasian \u0026lt;int\u0026gt;, popother \u0026lt;int\u0026gt;, percwhite \u0026lt;dbl\u0026gt;, ## # percblack \u0026lt;dbl\u0026gt;, percamerindan \u0026lt;dbl\u0026gt;, percasian \u0026lt;dbl\u0026gt;, percother \u0026lt;dbl\u0026gt;, ## # popadults \u0026lt;int\u0026gt;, perchsd \u0026lt;dbl\u0026gt;, percollege \u0026lt;dbl\u0026gt;, percprof \u0026lt;dbl\u0026gt;, ## # poppovertyknown \u0026lt;int\u0026gt;, percpovertyknown \u0026lt;dbl\u0026gt;, percbelowpoverty \u0026lt;dbl\u0026gt;, ## # percchildbelowpovert \u0026lt;dbl\u0026gt;, percadultpoverty \u0026lt;dbl\u0026gt;, ## # percelderlypoverty \u0026lt;dbl\u0026gt;, inmetro \u0026lt;int\u0026gt;, category \u0026lt;chr\u0026gt;  One-sample t-test The one-sample t-test compares a sample’s mean with a known value, when the variance of the population is unknown. Consider we want to assess the percent of college educated adults in the midwest and compare it to a certain value. For example, let’s assume the nation-wide average of college educated adults is approximately 37% (Bachelor’s degree or higher) and we want to see if the midwest mean is significantly different than the national average; in particular we want to test if the midwest average is less than the national average.\nLet’s first take a look at how the data looks (important!) and some descriptive statistics:\nhead(midwest$percollege, 10) ## [1] 19.63139 11.24331 17.03382 17.27895 14.47600 18.90462 11.91739 16.19712 ## [9] 14.10765 41.29581 summary(midwest$percollege) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 7.336 14.114 16.798 18.273 20.550 48.079 and then plot it to see the distribution:\np1 \u0026lt;- ggplot(midwest, aes(x = percollege)) + geom_histogram(aes(fill = ..count..), color = \u0026quot;#ccccaa\u0026quot;) + scale_fill_gradient(\u0026quot;Frequency\u0026quot;, low = \u0026quot;#5cb85c\u0026quot;, high = \u0026quot;#428bca\u0026quot;) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;, legend.direction = \u0026quot;horizontal\u0026quot;) p2 \u0026lt;- ggplot(midwest, aes(x = percollege)) + geom_histogram(aes(fill = ..count..), color = \u0026quot;#ccccaa\u0026quot;) + scale_fill_gradient(\u0026quot;Frequency\u0026quot;, low = \u0026quot;#5cb85c\u0026quot;, high = \u0026quot;#428bca\u0026quot;) + scale_x_log10() + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;, legend.direction = \u0026quot;horizontal\u0026quot;) p1 + p2 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. To test if the midwest average is less than the national average, we’ll perform three tests. First we test with a normal t-test without any distribution transformations. The results below show a p-value \u0026lt; .001 supporting the alternative hypothesis that “the true mean is less than 37%.”\nt.test(midwest$percollege, mu = 37, alternative = \u0026quot;less\u0026quot;) ## ## One Sample t-test ## ## data: midwest$percollege ## t = -62.518, df = 436, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true mean is less than 37 ## 95 percent confidence interval: ## -Inf 18.7665 ## sample estimates: ## mean of x ## 18.27274 Alternatively, due to the non-normality concerns we can perform this test in another way to ensure our results are not being biased due to assumption violations. We can perform the t-test and transform our data. The results support our initial conclusion that the percent of college educated adults in the midwest is statistically less than the nationwide average.\nt.test(log(midwest$percollege), mu = log(37), alternative = \u0026quot;less\u0026quot;) ## ## One Sample t-test ## ## data: log(midwest$percollege) ## t = -51.37, df = 436, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true mean is less than 3.610918 ## 95 percent confidence interval: ## -Inf 2.879812 ## sample estimates: ## mean of x ## 2.855574  Two-sample t-test Now let’s say we want to compare the differences between the average percent of college educated adults in Ohio versus Michigan. Here, we want to perform a two-sample t-test. So let’s first do some data wrangling…\nohio_mi \u0026lt;- midwest %\u0026gt;% filter(state == \u0026quot;OH\u0026quot; | state == \u0026quot;MI\u0026quot;) %\u0026gt;% select(state, percollege) …and find some descriptive statistics\n First for Ohio:  # Ohio summary stats summary(ohio_mi %\u0026gt;% filter(state == \u0026quot;OH\u0026quot;) %\u0026gt;% .$percollege) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 7.913 13.089 15.462 16.890 18.995 32.205  and then for Michigan:  # Ohio summary stats summary(ohio_mi %\u0026gt;% filter(state == \u0026quot;MI\u0026quot;) %\u0026gt;% .$percollege) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 11.31 14.61 17.43 19.42 21.31 48.08 We can see Ohio appears to have slightly less college educated adults than Michigan but the graphic doesn’t tell us if it is statistically significant or not.\nggplot(ohio_mi, aes(x = state, y = percollege, fill = state)) + geom_boxplot(alpha = 0.5) + scale_fill_manual(values = c(\u0026quot;#00274C\u0026quot;, \u0026quot;#BB0000\u0026quot;)) + theme_minimal()  We can also once again see similar skewness within the sample distributions by running the following.\np3 \u0026lt;- ggplot(ohio_mi, aes(x = percollege)) + geom_histogram(aes(fill = ..count..), color = \u0026quot;#ccccaa\u0026quot;) + scale_fill_gradient(\u0026quot;Frequency\u0026quot;, low = \u0026quot;#5cb85c\u0026quot;, high = \u0026quot;#428bca\u0026quot;) + facet_wrap(~ state) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;, legend.direction = \u0026quot;horizontal\u0026quot;) p4 \u0026lt;- ggplot(ohio_mi, aes(x = percollege)) + geom_histogram(aes(fill = ..count..), color = \u0026quot;#ccccaa\u0026quot;) + scale_fill_gradient(\u0026quot;Frequency\u0026quot;, low = \u0026quot;#5cb85c\u0026quot;, high = \u0026quot;#428bca\u0026quot;) + facet_wrap(~ state) + scale_x_log10() + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;, legend.direction = \u0026quot;horizontal\u0026quot;) p3 + p4 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Similar to the plots in the one-sample example, to test if the Ohio and Michigan averages differ we’ll perform two tests. Also, note that we am searching for any differences between the means rather than if one is specifically less than or greater than the other. First we test with a normal t-test without any distribution transformations. The results below show a p-value \u0026lt; 0.01 supporting the alternative hypothesis that “true difference in means is not equal to 0”; essentially it states there is a statistical difference between the two means.\nt.test(percollege ~ state, data = ohio_mi) ## ## Welch Two Sample t-test ## ## data: percollege by state ## t = 2.5953, df = 161.27, p-value = 0.01032 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.6051571 4.4568579 ## sample estimates: ## mean in group MI mean in group OH ## 19.42146 16.89045 Again as an alternative, due to the non-normality concerns we can perform this test in another way to ensure our results are not being biased due to assumption violations. We can perform the t-test and transform our data. The results support our initial conclusion that the percent of college educated adults in Ohio is statistically different than the percent in Michigan.\nt.test(log(percollege) ~ state, data = ohio_mi) ## ## Welch Two Sample t-test ## ## data: log(percollege) by state ## t = 2.9556, df = 168.98, p-value = 0.003567 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.04724892 0.23732151 ## sample estimates: ## mean in group MI mean in group OH ## 2.915873 2.773587  The Paired t-test To illustrate the paired t-test we’ll use the built-in sleep data set.\nPlease load up the following packages\nsleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 ggplot(sleep, aes(group, extra, fill = group)) + geom_boxplot(alpha = 0.5) + scale_fill_manual(values = c(\u0026quot;#00274C\u0026quot;, \u0026quot;#BB0000\u0026quot;)) + theme_minimal()  In this case we are assessing if there is a statistically significant effect of a particular drug on sleep (increase in hours of sleep compared to control) for 10 patients. Please see ?sleep for more details on the variables.\nWe want to see if the mean values for the extra variable differs between group 1 and group 2. Here, we perform the t.test as in the previous sections but just add the paired = TRUE argument:\nt.test(extra ~ group, data = sleep, paired = TRUE) ## ## Paired t-test ## ## data: extra by group ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean of the differences ## -1.58 In this example it appears that the drug does have an effect as the \\(p\\)-value = 0.0028 suggesting that the drug increases sleep on average by 1.58 hours.\nThe above looks at exact matching. If you are getting an error, try running which will only run on values that can be matched. To use the command below, make sure both comparison variables are numeric.\nsleep \u0026lt;- sleep %\u0026gt;% mutate(group = as.numeric(group)) t.test(sleep$extra, sleep$group, paired = TRUE) ## ## Paired t-test ## ## data: sleep$extra and sleep$group ## t = 0.095569, df = 19, p-value = 0.9249 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.8360223 0.9160223 ## sample estimates: ## mean of the differences ## 0.04   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7df8636a8a05fd384353be68edbd1ca1","permalink":"/lesson/hypotheses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/hypotheses/","section":"lesson","summary":"Preparation Purpose Objectives Packages  The t.test() Command It’s All About the Assumptions Example Midwest Data Set from ggplot2 One-sample t-test Two-sample t-test The Paired t-test    Preparation Download a script file of just the R chunks used in this walkthrough.","tags":null,"title":"The t-tests","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 10 after class ends What to do for Week 11 before coming to class Extra Credit   What to do for Week 10 after class ends1   eCampus?  Description  Location         Complete problems 1, 3, 5, 6, 7 and 9 in Chapter 9.  Link       Take a look at the R Walkthrough on comparing groups.  Link     \n What to do for Week 11 before coming to class2   Area  Description  Location         Read chapter 10 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes is still available.  Link       For grading. Due in two weeks.↩︎\n For preparation.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22f9069b8627ff0c72a9280426f5ff6d","permalink":"/due/11-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/11-due/","section":"due","summary":"What to do for Week 10 after class ends What to do for Week 11 before coming to class Extra Credit   What to do for Week 10 after class ends1   eCampus?","tags":null,"title":"Bivariate Tables","type":"docs"},{"authors":null,"categories":null,"content":"   In Class Notes Textbook Class Notes R Walkthrough: Comparing Groups   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation.   Textbook Class Notes Download the textbook\n Ch 9 PowerPoint, Ch 9 Lecture Notes,   R Walkthrough: Comparing Groups Click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download\n The PREDIMED Codebook used in the slideshow. a PDF of the presentation above. a script file of just the R chunks used in the presentation.   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2cd5967b5ce567cec7f3684c70946623","permalink":"/lesson/11-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/11-lesson/","section":"lesson","summary":"In Class Notes Textbook Class Notes R Walkthrough: Comparing Groups   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may","tags":null,"title":"Bivariate Tables","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 9: Bivariate Tables\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eb299dbb9100abaed1c560aaa50941b2","permalink":"/readings/11-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/11-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 9: Bivariate Tables\n ","tags":null,"title":"Bivariate Tables","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 12 after class ends What to do for Week 13 before coming to class Extra Credit   What to do for Week 12 after class ends1   eCampus?  Description  Location         Begin problems 1, 2, 3, 5, 7, and 13 in Chapter 10.  Link       Take a look at the R Walkthrough on the Chi Square Distribution.  Link     \n What to do for Week 13 before coming to class2   Area  Description  Location         Review chapter 11 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch videos on ANOVAs3.  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes is still available.  Link       For grading.↩︎\n For preparation.↩︎\n We will cover this in one week↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9ad95ac1cffe3318ca0019fb8910275","permalink":"/due/12-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/12-due/","section":"due","summary":"What to do for Week 12 after class ends What to do for Week 13 before coming to class Extra Credit   What to do for Week 12 after class ends1   eCampus?","tags":null,"title":"The Chi-Square Test and Measures of Association","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we are looking at the all mighty Chi Square test. Akin to a punnet square, these are a great way of seeing interactions between individual variables.\n Remember you can always access a Crash Course Statistics playlist using the following link.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f294a1c92be1a918ba3fa24cc427a78","permalink":"/example/12-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/12-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we are looking at the all mighty Chi Square test. Akin to a punnet square, these are a great way of seeing interactions between individual variables.","tags":null,"title":"The Chi-Square Test and Measures of Association","type":"docs"},{"authors":null,"categories":null,"content":"   In Class Notes Textbook Class Notes R Walkthrough: Chi Square   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation.   Textbook Class Notes Download the textbook\n PowerPoint Lecture Notes   R Walkthrough: Chi Square Take a look at a walkthrough about the \\(\\chi^2\\) statistic.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"05e82a05a3ed4f364a54f9dff01f853b","permalink":"/lesson/12-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/12-lesson/","section":"lesson","summary":"In Class Notes Textbook Class Notes R Walkthrough: Chi Square   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may","tags":null,"title":"The Chi-Square Test and Measures of Association","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 10: The Chi-Square Test and Measures of Association\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"08be2f669f9c0930bca2c6b966d76564","permalink":"/readings/12-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/12-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 10: The Chi-Square Test and Measures of Association\n ","tags":null,"title":"The Chi-Square Test and Measures of Association","type":"docs"},{"authors":null,"categories":null,"content":"   Purpose Objectives Packages  It’s All About the Assumptions Example The General Social Survey (GSS) Getting (Some of) the GSS Data Set: Purpose Data Wrangling  Setting Up Hypothesis Assumptions  Analysis Analytically Programatically Interpretation and Conclusion  Your turn!   Purpose While we can certainly perform statistical analysis in Base R, it is far more dynamic when we use the tidyverse package. In this walk through, we’ll explore chi-square statistics using the infer package which conducts statistical inference and plays well with tidyverse design framework.\nObjectives  introduce the infer and patchwork packages apply the Chi square statistic provide some familiarity with the General Social Survey   Packages Please load up the following packages\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::group_rows() masks kableExtra::group_rows() ## x dplyr::lag() masks stats::lag() library(infer) library(patchwork) library(viridis) Remember to download them if you receive an error:\ninstall.packages(\u0026quot;infer\u0026quot;) install.packages(\u0026quot;patchwork\u0026quot;) install.packages(\u0026quot;viridis\u0026quot;)   It’s All About the Assumptions Before we can move forward in the exploration of the \\(\\chi^2\\) statistic, we must first make sure that any data that we use adheres to the following checklist, which are formally known as , or just .\n : The variables are categorical and can be categorized in to one of the three standard types:  Dichotomous: Two (2) groups (e.g. Male and Female). Nominal: Three (3) or more categorical groups (e.g. undergraduate, graduate student, postdoctoral scholar, professor). Ordinal: ordered groups (e.g. Pain Level 1, Pain Level 2, Pain Level 3, …).  : Observations are independent of one another (e.g. no relationship between any of the cases). : Categorical variable groupings must be mutually exclusive (e.g. a participant cannot be both a Democrat and Republican). : There must be at minimum five (5) expected frequencies in each group of your categorical variable.  NOTE: If you do not meet ALL of the assumptions for any statistical test, this violation changes the conclusion of the research and interpretation of the results of any analysis which is be misleading or complete nonsense, or analytical garbage. So please please please make sure you account for all assumptions! Generally, most tests that are used the same four assumptions about the data set being analyzed (see ). The four may not be the only ones, but they are typically observed in many commonly used approaches.\n  OK you have that in mind? Now let’s go exploring!\n Example The General Social Survey (GSS) Funded by the National Science Foundation (NSF), the General Social Survey (GSS) is a social science oriented opinion based survey that has been regularly administered since 1972 by the National Opinion Research Center (NORC) at the University of Chicago.\nThe GSS provides two pieces of key information about the American society at a given point in time:\n It gathers data on our contemporary society to assess and describe trends and constants in key areas related to people’s attitudes, behaviors, and attributes. It contains a core of demographic, behavioral, attitudinal questions, and those deemed important at the time. Topics that describe the latter include civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events. (NORC at the University of Chicago, 2016)  The de-identified variant, or the public data set is free to use by anyone. As far as “big data” that is also longitudinal, the GSS is a fantastic source of information about how the views of those in the United States has or has not shifted since 1972.\n Getting (Some of) the GSS Data Set: We are only using a select portion of the GSS because the data set is humongous! R is really good at getting analyses performed in a logical and comprehensive way but what it has in processing power, it lacks in handling “big data” sets. For the processing of very large data sets, you really need a software like Python. With that said, R can still handle a lot. It is rare to come across a data set that R cannot handle well, but the entire GSS is one of the few. Of course other criteria such as a computer’s memory, disk space, internet speed, etc. also limit R or any other software package from loading and analyzing any data set.\nRather than having you load an external file into R Studio, we’ll do it via the web. Run the following command that uses the read_csv() command to grab and load external data sets from an external website. In this case it is a site called Github, namely from a repository from my public site (which doesn’t actually have much of anything publicly available).\ngss_nasa \u0026lt;- \u0026quot;https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/gss_nasa.csv\u0026quot; %\u0026gt;% read_csv() ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## id = col_double(), ## year = col_double(), ## age = col_double(), ## consci = col_double(), ## oversamp = col_double() ## ) ## ℹ Use `spec()` for the full column specifications.  Purpose In this walk through, we’ll be assessing if funding for space exploration is a partisan oriented issue. Many data sets include more information than you need to answer your questions, some of which have nothing to do with your goals. Let’s see what this data set has:\nnames(gss_nasa) ## [1] \u0026quot;id\u0026quot; \u0026quot;year\u0026quot; \u0026quot;age\u0026quot; \u0026quot;class\u0026quot; \u0026quot;degree\u0026quot; \u0026quot;sex\u0026quot; ## [7] \u0026quot;marital\u0026quot; \u0026quot;race\u0026quot; \u0026quot;region\u0026quot; \u0026quot;partyid\u0026quot; \u0026quot;happy\u0026quot; \u0026quot;relig\u0026quot; ## [13] \u0026quot;cappun\u0026quot; \u0026quot;finalter\u0026quot; \u0026quot;natspac\u0026quot; \u0026quot;natarms\u0026quot; \u0026quot;conclerg\u0026quot; \u0026quot;confed\u0026quot; ## [19] \u0026quot;conpress\u0026quot; \u0026quot;conjudge\u0026quot; \u0026quot;consci\u0026quot; \u0026quot;conlegis\u0026quot; \u0026quot;zodiac\u0026quot; \u0026quot;oversamp\u0026quot; ## [25] \u0026quot;postlife\u0026quot; \u0026quot;party\u0026quot; \u0026quot;space\u0026quot;  Data Wrangling You should never assess a data set without a codebook. The one for the GSS is pretty large and can be found here, but for the purposes of this walk through, we’ll be looking at the fields party and space.\ngss_select \u0026lt;- gss_nasa %\u0026gt;% select(party, space) gss_select %\u0026gt;% head() ## # A tibble: 6 x 2 ## party space ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Ind TOO LITTLE ## 2 Ind ABOUT RIGHT ## 3 Dem ABOUT RIGHT ## 4 Ind TOO LITTLE ## 5 Ind TOO MUCH ## 6 Ind TOO LITTLE OK great but what about the choices? How would we know what factors make up each vector? Well we can do this using the command unique() by\ngss_select %\u0026gt;% select(party) %\u0026gt;% unique() ## # A tibble: 3 x 1 ## party ## \u0026lt;chr\u0026gt; ## 1 Ind ## 2 Dem ## 3 Rep gss_select %\u0026gt;% select(space) %\u0026gt;% unique() ## # A tibble: 3 x 1 ## space ## \u0026lt;chr\u0026gt; ## 1 TOO LITTLE ## 2 ABOUT RIGHT ## 3 TOO MUCH We can see that people identified themselves as either Democrat (Dem), Independent (Ind), or Republican (Rep) and they could decide if the funding for space exploration was TOO LITTLE, ABOUT RIGHT, or TOO MUCH.\nNow is probably a good time to visualize the data so that we can simply get an idea of any disparity between the views of those within each political organization, if it exists.\ngss_select %\u0026gt;% ggplot(aes(x=party, fill = space)) + geom_bar() + scale_fill_viridis(discrete = TRUE) + theme_minimal()  Figure 1: Bar plot by frequencies.  Well that is nice but the legend is out of order. Let’s fix that!\nLet’s first get information about the format of each vector.  str(gss_select) ## tibble [149 × 2] (S3: tbl_df/tbl/data.frame) ## $ party: chr [1:149] \u0026quot;Ind\u0026quot; \u0026quot;Ind\u0026quot; \u0026quot;Dem\u0026quot; \u0026quot;Ind\u0026quot; ... ## $ space: chr [1:149] \u0026quot;TOO LITTLE\u0026quot; \u0026quot;ABOUT RIGHT\u0026quot; \u0026quot;ABOUT RIGHT\u0026quot; \u0026quot;TOO LITTLE\u0026quot; ... Well it looks like we have two character vectors. We can’t do much with the order of those types of variables. This is where the variable type factor comes in handy. So what are they? It is how we store truly categorical information in R. In general, the levels are friendly human-readable character strings, like “male/female/transgender/other”, “control/treatment”, etc. The values a factor can take on are called the levels. For example, in the Gapminder data set, the levels of the factor continent were Africa, Americas, Asia, Europe, and Oceania. To verify, simply load the gapminder package and run the command levels() on the continent vector.\nlibrary(gapminder) levels(gapminder$continent) ## [1] \u0026quot;Africa\u0026quot; \u0026quot;Americas\u0026quot; \u0026quot;Asia\u0026quot; \u0026quot;Europe\u0026quot; \u0026quot;Oceania\u0026quot; Converts any vector we want to reorder into factors:  gss_factors \u0026lt;- gss_select %\u0026gt;% mutate(party = as.factor(party)) %\u0026gt;% mutate(space = as.factor(space)) Check the current factor orders.  gss_factors %\u0026gt;% pull(party) %\u0026gt;% levels() ## [1] \u0026quot;Dem\u0026quot; \u0026quot;Ind\u0026quot; \u0026quot;Rep\u0026quot; gss_factors %\u0026gt;% pull(space) %\u0026gt;% levels() ## [1] \u0026quot;ABOUT RIGHT\u0026quot; \u0026quot;TOO LITTLE\u0026quot; \u0026quot;TOO MUCH\u0026quot; Note in the above two commands that the pull() command is used because levels() expects a vector. Using select() would have kept the space column as a data frame. An alternative approach would be to use Base R\nlevels(gss_factors$party) ## [1] \u0026quot;Dem\u0026quot; \u0026quot;Ind\u0026quot; \u0026quot;Rep\u0026quot; levels(gss_factors$space) ## [1] \u0026quot;ABOUT RIGHT\u0026quot; \u0026quot;TOO LITTLE\u0026quot; \u0026quot;TOO MUCH\u0026quot; You may have noticed by now that the default factor order is in its natural positioning, which in this case is in alphabetical order. While this appears fine for some items, its probably better to report items in the order they normally are. In the case of party affiliation, we should have Democrats (Dem), Republicans (Rep), and then Independents(Ind) and their possible choices of TOO MUCH,ABOUT RIGHT, and TOO LITTLE. Of course how you order factors is absolutely dependent on each situation. Sometimes its for reporting but other times its simply for aesthetic reasons.\nReorder the factor levels  gss_newlevels \u0026lt;- gss_factors %\u0026gt;% mutate(party = factor(party, levels = c(\u0026quot;Dem\u0026quot;, \u0026quot;Rep\u0026quot;, \u0026quot;Ind\u0026quot;))) %\u0026gt;% mutate(space = factor(space, levels = c(\u0026quot;TOO MUCH\u0026quot;, \u0026quot;ABOUT RIGHT\u0026quot;, \u0026quot;TOO LITTLE\u0026quot;))) We use factor() on the columns party and space and then define the order we want by setting levels =. Let’s check them:\ngss_newlevels %\u0026gt;% pull(space) %\u0026gt;% levels() ## [1] \u0026quot;TOO MUCH\u0026quot; \u0026quot;ABOUT RIGHT\u0026quot; \u0026quot;TOO LITTLE\u0026quot; gss_newlevels %\u0026gt;% pull(party) %\u0026gt;% levels() ## [1] \u0026quot;Dem\u0026quot; \u0026quot;Rep\u0026quot; \u0026quot;Ind\u0026quot; That looks right!\nPlot with the new factor levels  gss_newlevels %\u0026gt;% ggplot(aes(x=party, fill = space)) + geom_bar() + scale_fill_viridis(discrete = TRUE) + theme_minimal()  Figure 2: Ordered bar plot by frequencies.  OK now it plots in the correctly in the order we want. However, the problem now lies in the fact that its hard to compare the three bar plots since they aren’t on equal footing. This is the problem with frequency data, in that it can be misleading when we’re looking for trends between categorical variables within groups. To counteract this, let’s normalize them by looking at each chunk of each bar as a percent using the command position = fill.\ngss_newlevels %\u0026gt;% ggplot(aes(x=party, fill = space)) + geom_bar(position = \u0026quot;fill\u0026quot;) + scale_fill_viridis(discrete = TRUE) + theme_minimal() + ylab(\u0026quot;within group percentage\u0026quot;)  Figure 3: Ordered bar plot by percent total.  That is so much nicer! However, it doesn’t really look like there is much of a difference in how Democrats, Republicans, and Independents support space exploration/ Remember that the bar plot represents descriptive statistics but to infer any differences, we must use the aptly named inferential statistics.\nLet’s drill down into this with some hypothesis testing, comparing Base R and the infer package. What we essentially have is a contingency table of party affiliation and attitude towards space exploration, and we want to see if there’s a relationship between these variables. The Chi Squared Test of independence is used to determine if a significant relationship exists between two categorical variables, so we will use this test.\n  Setting Up Hypothesis  Null hypothesis: There is no relationship between party (Democrat, Republican, Independent) and attitude towards space exploration (too little, about right, too much). Alternative hypothesis: There is a relationship between party (Democrat, Republican, Independent) and attitude towards space exploration (too little, about right, too much).  OK first, do we meet the assumptions?\n Assumptions Clearly both party and space are categorical. Observations are clearly independent in that party and space are measure two very different concepts. Respondents had to choose between being Democrats, Republicans, or Independents and their corresponding viewpoints were delineated between too little, about right, or too much. These are mutually exclusive “events” or values. We have more than five cases for each. In fact, we have  gss_newlevels %\u0026gt;% count() ## # A tibble: 1 x 1 ## n ## \u0026lt;int\u0026gt; ## 1 149 or if you want to get counts of each value, run\ngss_newlevels %\u0026gt;% summary() ## party space ## Dem:43 TOO MUCH :43 ## Rep:34 ABOUT RIGHT:76 ## Ind:72 TOO LITTLE :30 In any case, we more than meet the minimum threshold.\n  Analysis There are two main ways to solve this problem:\n Analytically Programatically  Analytically We assume the expected values follow a Chi-squared distribution, with a probability density function that depends on the degrees of freedom. Looking at the plot below, observe how the distribution varies with the degrees of freedom (\\(k\\)) on the \\(x\\)-axis is the Chi-squared statistic, which we can calculate in R.\n Figure 4: Chi-squre distribution constructed in R (code availible upon request).  We could then see where it falls in the distribution, and observe the probability of arriving at that combination of variables, or a more extreme example. As our Chi-squared test statistic increases, we move further along the \\(x\\)-axis to the right. There is less area under the curve to the right, and our \\(p\\)-value (the area under the curve to the right of the observed statistic) decreases.\nGenerally speaking, a larger Chi-squared statistic suggests stronger evidence for rejecting our null hypothesis. If we observe a \\(p\\)-value \\(\\leq 0.05\\), we would reject our null hypothesis.\nWhat would it mean to accept our alternative hypothesis?\nIn the case of our example, if we we lived in a completely random universe, less than 5% of the time we would arrive at the particular combination of party and attitude towards space exploration we observe in our data. In other words, the relationship between party and attitude towards space exploration we see in our data is .\nBut we don’t live in a completely random universe - think about getting randomly married to someone else - so the real question still remains: Well we can use Base R’s Chi-squared test to find out:\nchisq.test(gss_newlevels$party, gss_newlevels$space) ## ## Pearson\u0026#39;s Chi-squared test ## ## data: gss_newlevels$party and gss_newlevels$space ## X-squared = 1.3261, df = 4, p-value = 0.8569 Let’s save this observed Chi statistic for later use.\nobserved_stat \u0026lt;- chisq.test(gss_newlevels$party, gss_newlevels$space)$stat We might be tempted to look at this and say, there’s a high \\(p\\)-value. No significant relationship exists. So we’re done! This is what we expected looking at the bar plots earlier! Well not so fast? Let’s look at it programatically.\n Programatically Another way to test if there is a significant relationship in our data is to take a programmatic approach. Basically the idea here is if we live in world where variables are totally unrelated, they might as well have been randomly put together. Basically in the real world, yes of course things (variables) are related! Someone’s party affiliation is predicated on their history and maturation and that then most likely informs their views on funding, especially for an area of the sciences like space exploration. So does the data we have look more like random or normal world?\nLet’s explore this by taking one of the columns of our data frame and scrambling it.\ngss_newlevels %\u0026gt;% mutate(random_1 = sample(space), random_2 = sample(space)) ## # A tibble: 149 x 4 ## party space random_1 random_2 ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Ind TOO LITTLE ABOUT RIGHT TOO MUCH ## 2 Ind ABOUT RIGHT TOO MUCH ABOUT RIGHT ## 3 Dem ABOUT RIGHT TOO LITTLE TOO LITTLE ## 4 Ind TOO LITTLE TOO LITTLE TOO MUCH ## 5 Ind TOO MUCH TOO LITTLE ABOUT RIGHT ## 6 Ind TOO LITTLE ABOUT RIGHT TOO MUCH ## 7 Ind ABOUT RIGHT ABOUT RIGHT ABOUT RIGHT ## 8 Dem ABOUT RIGHT ABOUT RIGHT ABOUT RIGHT ## 9 Dem TOO LITTLE TOO MUCH ABOUT RIGHT ## 10 Ind TOO LITTLE ABOUT RIGHT ABOUT RIGHT ## # … with 139 more rows The last two columns are distributions of random samples from the available choices within space and they represent what we would expect to see if the relationship between variables was completely random. We could generate many, many permutations, calculate an Chi-squared statistic for each, and we would expect their distribution to approach the density functions shown above. Then we could plot our data on that distribution and see where it fell. If the area under the curve to the right of the point was less than 5%, we could reject the null hypothesis.\n Figure 5: Inferential testing laid out in the infer package.  Some definitions  specify() is like dplyr::select(): choose the variables from your data frame to test hypothesize() is where we select the null hypothesis generate() creates randomized values form a predefined set of values calculate() lets you choose what test statistic to calculate visualize() automatically plots permuted with ggplot, making it easy to edit as needed   Benefits  inputs and outputs are both data frames composing tests with pipes reading an inferential chain describes an inferential procedure  gss_newlevels %\u0026gt;% specify(space ~ party) %\u0026gt;% hypothesize(null = \u0026quot;independence\u0026quot;) %\u0026gt;% generate(reps = 1000, type = \u0026quot;permute\u0026quot;) %\u0026gt;% calculate(stat = \u0026quot;Chisq\u0026quot;) %\u0026gt;% visualize() + geom_vline(aes(xintercept = observed_stat), color = \u0026quot;red\u0026quot;)  Figure 6: Visualization of the gss_newlevels data set using infer.  If we wanted to get a \\(p\\)-value from this programmatic approach, we can calculate the area under the curve to the right of the observed statistic:\ngss_newlevels %\u0026gt;% specify(space ~ party) %\u0026gt;% hypothesize(null = \u0026quot;independence\u0026quot;) %\u0026gt;% generate(reps = 1000, type = \u0026quot;permute\u0026quot;) %\u0026gt;% calculate(stat = \u0026quot;Chisq\u0026quot;) %\u0026gt;% summarise(p_val = mean(stat \u0026gt; observed_stat)) ## # A tibble: 1 x 1 ## p_val ## \u0026lt;dbl\u0026gt; ## 1 0.859   Interpretation and Conclusion So when we compare the \\(p\\)-value of this simulated data set with that of our real-world one, it sure looks like they are nearly identical. So It looks like a significant relationship does not exist so there isn’t a measurable difference between one’s party affiliation and their attitude towards space exploration funding.\n  Your turn! You may do this independently or in groups of two.\nDownload both of the following items:\nGSS Extract Data Set.csv data set and GSS Extract Codebook.csv code book  from my Github site using the following site addresses\nhttps://raw.githubusercontent.com/piechartssuck/RWDataSets/master/GSS%20Extract%20Data%20Set.csv https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/GSS%20Extract%20Codebook.csv  You are tasked to find if there is a relationship between a respondent and their views on science. This is exploratory, so it is incumbent on you to\ndefine the input and output variables. make a clear justification of why both are reasonable measures in the real world. create a null and alternative hypothesis perform a chi-squared test interpret the results  This is how real world data explorations work! There is no expectation that you will find anything at all or that you are expected to discover some grand connection. The points of this exploration are: Can you\nidentify important variables in a real-world data set? formulate a question about them off based of a vague request? analyze the data in an appropriate manner? draw conclusions and report them?   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3706e76979690d65fb76f9550f08e045","permalink":"/lesson/chisquare/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/chisquare/","section":"lesson","summary":"Purpose Objectives Packages  It’s All About the Assumptions Example The General Social Survey (GSS) Getting (Some of) the GSS Data Set: Purpose Data Wrangling  Setting Up Hypothesis Assumptions  Analysis Analytically Programatically Interpretation and Conclusion  Your turn!","tags":null,"title":"Week 12: Tidying Up the Chi Square Statistics","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 13s after class ends Extra Credit   What to do for Week 13s after class ends1   eCampus?  Description  Location         Submit problems 1, 2, 3, 5, 7, and 13 in Chapter 10.  Link       Take a look at jamovi.  Link       Area  Description  Location        Review chapter 10 in the text.  [Link](/readings/11-readings/#book-stuff-1)      Look over the text PowerPoint and corresponding notes.  [Link](/lesson/11-lesson/#textbook-class-notes-1)      Watch videos on ANOVAs^[We will cover this in one week].  [Link](/example/11-example/#crash-course-statistics-1)     --  Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes is still available.  Link       For grading.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"de590e50cde6933fa1397c3c7665a2a9","permalink":"/due/13-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/13-due/","section":"due","summary":"What to do for Week 13s after class ends Extra Credit   What to do for Week 13s after class ends1   eCampus?  Description  Location         Submit problems 1, 2, 3, 5, 7, and 13 in Chapter 10.","tags":null,"title":"Chi-Square and ANOVAs","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics The Difference is a More than the Letter C but not by Much!   Crash Course Statistics This week we are looking at understanding a bit about regression and ANOVAs. Your text switches these ideas around and while I understand why, having regression first is generally a more natural sequence.\nRegression   ANOVAs     The Difference is a More than the Letter C but not by Much! If you want to take a look at ANOVA+1, try giving this video a go. It discusses an idea called covariance which takes ANOVAS to ANCOVAS.  There are also more complex models called MANOVA, MANCOVAS, and so forth. Always keep in mind that as the complexity of a statistical model goes up, the ability to apply it generally goes down! If you want a rundown of the differing types, take a look at this page over at Stats Make Me Cry Consulting2.\nIn a nutshell, most problems that require inferences can be solved using a t-test or \\(\\chi^2\\). Keep it simple and accessible!\nRemember you can always access a Crash Course Statistics playlist using the following link.\n  A term I totally made up↩︎\n I also just love the name!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b49beb720db92e02ef300134b02a9dc","permalink":"/example/13-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/13-example/","section":"example","summary":"Crash Course Statistics The Difference is a More than the Letter C but not by Much!   Crash Course Statistics This week we are looking at understanding a bit about regression and ANOVAs.","tags":null,"title":"Chi-Square and ANOVAs","type":"docs"},{"authors":null,"categories":null,"content":"   In Class Notes Textbook Class Notes Jamovi   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation.   Textbook Class Notes Download the textbook\n Ch 12 PDF Ch 12 Outline   Jamovi Download and explore the jamovi software that uses the power and open source abilities of R in an environment that resembles SPSS.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ea81152b1f77e665a1de070264de2ad","permalink":"/lesson/13-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/13-lesson/","section":"lesson","summary":"In Class Notes Textbook Class Notes Jamovi   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may","tags":null,"title":"Chi-Square and ANOVAs","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff  Read Chapter 11: Analysis of Variance\n Read over A Gentle Introduction to Bayesian Analysis: Applications to Developmental Research1\n    You certainly do not have to understand this. It is a different way of thinking. If you can just get the idea of how Bayesian thinking works, that is win!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"87ae5dd2a95f648d734db4abb8d71ddd","permalink":"/readings/13-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/13-readings/","section":"readings","summary":"Book Stuff   Book Stuff  Read Chapter 11: Analysis of Variance\n Read over A Gentle Introduction to Bayesian Analysis: Applications to Developmental Research1\n    You certainly do not have to understand this.","tags":null,"title":"Chi-Square and ANOVAs","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 13 after class ends Extra Credit   What to do for Week 13 after class ends1   eCampus?  Description  Location         Submit the Data portion of the second exam.  Link       Submit the Paper portion of the second exam.  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes has posted.  Link       For grading.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e3bf3db9ce1c1bc8bc30f9deb8e34d5b","permalink":"/due/14-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/14-due/","section":"due","summary":"What to do for Week 13 after class ends Extra Credit   What to do for Week 13 after class ends1   eCampus?  Description  Location         Submit the Data portion of the second exam.","tags":null,"title":"Exam II","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 14 Extra Credit   What to do for Week 141   eCampus?  Description  Location         If you are interested, take a look at how statistics can be used to probabilistically determine sentiments from open text.  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes has posted.  Link       For grading.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2799d41736b286e605dbae19e4f66e87","permalink":"/due/15-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/15-due/","section":"due","summary":"What to do for Week 14 Extra Credit   What to do for Week 141   eCampus?  Description  Location         If you are interested, take a look at how statistics can be used to probabilistically determine sentiments from open text.","tags":null,"title":"Sentiment Analysis","type":"docs"},{"authors":null,"categories":null,"content":"   R Walkthrough   R Walkthrough Take a look at a fun walkthrough about applying statistics and machine learning to perform a sentiment analysis.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eba25e9e890c63b7e08cf49cf2816105","permalink":"/lesson/15-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/15-lesson/","section":"lesson","summary":"   R Walkthrough   R Walkthrough Take a look at a fun walkthrough about applying statistics and machine learning to perform a sentiment analysis.\n ","tags":null,"title":"Sentiment Analysis","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 1 after class ends What to do for Week 2 before coming to class   What to do for Week 1 after class ends1   eCampus?  Description  Location         Start a module in Data Camp that addresses the basic elements of R  Link       Complete problems 3, 4, 6, 7, and 8 in Chapter 1.  Link     \n What to do for Week 2 before coming to class2   Area  Description  Location         Read chapter 2 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on Social Explorer and the WVU Libraries Database.  Link       Watch a video on Data Visualization.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ac82a4ccda585b1b2dd58a008ca65ad","permalink":"/due/01-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/01-due/","section":"due","summary":"What to do for Week 1 after class ends What to do for Week 2 before coming to class   What to do for Week 1 after class ends1   eCampus?","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Nothing!   Nothing! There are no examples this week. Go do something!\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"059bb398e999a9d10b388c3df2b5644f","permalink":"/example/01-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/01-example/","section":"example","summary":"   Nothing!   Nothing! There are no examples this week. Go do something!\n ","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes   Data Camp The first Data Camp module is due the Wednesday following the completion of the Week 1 class. It provides an introduction to the structure in and elementary use of R with an end goal of getting you familiar with its functionality. In particular the sections address:\n Intro to basics Vectors Matrices Factors Data frames Lists  As noted last week, there is a lot on there but please refer to the syllabus for the grading policy regarding those tasks. Just remember, this is not a programming class!\n Book Materials Download the textbook\n Chapter 1 PowerPoint Chapter 1 Lecture Notes.   Class Notes Download the in-class slides via\n PDF and Outline.   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2248ef62e0fddbf88b2832cdafb38b9a","permalink":"/lesson/01-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/01-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes   Data Camp The first Data Camp module is due the Wednesday following the completion of the Week 1 class. It provides an introduction to the structure in and elementary use of R with an end goal of getting you familiar with its functionality.","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Get Some Advice Review Syllabus Book Stuff   Get Some Advice First things first: Read the syllabus. Far be it from us to disagree with Snoop, so you should probably go read the syllabus as soon as possible!\n Review Syllabus Review the overviews of the other areas as well: content, lessons, examples, and assignments pages for this class.\n Book Stuff Read Chapter 1: The What and the Why of Statistics in Frankfort-Nachmias and Leon-Guerrero.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ca270d2bbdb77aeeab9c41859c1ca7a","permalink":"/readings/01-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/01-readings/","section":"readings","summary":"Get Some Advice Review Syllabus Book Stuff   Get Some Advice First things first: Read the syllabus. Far be it from us to disagree with Snoop, so you should probably go read the syllabus as soon as possible!","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 2 after class ends What to do for Week 3 before coming to class   What to do for Week 2 after class ends1   eCampus?  Description  Location         Start a module in Data Camp that addresses some essential tidyverse commands.  Link       Review the R Walkthrough on the importance of data frames.  Link       Complete problems 1, 2, 3, 4, 5, 6 and 7 in Chapter 2.  Link     \n What to do for Week 3 before coming to class2   Area  Description  Location         Read chapter 3 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on the Measures of Central Tendency.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac09a96a006ea325c7f2e74716cc60ae","permalink":"/due/02-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/02-due/","section":"due","summary":"What to do for Week 2 after class ends What to do for Week 3 before coming to class   What to do for Week 2 after class ends1   eCampus?","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics Social Explorer   Crash Course Statistics1 Crash Course Statistics is a freely available channel on YouTube which covers a range of topics in..well statistics. While I encourage you to watch all of their videos, the ones that really applies to Chapter 2 is one of two discussions on visualizations which you can see below.\n \n Social Explorer Social Explorer is a paid site that allows people to visualize public data sets pretty easily and without the need for coding anything. Luckily WVU has a site license for the site but it can be a bit difficult to locate. Follow the video below to get an idea of Social Explorer and the WVU Database.\n     Most weeks I’ll post some external resources that may help you get the overall idea of what we are covering. These are optional, but the presentations are generally engaging and the visuals are created by people who are more talented than me so it may be worth your time.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"/example/02-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"Crash Course Statistics Social Explorer   Crash Course Statistics1 Crash Course Statistics is a freely available channel on YouTube which covers a range of topics in..well statistics.","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough   Data Camp The second Data Camp module is due the Wednesday following the completion of the Week 2 class. It covers the tidyverse family of packages in R which we will be using throughout the remainder of the course. In particular the sections address:\n Data wrangling1 Data visualization Grouping and summarizing Types of visualizations   Book Materials Download the textbook\n Chapter 2 PowerPoint and Chapter 2 Lecture Notes.   Class Notes Download the in-class slides via\n PDF Outline   R Walkthrough First click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download a PDF of the presentation above.\n  Please get used to this term!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"911ebe9376618b75c6ca9ac02110b8b9","permalink":"/lesson/02-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/02-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough   Data Camp The second Data Camp module is due the Wednesday following the completion of the Week 2 class.","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 2: The Organization and Graphic Presentation of Data in Frankfort-Nachmias and Leon-Guerrero.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"510696b7bb47e51213a79e8cd09e243d","permalink":"/readings/02-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/02-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 2: The Organization and Graphic Presentation of Data in Frankfort-Nachmias and Leon-Guerrero.\n ","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 3 after class ends What to do for Week 4 before coming to class   What to do for Week 3 after class ends1   eCampus?  Description  Location         Complete a module in Data Camp focused on arguably the most important tidyverse package: dplyr.  Link       Go over an R Walkthrough on distributions and measures of central tendency.  Link       Complete problems 1, 2, 3, 4, 5, 6, 10, and 11 in Chapter 3.  Link     \n What to do for Week 4 before coming to class2   Area  Description  Location         Read chapter 4 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on the Measures of Variability.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2a9ebfd3c92be577bc4d52f8871ae75c","permalink":"/due/03-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/03-due/","section":"due","summary":"What to do for Week 3 after class ends What to do for Week 4 before coming to class   What to do for Week 3 after class ends1   eCampus?","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Central Tendency. Think about the terms, in that something wants to be gauged towards the middle. But as you’ll learn soon, the middle isn’t always where you think it is!\n \nRemember you can always access a Crash Course Statistics playlist using the following link.\n  --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"73664027ae41c739f0c70a62c901a4c5","permalink":"/example/03-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/03-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Central Tendency.","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough   Data Camp The third Data Camp module is due the Wednesday following the completion of the Week 3 class. It covers a particular tidyverse package called dplyr which is by far the one of the most useful tools you can use. In particular the sections address:\n Transforming Data with dplyr Aggregating Data Selecting and Transforming Data Case Study: The babynames Dataset  This now sets us up to do meaningful statistics in R.\n Book Materials Download the textbook\n Chapter 3 PowerPoint Chapter 3 Lecture Notes   Class Notes Download the class\n PDF and Outline.   R Walkthrough Download the data sets1 and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.   Week 3 Walkthrough Materials  Click on the presentation itself and then you may   Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download a PDF of the presentation above.\n  You will have to unzip this file. If you are unfamilair with this process, please check the Unzipping files section under Resources for assistance.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e88a62444161c21a7f4779be93acbf33","permalink":"/lesson/03-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/03-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough   Data Camp The third Data Camp module is due the Wednesday following the completion of the Week 3 class.","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 3: Measures of Central Tendency in Frankfort-Nachmias and Leon-Guerrero.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4e1bd317fe84602507de5f25991f630a","permalink":"/readings/03-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/03-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 3: Measures of Central Tendency in Frankfort-Nachmias and Leon-Guerrero.\n ","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 4 after class ends What to do for Week 5 before coming to class   What to do for Week 4 after class ends1   eCampus?  Description  Location         Submit the first quiz.2  Link       Complete a module in Data Camp focused on basic descriptive statistics and an introduction to probability.  Link       Complete problems 1, 2, 3, 7, 8, 12, and 13 in Chapter 4.  Link     \n What to do for Week 5 before coming to class3   Area  Description  Location         Read chapter 5 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on The Normal Distribution.4  Link       For grading.↩︎\n Note: Due this Friday↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4bca9f280d6851de4bfe166681764e5c","permalink":"/due/04-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/04-due/","section":"due","summary":"What to do for Week 4 after class ends What to do for Week 5 before coming to class   What to do for Week 4 after class ends1   eCampus?","tags":null,"title":"Measures of Variability \u0026 Quiz 1","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Spread. You may be on point if you think spread is referring to the dispersion of data.\n \nIf you are feeling up to it and want to know more about how data spread and Distributions relate to each other, take a look at this one too.\n \nRemember you can always access a Crash Course Statistics playlist using the following link.\n  --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"/example/04-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Spread.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough - Doing the Class Notes   Data Camp The fourth Data Camp module is due the Wednesday following the completion of the Week 4 class. The module covers doing basic descriptive statistics in R out of the box1. In particular, the sections are:\n Variables Histograms and Distributions Scales of Measurement Measures of Central Tendency Measures of Variability  As promised, we start performing basic statistical analyses in R.\n Book Materials Download the textbook\n Chapter 4 PowerPoint Chapter 4 Lecture Notes   Class Notes Posted after class\n PDF and Outline.   R Walkthrough - Doing the Class Notes Posted after class\n You can download a [PDF](/slides/Week 4/Slides-Week-4.pdf){target=\"_blank\"} of the presentation above. --   Mostly without any additional packages or what is known as Base R↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a003ab3247913ac8f1034d05b4692ee","permalink":"/lesson/04-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/04-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough - Doing the Class Notes   Data Camp The fourth Data Camp module is due the Wednesday following the completion of the Week 4 class.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 4: Measures of Variability1 in Frankfort-Nachmias and Leon-Guerrero.\n  Remember to please take this sections slow and to pay particular attention to the ideas of standard deviation and variance. These are essentially “gateway” concepts which is to say your ability to understand the remaining concepts in the course hinges on really knowing what both of these are, what they measure, and how they are found. No pressure!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8dd4002cb4ac647f78e11eaaa49febed","permalink":"/readings/04-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/04-readings/","section":"readings","summary":"Book Stuff   Book Stuff Read Chapter 4: Measures of Variability1 in Frankfort-Nachmias and Leon-Guerrero.\n  Remember to please take this sections slow and to pay particular attention to the ideas of standard deviation and variance.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 5 after class ends What to do for Week 6 before coming to class   What to do for Week 5 after class ends1   eCampus?  Description  Location         Complete a module in Data Camp focused on basic descriptive statistics and an introduction to probability.  Link       Go through the NFL and Pipes R walkthrough  Link       Complete problems 1, 2, 5, 6, 7, 8, 11 and 12 in Chapter 5.  Link     \n What to do for Week 6 before coming to class2   Area  Description  Location         Read chapter 6 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch videos on Experimental Designs and Sampling Methods.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a231b77af4723575ec2f6a55e71ed37d","permalink":"/due/05-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/05-due/","section":"due","summary":"What to do for Week 5 after class ends What to do for Week 6 before coming to class   What to do for Week 5 after class ends1   eCampus?","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics on the Bell Curve, or what is formally known as The Normal Distribution.\n \nRemember you can always access a Crash Course Statistics playlist using the following link.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"/example/05-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics on the Bell Curve, or what is formally known as The Normal Distribution.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class. The module covers types of data sampling, and experiments. In particular, the sections are:\n Language of data Study types and cautionary tales Sampling strategies and experimental design Case study   Book Materials Download the textbook\n Chapter 5 PowerPoint Chapter 5 Lecture Notes   Class Notes Download the class\n PDF and Outline.   R Walkthrough Take a look at a walkthrough about using NFL data and pipes from tidyverse.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aa54929a56a6d237dc8692bd2c9cd024","permalink":"/lesson/05-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/05-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 5: The Normal Distribution\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"face5aa80a48f6b904e9856a6d9c7340","permalink":"/readings/05-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/05-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 5: The Normal Distribution\n ","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have two videos from Crash Course Statistics. The first is one on Experimental Designs which may benefit many of you after the conclusion of this course.\n \nThe second is on Sampling which focuses more on the chapter elements. Sampling is one of the most important aspects of any quantitative study. If you underestimate a sample (aka undersample), then a whole study can become useless. If you overestimate a sample (aka oversample), then you’ll be doing a bunch of extra work for no reason.\n \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"/example/06-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have two videos from Crash Course Statistics. The first is one on Experimental Designs which may benefit many of you after the conclusion of this course.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough: NFL Ticket Prices   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class. The module covers types of data sampling, and experiments. In particular, the sections are:\n Language of data Study types and cautionary tales Sampling strategies and experimental design Case study   Book Materials Download the textbook\n Chapter 6 PowerPoint Chapter 6 Lecture Notes   Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation above. an Outline.   R Walkthrough: NFL Ticket Prices Download the data sets1 and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.   Week 6 Walkthrough Data Set  Click on the presentation itself and then you may   Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download\n a PDF of the presentation above. a script file of just the R chunks used in the presentation.    You will have to unzip this file. If you are unfamilair with this process, please check the Unzipping files section under Resources for assistance.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4cd52ded401299a83d62039ff42e8b4","permalink":"/lesson/06-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/06-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough: NFL Ticket Prices   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"48122284ea46294df4577acb15f0803c","permalink":"/readings/06-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/06-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 7 after class ends What to do for Week 8 before coming to class Extra Credit   What to do for Week 7 after class ends1   eCampus?  Description  Location         Take a look at the coding chunks page that may help you on the upcoming exam.  Link       Complete problems 1, 2, 3, 4, 5, 6, 7, 8, and 9 in Chapter 6.  Link       Is reality probabilistic or deterministic? Figure out what kind of statistical thinking you have.  Link     \n What to do for Week 8 before coming to class2   Area  Description  Location         Being reviewing Chapters 1-6 and material on Data Camp/Walkthroughs for the exam.  Everywhere       Watch videos on the differences between Bayesian and Frequentist Statistics.3  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes has posted.  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fa05ee7a5ec9b3ad5001fe63b7e34625","permalink":"/due/07-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/07-due/","section":"due","summary":"What to do for Week 7 after class ends What to do for Week 8 before coming to class Extra Credit   What to do for Week 7 after class ends1   eCampus?","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"    Crash Course Statistics Warning A Line is Drawn Full Disclosure on My Unintended Bias A Mostly Non Stats Example A Table of Differences So Who is Right?   Crash Course Statistics This week we’re doing something a little different. We have two videos from Crash Course Statistics and another from a channel called Veritasium which all deal with the idea of probability. But you’ll soon see that there are two types of statisticians who fundamentally disagree on how probability explains the world around them.\n Warning There’s going to be some philosophical ideas discussed here. Get through it and you may figure out what kind of statistics you would practice. Why does it matter? Oh only that it may define how you not only practice statistics, but also explain how you view and deal with daily problems. No big deal.\n   A Line is Drawn1 Statisticians and those who practice statistics are at odds on how the world works. One group - the frequentists - think deductively and see probability as a means for finding a single defined outcome while another - the Bayesians - think inductively and use probability to describe the chance of many possible outcomes.\n Full Disclosure on My Unintended Bias I am a Bayesian statistician through and through so there is an inherent bias in this writing even though I did my best to stay neutral. Hopefully its small and not detectable. But now you have an inherent bias having read this. What if I had not written this disclosure?\nFrequentist Statisticians Statisticians who view the world as deterministic, do not include subjectivity, and see probabilities as a way to explain how random events would look after a bunch of trials are known as frequentist statisticians. To show this through a statistical lens, let’s look at the traditional coin flipping example: As a frequentist statistician, we would\nfirst suppress any prior ideas of how the outcome should look; then flip a coin over and over and record the results; and find that after enough flips that while we will likely never get 50-50 odds, the data shows that we’re heading that way so with the idea if we flipped that coin an infinite number of times, that’s the true outcome.  This is the idea of something being deterministic where probabilities are used to describe a fixed value which in this case is always going to be 50-50. For any social scientists, the epistemological perspective is that frequentist for the most part believe in a single truth.\n Bayesian Statisticians Statisticians who view the world as probabilistic, allow for prior beliefs about a phenomena, and update the probability of those beliefs with new evidence are known as Bayesian statisticians. To again show this through a statistical lens, let’s look at the traditional coin flipping example: As a Bayesian statistician, we would\nfirst have a prior belief of what the probability of getting a heads or tails is say 50-50. then flip a coin over and over and record the results; and find that after enough flips that we will never get to the 50-50 odds implying that there are multiple possible outcomes, each with its own associated probability.  This is the idea of something being deterministic where probabilities are used to describe multiple values which in this case may be 50-50, but could also be 40-60, 60-40, 20 - 80 and so on, each associated with some chance of being true. For any social scientists, the epistemological perspective is that Bayesian for the most part believe in multiple truths with some more likely than others.\n  A Mostly Non Stats Example Rather than provide an example that uses a bunch of statistics, let’s look at it practically.\nSituation: You have misplaced your iPhone somewhere in your apartment or home. You can use the Find My app ® to find it and hear beeping.\nProblem: Which room in your apartment or home should you search?\nApproaches\n Frequentist: You hear the phone beeping and you have some approach using a mental model to help figure out what room the beeping is coming from. So you used inferences from the beeps to locate the room in your home you must search to find the phone.\n Bayesian: You hear the phone beeping and along with some approach using a mental model, you also know all of the rooms you have misplaced the phone before which combined help to figure out what room the beeping is coming from. So you used inferences from the beeps and prior knowledge  to locate the room in your home you must search to find the phone.\n   A Table of Differences   Type  Information Used  What is Random?  Type of Reasoning  Terminology  Observed Data      Frequentist statistician  Outcomes derived strictly from experiments  Observed data2  Deductive logic.  Common terms like p-value, significant, null hypothesis, and confidence interval.  Unknown and comes only from experiments.    Bayesian statistician  Prior beliefs about what the truth might be which are updated as experiments progress.  Population parameters3  Inductive logic.  Uncommon terms like prior probability, noninformative priors, and credible intervals.  Known since we already know what we know.      So Who is Right? We honestly just don’t know right now. Similar to physics where there are two seemingly incompatible laws governing the universe - relativity for large and quantum for small objects, respectively - statistics has the same difficulty regarding inferences. The frequentist thinking is by far the dominant approach because it involves fairly “simple” and concrete calculations that can be tested and verified when explaining the phenomena around us. In fact, the approach aligns with the idea of certainty that humans like and need to make sense of the world. However, humans also live with some level of uncertainty everyday in everything we do. Until recently testing these ideas were difficult but with the rise of machine learning and advancements in general computing4, complex calculations allow us to consider the possibilities of multiple outcomes for a given problem. This then necessitates the need for a Bayesian point of view.\nThe actual explanation is likely a combination of both approaches but there could also be another method that we have not considered yet. Just because both seem to work doesn’t mean either are correct.\nFrequentist Statistics  \n Bayesian Statistics  \n Bayes Theorem  \n   Note that what is described is an oversimplification and there are multiple differences between the two viewpoints.↩︎\n Any data that has been collected from an experiment.↩︎\n Any summary number, like an average or percentage, that describes the entire population.↩︎\n Fun fact: The fact you are able to view this exact page right now is based on based on probability↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"/example/07-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Crash Course Statistics Warning A Line is Drawn Full Disclosure on My Unintended Bias A Mostly Non Stats Example A Table of Differences So Who is Right?","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough: An Incomplete Review of Descriptive Statistics Commands in R Kaggle Other Sites   Data Camp The fifth Data Camp module is due. The module covers types of data sampling, and experiments. In particular, the sections are:\n Language of data Study types and cautionary tales Sampling strategies and experimental design Case study   Book Materials Download the textbook\n Chapter 6 PowerPoint Chapter 6 Lecture Notes   Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation. a script file of just the R chunks used in the presentation.   R Walkthrough: An Incomplete Review of Descriptive Statistics Commands in R Click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download\n a PDF of the presentation above. a script file of just the R chunks used in the presentation.   Kaggle Kaggle1 is an online learning community for data scientists and machine learning practitioners2 That’s great but its greatest benefit are the numerous tutorials/walkthroughs provided and also to locate data sets3. In an upcoming data task, you’ll be asked to locate a data set of interest to perform some analysis and Kaggle is a great place to start so if you get a chance, sign up for free and look around. However for now, there is an excellent dplyr walkthrough that was posted recently and may be helpful to you for the exam.\n Other Sites Head over to the Data page under Resources to find more outlets!\n  Subsidiary of Google.↩︎\n Stolen from Wikipedia.↩︎\n All data sets are submitted by people so while its fun for window shopping, if possible find the original source. Typically users will list where their posted data sets can be found publicly↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"36f453f698c7a3972914c894cffd917b","permalink":"/lesson/07-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/07-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough: An Incomplete Review of Descriptive Statistics Commands in R Kaggle Other Sites   Data Camp The fifth Data Camp module is due.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"52401e1c7f6b8dc680826870d85b28f1","permalink":"/readings/07-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/07-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 7 after class ends What to do for Week 8 before coming to class Extra Credit   What to do for Week 7 after class ends1   eCampus?  Description  Location         Submit the Data portion of the first exam.  Link       Submit the Paper portion of the first exam.  Link     \n What to do for Week 8 before coming to class2   Area  Description  Location         Being reviewing Chapters 1-6 and material on Data Camp/Walkthroughs for the exam.  Everywhere       Watch videos on the differences between Bayesian and Frequentist Statistics.3  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes has posted.  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"38eafb8fcf772701f82fe65edf8315f8","permalink":"/due/08-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/08-due/","section":"due","summary":"What to do for Week 7 after class ends What to do for Week 8 before coming to class Extra Credit   What to do for Week 7 after class ends1   eCampus?","tags":null,"title":"Exam I","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 9 after class ends What to do for Week 10 before coming to class Extra Credit   What to do for Week 9 after class ends1   eCampus?  Description  Location         Complete problems 1, 2, 3, 4, 5, and 10 in Chapter 7.  Link       Complete problems 1, 2, 3, 4, and 5 in Chapter 8.  Link       Take a look at the R Walkthrough on Confidence Intervals.  Link     \n What to do for Week 10 before coming to class2   Area  Description  Location         Read chapter 7 and 8 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch videos on Hypothesis Testing and Confidence Intervals.  Link      Extra Credit   Area  Description  Location         An extra credit task on using Rmarkdown for reporting purposes is still available.  Link       For grading. Due in two weeks.↩︎\n For preparation.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7c9980c03e532205c3dcaf872e7e3208","permalink":"/due/09-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/09-due/","section":"due","summary":"What to do for Week 9 after class ends What to do for Week 10 before coming to class Extra Credit   What to do for Week 9 after class ends1   eCampus?","tags":null,"title":"Estimations","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics Power and Significance Election Season and Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics on Confidence Intervals and a great introduction to the p-value.\n  Remember you can always access a Crash Course Statistics playlist using the following link.\n Power and Significance If you like interactive things and want to see how Type I \u0026amp; Type II errors, \\(\\alpha\\), \\(\\beta\\), p-values, power and effect sizes all play together, try Kristoffer Magnusson’s wonderful page Understanding Statistical Power and Significance Testing: an interactive visualization.\n Election Season and Statistics If you want to see inferential statistics in action, give a look at FiveThirtyEight 2020 where researchers use polling and regional data to forecast the upcoming elections.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4ca8d8f57d9585dcab132a17e9a5a6e7","permalink":"/example/09-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/09-example/","section":"example","summary":"Crash Course Statistics Power and Significance Election Season and Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics on Confidence Intervals and a great introduction to the p-value.","tags":null,"title":"Estimations","type":"docs"},{"authors":null,"categories":null,"content":"   In Class Notes Textbook Class Notes R Walkthrough: Confidence Intervals   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download\n a PDF of the presentation.   Textbook Class Notes Download the textbook\n Chapter 7 PowerPoint Chapter 7 Lecture Notes Chapter 8 PowerPoint and Chapter 8 Lecture Notes   R Walkthrough: Confidence Intervals Click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download\n a PDF of the presentation above. a script file of just the R chunks used in the presentation.   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9ba3922ca694430b88b632d8dbab3bdb","permalink":"/lesson/09-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/09-lesson/","section":"lesson","summary":"In Class Notes Textbook Class Notes R Walkthrough: Confidence Intervals   In Class Notes  Download the class slides: Remember to click on the presentation itself and then you may","tags":null,"title":"Estimations","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff  Read Chapter 7: Estimation Read Chapter 8: Testing Hypothesis   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ad5bf04de635c6e8453fd253fbcbbb24","permalink":"/readings/09-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/09-readings/","section":"readings","summary":"   Book Stuff   Book Stuff  Read Chapter 7: Estimation Read Chapter 8: Testing Hypothesis   ","tags":null,"title":"Estimations","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to whatever you want. The ones below are some of the more larger repositories:\n Data World: A plethora of open data sets to peruse. If you are a data fiend, consider collaborating to solve problems.\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to whatever you want. The ones below are some of the more larger repositories:\n Data World: A plethora of open data sets to peruse.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"    Week 10 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; } li { margin-left: 1.5em; }  Week 10 Problem Set Assignment  Chapter 7 Exercises: 1, 2, 3, 4, 5, 10. Chapter 8 Exercises: 1, 2, 3, 4, 5.  Please turn both in as one task within the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions Chapter 7  1  The estimate at the 90% confidence level is 20.87–21.13%. This means that there are 90 chances out of 100 that the confidence interval will contain the true population percentage of victims in the American population. Due to the large sample size, we converted the proportions to percentages, subtracting from 100, rather than 1. \\[\\begin{align} s_p \u0026amp;= \\sqrt{\\dfrac{21\\cdot(100-21)}{239541}}\\\\ \u0026amp;\\approx 0.08 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 21\\pm 1.65\\cdot0.08\\\\ \u0026amp;= 21\\pm 0.13\\\\ \u0026amp;= (20.87, 21.13) \\end{align}\\] So the standard error of the mean is approximately 0.08 with a confidence interval between 20.87 to 21.13.\n The true percentage of crime victims in the American population is somewhere between 20.79% and 21.21% based on the 99% confidence interval. There are 99 chances out of 100 that the confidence interval will contain the true population percentage of crime victims. \\[\\begin{align} CI \u0026amp;= 21\\pm 2.58\\cdot0.08\\\\ \u0026amp;= 21\\pm 0.21\\\\ \u0026amp;= (20.79, 21.21) \\end{align}\\] So we have a confidence interval between 20.79 to 21.21.\n    2  For lower-class respondents, we have \\[\\begin{align} S_{\\overline{Y}} \u0026amp;=\\dfrac{3.08}{\\sqrt{102}}\\\\[0.5ex] \u0026amp;\\approx 0.30 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 12.19\\pm 1.96\\cdot0.30\\\\ \u0026amp;= 12.19\\pm 0.59\\\\ \u0026amp;= (11.60, 12.78) \\end{align}\\] So the estimated standard error for the sampling distribution is approximately 0.30 with a confidence interval between 11.60 to 12.78.\nFor working-class respondents, we have \\[\\begin{align} S_{\\overline{Y}} \u0026amp;=\\dfrac{2.93}{\\sqrt{523}}\\\\[0.5ex] \u0026amp;\\approx 0.13 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 13.16\\pm 1.96\\cdot0.13\\\\ \u0026amp;= 13.16\\pm 0.25\\\\ \u0026amp;= (12.91, 13.41) \\end{align}\\] So the estimated standard error for the sampling distribution is approximately 0.13 with a confidence interval between 12.91 to 13.41.\n For lower-class respondents, we have \\[\\begin{align} CI \u0026amp;= 12.19\\pm 2.58\\cdot0.13\\\\ \u0026amp;= 12.19\\pm 0.77\\\\ \u0026amp;= (11.42, 12.96) \\end{align}\\] So the confidence interval is between 11.42 to 12.96.\nFor working-class respondents, we have \\[\\begin{align} CI \u0026amp;= 14.60\\pm 2.58\\cdot0.13\\\\ \u0026amp;= 14.60\\pm 0.34\\\\ \u0026amp;= (14.26, 14.94) \\end{align}\\] So the confidence interval is between 14.26 to 14.94.\n As our confidence level increases, the confidence interval gets wider, not narrower. This is because a wider interval is needed to increase the probability that our calculated interval includes the true population value. Thus, increasing confidence leads to less precise intervals.\n    3  For Canadians we have \\[\\begin{align} s_p \u0026amp;= \\sqrt{\\dfrac{0.51\\cdot(1-0.51)}{1004}}\\\\ \u0026amp;\\approx 0.02 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 0.51\\pm 1.96\\cdot0.02\\\\ \u0026amp;= 0.51\\pm 0.04\\\\ \u0026amp;= (0.47, 0.55) \\end{align}\\] So the estimated standard error of proportions is approximately 0.02 with a confidence interval between 0.47 to 0.55.\n For Americans we have \\[\\begin{align} CI \u0026amp;= 0.45\\pm 1.96\\cdot0.02\\\\ \u0026amp;= 0.45\\pm 0.04\\\\ \u0026amp;= (0.39, 0.49) \\end{align}\\] implying a confidence interval between 0.39 to 0.49.\n Based upon the calculated 95% confidence interval, the majority of Americans do not believe climate change is a serious problem. The true percentage of Americans who believe climate change is a serious problem is under 50%, somewhere between 39% and 49%, based upon this Pew Research Center sample. On the other hand, it is possible that the majority of Canadians believe climate change is a serious problem. We can be 95% confident that the true percentage of Canadians is somewhere between 47% and 55%.\n    4  We have a 90% confidence interval for the males by calculating \\[\\begin{align} s_p \u0026amp;= \\sqrt{\\dfrac{0.21\\cdot(1-0.21)}{337}}\\\\ \u0026amp;\\approx 0.02 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 0.21\\pm 1.65\\cdot0.02\\\\ \u0026amp;= 0.21\\pm 0.03\\\\ \u0026amp;= (0.18, 0.24) \\end{align}\\] implying an estimated standard error of proportions is approximately 0.02 with a confidence interval between 0.18 to 0.24.\n We have a 90% confidence interval for the demales by calculating \\[\\begin{align} s_p \u0026amp;= \\sqrt{\\dfrac{0.37\\cdot(1-0.37)}{441}}\\\\ \u0026amp;\\approx 0.02 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 0.37\\pm 1.65\\cdot0.02\\\\ \u0026amp;= 0.37\\pm 0.03\\\\ \u0026amp;= (0.34, 0.40) \\end{align}\\] implying an estimated standard error of proportions is approximately 0.02 with a confidence interval between 0.34 to 0.40.\n    5  Due to the large sample size, we converted the proportion to full percentages, subtracting from 100 rather than 1. \\[\\begin{align} SE \u0026amp;= \\sqrt{\\dfrac{51\\cdot(100-51)}{5490}}\\\\ \u0026amp;\\approx 0.67 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 51\\pm 1.96\\cdot0.67\\\\ \u0026amp;= 51\\pm 0.13\\\\ \u0026amp;= (49.69, 52.31) \\end{align}\\] We set the interval at the 95% confidence level. However, no matter whether the 90%, 95%, or 99% confidence level is chosen, the calculated interval includes values below 50% for the vote for a Republican candidate. Therefore, you should tell your supervisors that it would not be possible to declare a Republican candidate the likely winner of the votes coming from men if there was an election today because it seems quite possible that less than a majority of male voters would support her/him.\n   10  For Bernie Sanders we have \\[\\begin{align} s_p \u0026amp;= \\sqrt{\\dfrac{0.55\\cdot(1-0.55)}{1754}}\\\\ \u0026amp;\\approx 0.01 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 0.55\\pm 1.65\\cdot0.01\\\\ \u0026amp;= 0.55\\pm 0.02\\\\ \u0026amp;= (0.53, 0.57) \\end{align}\\] implying an estimated standard error of proportions is approximately 0.02 with a confidence interval between 0.53 to 0.57.\nFor Hillary Clinton we have \\[\\begin{align} s_p \u0026amp;= \\sqrt{\\dfrac{0.38\\cdot(1-0.38)}{1754}}\\\\ \u0026amp;\\approx 0.01 \\end{align}\\] and \\[\\begin{align} CI \u0026amp;= 0.38\\pm 1.65\\cdot0.01\\\\ \u0026amp;= 0.38\\pm 0.02\\\\ \u0026amp;= (0.36, 0.40) \\end{align}\\] implying an estimated standard error of proportions is approximately 0.02 with a confidence interval between 0.36 to 0.40.   Chapter 8 Note: The \\(z\\) value obtained will be referenced by the variable \\(z_{obt}\\).\n 1  We have \\[\\begin{align} H_0:\\,\\mu_Y=13.5\\,\\,\\text{years}\\\\ H_1:\\,\\mu_Y\u0026lt;13.5\\,\\,\\text{years} \\end{align}\\]\n We have \\[\\begin{align} z \u0026amp;= \\dfrac{10.9-13.5}{7.6/\\sqrt{150}}\\\\[0.5ex] \u0026amp;\\approx -4.19 \\end{align}\\] yielding that \\(z\\) value, or \\(z_{obt} = -4.19\\). The \\(p\\)-value for this \\(z_{obt} \u0026lt; 0.001\\) for a one-tailed test where \\(\\alpha = 0.01\\). Since \\(z_{obt} \u0026lt; 0.001\\) and \\(0.001 \u0026lt; \\alpha\\), we reject \\(H_0\\) and can likely assume that the doctors at the HMO do have less experience than the population of doctors at all HMOs.\n    2  We have\n\\[\\begin{align} s_{p_1-p_2} \u0026amp;=\\sqrt{\\dfrac{0.43\\cdot(1-0.43)}{899}+\\dfrac{0.50\\cdot(1-0.50)}{351}}\\\\[0.5ex] \u0026amp;= \\sqrt{0.0313}\\\\ \u0026amp;\\approx 0.03 \\end{align}\\] implying \\[\\begin{align} z \u0026amp;= \\dfrac{0.43-0.50}{0.03}\\\\[0.5ex] \u0026amp;\\approx -2.33 \\end{align}\\] so \\(z_{obt} = -2.33\\) and the probability of obtaining this \\(z\\) statistic is \\(0.0099\\cdot 2=0.0198\\) which is less than \\(\\alpha = 0.01\\). So we reject \\(H_0\\) and can likely assume there is a significant difference in the proportion of homeowners between first-generation and second-generation Hispanic Americans. In fact given we have \\(0.43-0.50=-0.07\\), there is a lower proportion of home ownership among first-generation Hispanic Americans than second-generation Hispanic Americans.\n We have\n\\[\\begin{align} s_{p_1-p_2} \u0026amp;=\\sqrt{\\dfrac{0.58\\cdot(1-0.58)}{2684}+\\dfrac{0.51\\cdot(1-0.51)}{566}}\\\\[0.5ex] \u0026amp;= \\sqrt{0.0229}\\\\ \u0026amp;\\approx 0.02 \\end{align}\\] implying \\[\\begin{align} z \u0026amp;= \\dfrac{0.58-0.51}{0.02}\\\\[0.5ex] \u0026amp;\\approx 3.50 \\end{align}\\] so \\(z_{obt} = 3.50\\) and the probability of obtaining this \\(z\\) statistic is \\(0.002\\cdot 2=0.0004\\) which is less than \\(\\alpha = 0.01\\). So we reject \\(H_0\\) and can likely assume there is a significant difference in the proportion of homeowners between first-generation and second-generation Asian Americans. In fact given we have \\(0.58-0.51=0.07\\), there is a higher proportion of home ownership among first-generation Hispanic Americans than second-generation Hispanic Americans.\n    3    Item  Test  Research Hypothesis  Null Hypothesis      \\(\\text{a}\\)  Two-tailed  \\(\\mu\\neq53657\\)  \\(\\mu=53657\\)    \\(\\text{b}\\)  One-tailed  \\(\\mu\u0026gt;3.2\\)  \\(\\mu=3.2\\)    \\(\\text{c}\\)  One-tailed  \\(\\mu_1\u0026lt;\\mu_2\\)  \\(\\mu_1=\\mu_2\\)    \\(\\text{d}\\)  Two-tailed  \\(\\mu_1\\neq\\mu_2\\)  \\(\\mu_1=\\mu_2\\)    \\(\\text{e}\\)  One-tailed  \\(\\mu_1\u0026gt;\\mu_2\\)  \\(\\mu_1=\\mu_2\\)    \\(\\text{f}\\)  One-tailed  \\(\\mu_1\u0026lt;\\mu_2\\)  \\(\\mu_1=\\mu_2\\)         4  We have \\[\\begin{align} H_0:\\,\\pi_1=\\pi_2\\\\ H_1:\\,\\pi_1\u0026lt;\\pi_2 \\end{align}\\]\n We have \\[\\begin{align} s_{p_1-p_2} \u0026amp;=\\sqrt{\\dfrac{0.34\\cdot(1-0.34)}{1799}+\\dfrac{0.41\\cdot(1-0.41)}{1001}}\\\\[0.5ex] \u0026amp;= \\sqrt{0.0003664}\\\\ \u0026amp;\\approx 0.02 \\end{align}\\] implying \\[\\begin{align} z \u0026amp;= \\dfrac{0.34-0.41}{0.02}\\\\[0.5ex] \u0026amp;\\approx -3.50 \\end{align}\\] yielding that \\(z_{obt}=-3.50 \u0026lt;\\alpha = 0.05\\) (and less than \\(0.0002\\) from Appendix A). So the proportion of White respondents who support the tactic of bringing people of different racial backgrounds together to talk about race is lower than the proportion of Black respondents. The difference given by \\(0.41-0.34\\) is likely significant at the \\(0.05\\) level.\n    5  We have \\[\\begin{align} H_0:\\,\\mu = 37.8\\\\ H_1:\\,\\mu\\neq 37.8 \\end{align}\\]\n We have \\[\\begin{align} t \u0026amp;= \\dfrac{48.69-37.80}{17.99/\\sqrt{1495}}\\\\[0.5ex] \u0026amp;\\approx 23.17 \\end{align}\\] yielding that \\(t_{obt}=23.17\\) with \\(p \u0026lt; 0.001\\) (and is greater than \\(3.291\\) from Appendix B).\n We can say that \\(H_0\\) should likely be rejected in favor of the \\(H_1\\). So there is a difference between the mean age of the GSS sample and the mean age of all American adults. Relative to age, the GSS sample is not representative of all American adults (the GSS sample is significantly older).\n   :::\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37f2231200d5be3129c04660d99b645c","permalink":"/assignment/10-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/10-homeworks/","section":"assignment","summary":"Week 10 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; } li { margin-left: 1.","tags":null,"title":"Testing Hypotheses","type":"docs"},{"authors":null,"categories":null,"content":"    Week 11 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 11 Problem Set Assignment Chapter 9 Exercises: 1, 3, 5, 6, 7, 9. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  The independent variable is sex while the dependent variable is fear of walking alone at night.\n  Choice\n Male\n Female\n     Yes\n 2 (22%)\n 8 (73%)\n   No\n 7 (78%)\n 3 (27%)\n      Approximately \\(73\\)% of women are afraid to walk alone in their neighborhoods at night, whereas approximately \\(22\\)% of men said the same. This amounts to about a \\(51\\)% difference between women and men who are not afraid to walk alone at night, indicating a strong relationship. It is important to keep in mind that our small sample size limits the generalizability of these results.\n There does not appear to be much of a relationship between homeownership and fear of walking alone at night. The majority in both homeownership categories (\\(54\\)% and \\(55\\)%) indicated that they were not afraid of walking alone at night in their neighborhood.\n  Response\n Male\n Female\n     Yes\n 4 (44%)\n 5 (45%)\n   No\n 5 (54%)\n 6 (55%)\n         3  Based on the student’s argument, the independent variable is race\u0026lt; and the dependent variable is home ownership.\n We have \\[\\dfrac{539}{857} \\approx 0.6289\\] or about \\(63\\)%.\n There appears to be a relationship between race and home ownership. While \\(63\\)% of those surveyed report owning a home, far less Black respondents report such (\\(41.3\\)%) than White respondents (\\(67.7\\)%). Furthermore, \\(58.7\\)% of Black respondents rent as compared to \\(32.3\\)% of White respondents.\n    5  There appears to be a relationship between race and the frequency of being drunk in the last 12 months. The majority of students (\\(60\\)%) are likely to report not being drunk in the last 12 months. However, the percentage of students being drunk at least 3 or more times is highest for Whites (\\(28\\)%), followed by Hispanic (\\(16\\)%) and Black (\\(11\\)%) students.\n  Frequency  Black  White  Hispanic  Total      None  359 (75%)  1081 (55%)  480 (67%)  1920 (60%)    1-2 times  63 (13%)  342 (17%)  125 (17%)  530 (17%)    3-5 times  24 (5%)  168 (8%)  50 (7%)  242 (8%)    \u0026gt; 6 times  31 (6%)  389 (20%)  64 (9%)  484 (15%)    Total  477 (99%)  1980 (100%)  719 (100%)  3176 (100%)         6  The death penalty is more likely in cases where the victim was female or White. The percentage difference is greater between female versus male victims (\\(57.5 – 45.4 = 12.1\\)%) than White versus Black victims (\\(52.7 – 46.5 = 6.2\\)%). When considering the interactive effects of gender and race, death sentences are more likely in cases with White and Black female victims (\\(55\\)% or higher). Death sentences are least likely in cases with a Black male victim.\nWe have \\[\\dfrac{703}{766}\\approx 0.9177\\] or about \\(91.8\\)%.\n 14.\n We have \\[4 + \\dfrac{19}{384}\\approx 4.0495\\] or about \\(4.05\\)%.\n There appears to be a relationship between age and feeling of age discrimination. \\(8.2\\)% of all respondents felt discriminated against because of their age. However, those between 18 and 29 were more likely to feel discriminated against because of their age than any other age category represented in the cross-tabulation.\n  Response\n 18-29\n 30-39\n 40-49\n 50-59\n Total\n     Yes\n 26 (16%)\n 14 (6%)\n 4 (2%)\n 19 (10%)\n 63 (8%)\n   No\n 136 (84%)\n 206 (94%)\n 183 (98%)\n 178 (90%)\n 703 (92%)\n   Total\n 162 (100%)\n 220 (100%)\n 187 (100%)\n 197 (100%)\n 766 (100%)\n       7  The dependent variable is housework disagreement. \\(n = 7144\\) is the total sample size. We have \\[\\dfrac{3953}{7144} \\approx 0.5533\\] or about \\(55.33\\)%. There is a relationship between perceived unfairness of housework divisions and housework disagreement for women. \\(33.68\\)% of women reported never having housework disagreement. However, women who felt housework divisions were fair to women (\\(43.65\\)%) were more likely to report never having disagreement over housework than women who felt housework divisions were unfair to women (\\(25.62\\)%).    9  Yes, there is a relationship between political party affiliation and attitudes toward the Affordable Care Act. The majority of physicians who reported being Republican or Other party were strongly against or against ACA. The largest reporting percentage was among \\(31.08\\)% + \\(52.99\\)% \\(=\\) \\(84.07\\)% Republicans. Only \\(11.20\\)% + \\(36\\)% \\(=\\) \\(47.2\\)% of Democrats were strongly against or against the act.  :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c36ce7b7f61ec2b6f2cdd84272965531","permalink":"/assignment/11-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/11-homeworks/","section":"assignment","summary":"Week 11 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 11 Problem Set Assignment Chapter 9 Exercises: 1, 3, 5, 6, 7, 9.","tags":null,"title":"Bivariate Tables","type":"docs"},{"authors":null,"categories":null,"content":"    Week 13 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; } li { margin-left: 1.5em; }  Week 13 Problem Set Assignment Chapter 10 Exercises: 1, 2, 3, 5, 7, 13. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions Note: The \\(\\chi^2\\) value obtained will be referenced by the variable \\(\\chi^2_{obt}\\).  1  We have \\[\\begin{align} df \u0026amp;= (3-1)\\cdot(2-1)\\\\ \u0026amp;= 2 \\end{align}\\] yielding 2 degrees of freedom.\n We have \\(\\chi^2 = 3.311\\). With \\(\\chi^2_{obt} = 0.191 \u0026gt; \\alpha = 0.01\\), we fail to reject \\(H_0\\) and conclude that sex and opinion of whether or not people can be trusted are likely independent. Specifically, similar percentages of men (\\(34.94\\)%) and women (\\(29.7\\)%) felt that people can be trusted.\n If \\(\\alpha = 0.05\\), \\(\\chi^2_{obt} = 0.191\\) would still be greater and thus we would still reject \\(H_0\\).\n We have \\(\\lambda = 0.000\\) implying that there is no proportional reduction of error using sex to predict whether or not people can be trusted.\n    2  There does not appear to be a relationship between gender and views on restoring the voting rights of felons in prison. \\(30\\)% of respondents supported restoring the voting rights of felons in prison, with only a slightly greater percentage of men (\\(31\\)%) reporting such compared to women (\\(28\\)%).   Restoring Voting Rights  Men  Women  Total      Support  31%  28%  30%    Oppose  69%  71%  70%    Total  100%  99%  100%       We fail to reject \\(H_0\\) of no relationship and conclude that there is not a relationship between gender and views on restoring the voting rights of felons in prison at the \\(0.05\\) \\(\\alpha\\) level.    3  A higher percentage of White respondents (\\(38.14\\)%), compared to Black respondents (\\(23\\)%), reported that people could be trusted.\n For Black respondents, a slightly higher percentage of men (\\(15.1\\)%) compared to women (\\(12.5\\)%) reported that people could be trusted.\n For\n   Whites with \\(\\chi^2 = 2.395\\), we fail to reject \\(H_0\\). Blacks with \\(\\chi^2 = 0.233\\), we fail to reject \\(H_0\\). It is also worth noting that two of the cells have expected counts less than 5 so results should be cautiously interpreted. It would likely be beneficial to repeat the study with a larger sample size.    5  We will make 2,973 errors since we predict that all victims fall in the modal category (White) where \\[\\begin{align} E_1 \u0026amp;= 6084 – 3111\\\\ \u0026amp;= 2973 \\end{align}\\]\n For\n   White offenders, we could make 373 errors Black offenders, we could make 493 errors Other offenders, we would make 42 errors given \\(E_2 = 9084\\).  The proportional reduction in error is then \\[\\begin{align} \\dfrac{2973 – 908}{2973} \u0026amp; \\approx 0.6946 \\end{align}\\] This indicates a very strong relationship between the two variables. We can reduce the error in predicting victim’s race based upon race of offender by 69.46%.    7  We have\n   Race/Status  \\(f_0\\)  \\(f_e\\)  \\(f_0-f_e\\)  \\((f_0-f_e)^2\\)  \\(\\dfrac{(f_0-f_e)^2}{f_e}\\)      White/First-Generation  1742  1749.6  -7.6  57.76  0.03    White/Nonfirst-Generation  2392  2384.4  7.6  57.76  0.02    Black/First-Generation  102  93.5  8.5  72.25  0.77    Black/Nonfirst-Generation  119  127.5  -8.5  72.25  0.57    Native American/First-Generation  41  36.4  4.6  21.16  0.58    Native American/Nonfirst-Generation  45  49.6  -4.6  21.16  0.43    Hispanic/First-Generation  19  18.6  0.4  0.16  0.01    Hispanic/Nonfirst-Generation  25  25.4  -0.4  0.16  0.01    Asian American/First-Generation  6  11.9  -5.9  34.81  2.93    Asian American/Nonfirst-Generation  22  16.1  5.9  34.81  2.16    \\(\\chi^2 = 7.51\\)            So \\(\\chi^2 = 7.51\\) with \\(df = (2-1)\\cdot(5-1) = 4\\) implying we would fail to reject \\(H_0\\). The probability of \\(\\chi^2_{obt}\\) is between \\((0.20, 0.10)\u0026gt;\\alpha = 0.05\\).\n   13  For\n  Gender, the model is significant at the \\(0.01 \\alpha\\) level, indicating a likely significant relationship between the variables. Though males contribute to more violent onset, in proportional terms, females exhibit a higher prevalence rate, in that \\(18.32\\)% of females exhibit violent onset compared with \\(11.71\\)% of males.\n Age at first offense, the model is significant at the \\(0.01 \\alpha\\) level, indicating a likely significant relationship between age at first offense and violent onset. Violent onset is more likely among the group 14 years and older (\\(14.74\\)%) than those less than 14 years of age at first onset (\\(9.67\\)%).\n   :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f6e644d447a2306d242e70f2d2b7b31","permalink":"/assignment/13-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/13-homeworks/","section":"assignment","summary":"Week 13 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; } li { margin-left: 1.","tags":null,"title":"Chi-Square and ANOVAs","type":"docs"},{"authors":null,"categories":null,"content":"    Class Slack Account Access Week 1 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Class Slack Account Access Please sign up for Slack!\nGo to the course Slack account Register if you already do not have an account. Note that you do not have to use your WVU account, but its not a bad idea. Head to to the channel #introduce-yourself and well introduce yourself by tell everyone about yourself, what you hope to achieve out of this course, and something about you that really defines who you are. For example, here is something about me:\nI teach data visualization BUT I am also about 40% colorblind! How do I know what color to use? Take EDP 693C: Data Visualization in the Spring to find out! 1\n (optional) Provide a picture if you are willing. I am a very visual person as are many other people so pictures help a great deal. However, you are not mandated to do so!   Week 1 Problem Set Assignment Chapter 1 Exercises: 3, 4, 6, 7, and 8. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  3  Interval–ratio Interval–ratio Nominal Ordinal Nominal Interval–ratio Ordinal    4  Discrete Continuous Continuous    6  Unemployment records could be used to determine the actual number of unemployed; a descriptive statistic based upon the population. A survey is taken to estimate student opinions about the quality of food; inferential statistic. National health records can be used to determine the incidence rate of breast cancer among all Asian women, so this would be a descriptive statistic. The ratings will be gathered from a survey, so this is an inferential statistic. A university should be able to report GPA by major, so this is a descriptive statistic based upon the population. In theory, the United States records all immigrants to this country. Therefore, the number of South East Asian immigrants would be a descriptive statistic. However, because of illegal immigration, surveys are also taken to estimate the total number of legal and unauthorized immigrants. In that event, the number of immigrants would be an inferential statistic.    7  Annual income Gender: nominal\nNumber of hours worked per week: interval ratio\nYears of education: interval ratio\nJob title: nominal. This is an application of inferential statistics. The researcher is using information based on her sample to predict the annual income of a larger population of young graduates.    8  At the nominal level, a simple measure of political participation is whether or not someone voted in the most recent general election. This variable would be coded either yes or no.\nAt the ordinal level, a composite measure could be constructed of both voting and political party membership, like this:\n  Behavior  Code      Didn’t vote; No membership  0    Voted; No membership OR Membership; Didn’t vote  1    Voted; Membership  2     These codes are ordinal in scale because the amount of political participation can be ranked from high to low. Other possible ordinal variables can be constructed from other sets of behaviors, such as working in a candidate’s campaign and signing a petition. The key points are to create a variable whose values can be ranked and whose values are not on an interval–ratio scale.\nAt the interval–ratio level, political participation could be measured by the percentage of elections in which a person has voted since becoming eligible to vote, or the amount of money a person donated to political candidates during some specified time period.\n :::\n   Shameless plug↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0c685bccb51eb8b27996aa20967c387f","permalink":"/assignment/01-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/01-homeworks/","section":"assignment","summary":"Class Slack Account Access Week 1 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Class Slack Account Access Please sign up for Slack!","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"    Week 2 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 2 Problem Set Assignment Chapter 2 Exercises: 1, 2, 3, 4, 5, 6 and 7. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  Race is a nominal. variable; Since categories can be ordered, Class is an ordinal variable; Trauma is an interval variable.\n Frequency table for Race:\n     Race  Frequency      White  17    Non-White  13    Total  30      We have     Classification  Proportion      White  17/30 = 0.57    Non-White  13/30 = 0.43    Total  30/30 = 1.00        2  Frequency and Percentage Distribution Table for Class:\n   Class  Frequency  Percent      Lower  3  10    Working  15  50    Middle  11  36.7    Upper  1  3.3    Total  30  100      The smallest perceived class group is the upper class, making up only 3.3% of the survey.\n Together, the working and middle class make up 50% + 36.7% = 86.7% of the survey.\n   3  Frequency table for Traumas:\n   Number of Traumas  Frequency      0  15    1  11    2  4    Total  30      Trauma is an interval- or ratio-level variable, since it has a real zero point and a meaningful numeric scale.\n People in this survey are more likely to have experienced no traumas last year (50% of the group).\n The proportion who experienced one or more traumas is calculated by first adding 36.7% and 13.3% equaling 50% and then divide that number by 100 to obtain the proportion, 0.50, or half the group.\n    4  Unfortunately since other visualizations have not been introduced, pie charts are likely the best method approach to present these data.\n  5  Yes because 79.2% had “hardly any” confidence in the press, compared to 19.3% of those who voted for Clinton.\n  6     E-mail Hours Per Week  Frequency  Cumulative Frequency  Percent  Cumulative Percent      0  19  19  19  19    1  20  39  20  39    2  13  52  13  52    3  5  57  5  57    4  2  59  2  59    5  6  65  6  65    6  5  70  5  70    7  2  72  2  72    8  3  75  3  75    9  1  76  1  76    10 or more  23  99  23  99    Total  99    99          7  According to this time series chart, rates of voting have consistently varied by race and Hispanic origin. The group with the largest increase in voting rates is Blacks, from 53% in 1996 to 66.6% in 2012. However, in 2016, the voting rates for Blacks declined to 59.6%–the lowest it has been since the 2000 election when it was 56.9%. Hispanic voting rates have, for the most part, remained stable, although with some fluctuation most notably from the 1992 election (51.6%) to the 1996 election (44%). As noted in the exercise, in the 2012 presidential election, Blacks had the highest voting rates for all groups, followed by non-Hispanic Whites, non-Hispanic other races, and Hispanics. White voting rates increased by 1.2% from 2012 to 2016. The highest voting rate for Whites was in 1992 (70.2%), 1992 for Hispanics (51.6%), 2012 for Blacks (66.6%), and 1992 for non-Hispanic other races (54%).\n :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dbe504b058cb2e5a3ee53153a9fa3a5b","permalink":"/assignment/02-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/02-homeworks/","section":"assignment","summary":"Week 2 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 2 Problem Set Assignment Chapter 2 Exercises: 1, 2, 3, 4, 5, 6 and 7.","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"    Week 3 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 3 Problem Set Assignment Chapter 3 Exercises: 1, 2, 3, 4, 5, 6, 10, 11. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  The mode is Routine.\n The median is Routine.\n Based on the mode and median for this variable, most respondents indicate that their lives are Routine.\n A mean score is nonsense for nominal measurements.\n    2  The is ordinal.\n By using either figure related to the highest frequency or the highest percentage, the outcome is strongly agree.\n By using either related to the highest frequency or the highest percentage, the outcome is strongly agree for both years this indicating a high level of support. When combined agree, more than half of the 2014 (32.5% + 25.3% = 57.5%) and (39.5% + 27.0% = 66.5%) 2018 GSS samples agree that homosexuals should have the right to marry.\n    3  This has a level of an interval-ratio. Using frequencies1, we have\n\\[\\begin{aligned} \\dfrac{N+1}{2}\u0026amp;= \\dfrac{32+1}{2}\\\\\\\\ \u0026amp;= 16.5 \\end{aligned}\\] Adding up the frequencies until locating the 16th and 17th cases yields the category 40 hr worked last week.\n We have\n   25th percentile = \\((32 × 0.25)\\) implying the 8th case, or 30 hr worked last week. 50th percentile is simply the median implying the category 40 hr worked last week. 75th percentile = \\((32 × 0.75)\\) implying the 24th case, or 40 hr worked last week.    4  The mode category is agree.\n Using frequencies, we find that the median is 2, or agree.\n    20th percentile = \\((835 × 0.20)\\) implying the 167th case, or strongly agree. 80th percentile = \\((835 × 0.80)\\) implying the 668th case, or agree.    5       Race  Mode  Median      Black  Seldom  Sometimes    Hispanic  Everyday  Nearly everyday    White  Everyday  Most days      Teens’ breakfast habits vary by race/ethnicity. Out of the three racial/ethnic groups, Black students were more likely to report seldom or sometimes eating breakfast. On the other hand, White and Hispanic students eat breakfast more frequently. The mode for White and Hispanic students is everyday.    6  The mode for both Males and Females is Working full-time.\nUsing frequencies, we have\n Males:\n\\[\\begin{aligned} \\dfrac{N+1}{2}\u0026amp;= \\dfrac{596+1}{2}\\\\\\\\ \u0026amp;= 298.5 \\end{aligned}\\] Adding up the frequencies until locating the 298th and 299th cases yields the category Working Full Time.\n Females:\n\\[\\begin{aligned} \\dfrac{N+1}{2}\u0026amp;= \\dfrac{781+1}{2}\\\\\\\\ \u0026amp;= 391 \\end{aligned}\\] Adding up the frequencies until locating the 391th case yields the category Working Full Time. Using either the mode or median implies that there are no substantial differences between males and females.\n    10  \n  Hours Worked Last Week  Frequency  Hours Worked Last Week x Frequency      20  3  60    25  2  50    28  1  28    29  1  29    30  3  90    32  1  32    40  14  560    50  2  100    52  1  52    55  1  55    60  1  60    64  1  64    70  1  70    Total  32  1250      so we have \\[\\begin{aligned} \\overline{Y}\u0026amp;=\\dfrac{1250}{32}\\\\\\\\ \u0026amp;\\approx 39.06 \\end{aligned}\\] or about 39 hours.\nSince the median was 40 hr worked last week, the distribution is slightly skewed in a positive direction.    11  Since Clinton and Sanders’ middle-class income amount is higher than the U.S. Census estimated mean or median, their definition of middle class is not based on the statistical middle. It is unclear what they were using but it was not based on the actual measure.  :::\n   You can also do this using cumulative percentages.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ef8cddedad708f695e6676e0c06e973a","permalink":"/assignment/03-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/03-homeworks/","section":"assignment","summary":"Week 3 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 3 Problem Set Assignment Chapter 3 Exercises: 1, 2, 3, 4, 5, 6, 10, 11.","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"    Week 4 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 4 Problem Set Assignment Chapter 4 Exercises: 1, 2, 3, 7, 8, 12, 13. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  There are seven response categories for political views.\n We can calculate the percentage square by\n   Political Views\n Percentage \\(c\\)\n Percentage Squared \\(c^2\\)\n     Extremely liberal\n 5.5\n 30.25\n   Liberal\n 12.0\n 144.00\n   Slightly liberal\n 12.3\n 151.29\n   Moderate\n 37.8\n 1428.84\n   Slightly conservative\n 13.2\n 174.24\n   Conservative\n 15.0\n 225.00\n   Extremely conservative\n 4.2\n 17.64\n   Total\n 100.0\n 2171.26\n     yields \\(\\sum c^2 = 2171.26\\).\n We can calculate the IQV by\n\\[\\begin{aligned} IQV\u0026amp;= \\dfrac{7\\cdot(100^2-2171.26)}{100^2\\cdot (7-1)}\\\\\\\\ \u0026amp;= \\dfrac{54801.18}{60000}\\\\\\\\ \u0026amp;\\approx 0.91 \\end{aligned}\\] which is relatively close to 1 suggesting that Americans are fairly diverse in their political views.\n    2  For the female population where \\(N = 629\\), we have\n \\(Q_1 = 629\\cdot 0.25 = 157.25\\)\n  \\(Q_3 = 629 \\cdot 0.75 = 471.75\\)\n or about the 158th and 472nd cases which are representative of cases where the degree are in the High school graduate and Bachelor’s degree categories, respectively. This yields \\(IQR = 471.75-157.25=314.5\\) which includes those who have a highest attained degree of high school, junior college and bachelor’s .\nFor the male population where \\(N = 488\\), we have\n \\(Q_1 = 488 \\cdot 0.25 = 122\\)\n  \\(Q_3 = 488 \\cdot 0.75 = 366\\)\n or the 122nd and 366th cases which are representative of cases where the degree are in the High school graduate and Bachelor’s degree categories, respectively. This yields \\(IQR = 366-122=244\\) that includes those who have a highest attained degree of high school, junior college and bachelor’s .\nSo both IQRs provide the same groups of cases.\n Based on the IQR calculation, we know that 50% of all cases for males and females lies between high school and bachelor’s degree, but that does not inform us about the variability of the distribution. A better measure would be IQV and if interval-ratio measures were available, the variance and standard deviation would provide better information and estimates.\n    3  The range of convictions in   2010 is given by \\(397 – 108 =\\) \\(289\\). 2015 is given by \\(402 – 97 =\\) \\(305\\).  Therefore the range of conviction in 2015 is larger.\nThe mean number in 2010 across all levels of government was\n\\[\\begin{aligned} \\overline{Y} \u0026amp;= \\dfrac{397+108+280}{3}\\\\\\\\ \u0026amp;\\approx 261.67 \\end{aligned}\\] or about an average of 262 convictions.\nThe mean number in 2015 across all levels of government was\n\\[\\begin{aligned} \\overline{Y} \u0026amp;= \\dfrac{402+97+200}{3}\\\\\\\\ \u0026amp;= 233 \\end{aligned}\\] or an average of 233 convictions.\n The sum of squares can be found by\n   Government Level\n Number of Convictions\n \\(Y-\\overline{Y}\\)\n \\((Y-\\overline{Y})^2\\)\n     Federal\n 397\n \\(397 - 261.1 = 135.33\\)\n \\((135.33)^2 = 18314.21\\)\n   State\n 108\n \\(108 - 261.67 = -153.67\\)\n \\((-153.67)^2 = 23614.47\\)\n   Local\n 280\n \\(280 - 261.67 = 18.33\\)\n \\((18.33)^2 = 335.99\\)\n   Total\n 785\n $ $\n 42264.67\n     implying that\n\\[\\begin{aligned} S \u0026amp;= \\sqrt{\\dfrac{42264.67}{2}}\\\\\\\\ \u0026amp;\\approx 145.37 \\end{aligned}\\] So in 2010, there was a standard deviation of about 145.37.\nThe sum of squares can be found by\n   Government Level\n Number of Convictions\n \\(Y-\\overline{Y}\\)\n \\((Y-\\overline{Y})^2\\)\n     Federal\n 402\n \\(402 - 233 = 169\\)\n \\((169)^2 = 28561\\)\n   State\n 97\n \\(97 - 233 = -136\\)\n \\((-136)^2 = 18496\\)\n   Local\n 200\n \\(200 - 233 = -33\\)\n \\((-33)^2 = 1089\\)\n   Total\n 699\n $ $\n 48146\n     implying that\n\\[\\begin{aligned} S \u0026amp;= \\sqrt{\\dfrac{48146}{2}}\\\\\\\\ \u0026amp;\\approx 155.15 \\end{aligned}\\] So in 2015, there was a standard deviation of about 155.15.\n The standard deviation is larger in the latter time period so there is more variability in number of convictions in 2015 than in 2010. This supports the results of the range.\n    7  The range can be found by \\(4.50-2.20=\\) \\(2.30\\).\nMeanwhile for \\(n=10\\), we have\n 25th percentile: \\(10\\cdot 0.25 = 2.5\\)th case so \\(\\dfrac{2.40+2.50}{2}=2.45\\)\n  75th percentile: \\(10\\cdot 0.75 = 7.5\\)th case so \\(\\dfrac{3.60+3.60}{2}=3.60\\)\n S. the IQR is \\(3.60 – 2.45 =\\) \\(1.15\\). While both measures will work, the range is a bit more precise, in that it provides a better picture of the variability of divorce rates for all states in the sample.\n The sum of squares can be found by\n   State\n Divorce rate/thousand\n \\(Y-\\overline{Y}\\)\n \\((Y-\\overline{Y})^2\\)\n     Florida\n 3.6\n 3.5 – 3.14\n 0.210\n   Idaho\n 3.9\n 3.9 – 3.14\n 0.460\n   Maine\n 3.2\n 3.2 – 3.14\n 0.004\n   Maryland\n 2.5\n 2.5 – 3.14\n 0.410\n   Nevada\n 4.5\n 4.5 – 3.14\n 1.850\n   New Jersey\n 2.6\n 2.6 – 3.14\n 0.290\n   Texas\n 2.2\n 2.2 – 3.14\n 0.880\n   Vermont\n 2.9\n 2.9 – 3.14\n 0.058\n   Wisconsin\n 2.4\n 2.4 – 3.14\n 0.550\n   Total\n 31.4\n  4.922\n     implying that\n\\[\\begin{aligned} \\overline{Y} \u0026amp;= \\sqrt{\\dfrac{3.6 + 3.6 + 3.9 + 3.2 + 2.5 + 4.5 + 2.6 + 2.2 + 2.9 + 2.4}{10}}\\\\\\\\ \u0026amp;\\approx 3.14 \\end{aligned}\\] and\n\\[\\begin{aligned} S \u0026amp;= \\sqrt{\\dfrac{4.922}{9}}\\\\\\\\ \u0026amp;\\approx 0.74 \\end{aligned}\\] So between 1997-2017, the average divorce rate per 1000 people was about \\(3\\) with an approximated standard deviation of 0.74.\n Divorce rates may vary by state due to multiple factors which are not represented in the included data set such as variations in \u0026gt;span class=“boxed”\u0026gt;employment status, policies and laws by region (i.e. states with no-fault divorce laws), religious beliefs, etc.\n    8  DEGREE is an ordinal measure whereas AGEKDBRN is an interval measure.\n As DEGREE increases, AGEKDBRN increases as well implying the likelihood of a positive relationship. The youngest first-time parents are those with less than a high school degree, while the oldest first-time parents are those with graduate degrees with difference \\(28.59 – 21.33 = 7.26\\) years. The variability in age when first child was born is larger as educational attainment increases. The standard deviation for the high school degree group is largest at \\(5.498\\) years, the smallest is for the some college group with \\(4.581\\).\n   11  a. `Type of Work` is a nominal variable implying that the appropriate measure of variability as the index of qualitative variation (IQV). b. The pertage squared for both grades can be found by   Type of Work  Grade 8 Percentage $c$  Grade 8 Percentage Squared $c^2$  Grade 10 Percentage $c$  Grade 10 Percentage Squared $c^2$      Lawn work  28  $(28)^2 = 784$  20  $(20)^2 = 400$    Food service  3  $(3)^2 = 9$  10  $(10)^2 = 100$    Babysitting  37  $(37)^2 = 1369$  28  $(28)^2 = 784$    Other  32  $(32)^2 = 1024$  42  $(42)^2 = 1764$    Total  100  $3186$  100  $2048$     So the IQV for both grades can be found by  Grade 8: $\\dfrac{4\\cdot(100^2-3186)}{100^2\\cdot (4-1)} = \\dfrac{27256}{30000} = 0.91$   Grade 10: $\\dfrac{4\\cdot(100^2-3048)}{100^2\\cdot (4-1)} = \\dfrac{27808}{30000} = 0.93$  implying IQV(Grade 8) $=0.91$ and IQV(Grade 10) $=0.93$. c. Though both IQVs are greater than $0.90$, there is slightly more variation among 10th graders than 8th gradersin the type of jobs they hold. The difference may be attributed to more employment options for older students and limitations for younger studentsxxxx leading to more informal jobs such as lawn work and babysitting.  --  12  Using the summary statistics for life expectancy    Measure  Statistic for European Countries  Statistic for Non-european Countries      Median  81.500  82.70    Variance  7.985  11.65    Standard deviation  2.830  3.41    Minimum  75.300  76.30    Maximum  82.000  85.50    Range  6.700  9.20    Interquartile range  3.800  4.95      we find that the variable is interval ratio implying that variance (or standard deviation), range, or IQR are possible and valid. Among these three measures, variance and/or standard deviation is preferred for precision. For just the average life expectancy for these 10 countries, we should rely on the mean.\nOn average, non-European countries have a slightly higher life expectancy at birth (\\(82.00\\) vs. \\(80.30\\)). Both the mean and median are higher for non-European countries than for European countries. Also, the distribution of non-European countries exhibits more variability; the standard deviation for European countries is \\(3.41\\) years, while for non-European countries it is \\(2.83\\) years.\nThese differences might be explained by access and availability of health care and/or diet. However, the difference might simply be random due to the small number of countries presented in this example. We are likely to find different results if more countries were incorporated into the analyses.\n  13  Overall, Clinton voters were younger, more educated, and attended religious services less than Trump voters. The youngest voters were male Clinton voters at \\(50.49\\) years (\\(s = 18.84\\)), followed by female Clinton voters, \\(51.93\\) years (\\(s = 18.21\\)). For education, males who voted for Clinton had the highest mean of \\(14.69\\) (\\(s = 2.73\\)). Males who voted for Trump had \\(13.80\\) years of education (\\(s = 2.75\\)). Trump voters, both males and females, attended religious services more often than Clinton voters. Mean scores were \\(3.72\\) for males (\\(s = 2.93\\)) and \\(3.82\\) for females (\\(s = 2.75\\)), indicating church attendance about several times a year to once a month. The standard deviations indicate a consistency in the distributions of education, age, and religious service attendance across all four groups. The largest standard deviations are for age, ranging from \\(15.94\\) to \\(18.84\\) years. These wide standard deviations indicate more dispersion around the mean age scores.\n :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d78716373c973d0053a3723106f8b81","permalink":"/assignment/04-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/04-homeworks/","section":"assignment","summary":"Week 4 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 4 Problem Set Assignment Chapter 4 Exercises: 1, 2, 3, 7, 8, 12, 13.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"   Week 5 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 5 Problem Set Assignment Chapter 5 Exercises: 1, 2, 5, 6, 7, 8, 11, 12. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  We are looking the area under the curve greater than \\(8\\). The \\(z\\)-score for a person who watches more than 8 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{8-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx 1.68 \\end{aligned}\\] So \\(z \\approx 1.68\\).\n We are looking the area under the curve less than \\(5\\). The \\(z\\)-score for a person who watches less than 5 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{5-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx 0.68 \\end{aligned}\\] So \\(z \\approx 0.68\\). Since the area between \\(z=0.68\\) and \\(z=0\\) is \\(0.2517\\), we have \\(0.5000+0.2517=0.7517\\). This implies that \\(0.7517\\cdot 1014 \\approx\\) \\(762\\) or \\(763\\) people watch television less than 5 hours/day.\n We have\n\\[\\begin{aligned} Y\u0026amp;=2.97+1\\cdot 3\\\\ \u0026amp;= 5.97 \\end{aligned}\\] implying that 5.97 television hr/day corresponds to a \\(z\\)-score of \\(+1\\).\n We are looking at the area between \\(1\\) and \\(6\\). The \\(z\\)-score for a person who watches more than 1 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{1-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx -0.66 \\end{aligned}\\] with the area between the mean and \\(z=0.66\\) found to be \\(0.2454\\). The \\(z\\)-score for a person who watches less than 6 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{6-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx 1.01 \\end{aligned}\\] with the area between the mean and \\(z=1.01\\) found to be \\(0.3438\\).\nSo the percentage of people who watch between 1 hr/day and 6 hr/day of television can be found by \\(0.2454 + 0.3438 = 0.5892\\), or about \\(59\\%\\).\n    2  The 95th percentile, or \\(0.9500\\) is approximately equivalent to \\(z=1.65\\). So the number of women needing shelter is\n\\[\\begin{aligned} Y\u0026amp;= 250+1.65\\cdot 75\\\\ \u0026amp;= 373.75 \\end{aligned}\\] or about \\(374\\) women. Since this figure exceeds the total capacity of 350, there will not be enough space for all abused women on 95% of all nights. at minimum, the city needs \\(374\\) beds.\n We are looking the area under the curve greater than \\(220\\). The \\(z\\)-score for exceeding the capacity is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{220-250}{75}\\\\\\\\ \u0026amp;\\approx -0.40 \\end{aligned}\\] The area below this value is \\(0.3446\\), so the area exceeding this is \\(1 – 0.3446 = 0.6554\\), or about \\(66\\%\\) of all nights the number of women seeking shelter will exceed the capacity of 220.\n    5  We are looking at the area between \\(12\\) and \\(16\\) for the Working class. The \\(z\\)-score for a person with more than 12 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{12-13.05}{2.77}\\\\\\\\ \u0026amp;\\approx -0.38 \\end{aligned}\\] with the area between the mean and \\(z=-0.38\\) found to be \\(0.1480\\).\nThe \\(z\\)-score for a person who watches less than 16 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-13.05}{2.77}\\\\\\\\ \u0026amp;\\approx 1.06 \\end{aligned}\\] with the area between the mean and \\(z=1.06\\) found to be \\(0.3554\\).\nSo the proportion of working-class respondents with 12–16 years of education can be found by \\(0.1480 + 0.3554 =\\) \\(0.5034\\).\nWe are still looking at the area between \\(12\\) and \\(16\\) for the Upper class. The \\(z\\)-score for a person with more than 12 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{12-15.48}{2.76}\\\\\\\\ \u0026amp;\\approx -1.26 \\end{aligned}\\] with the area between the mean and \\(z=-1.26\\) found to be \\(0.3962\\).\nThe \\(z\\)-score for a person who watches less than 16 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-15.48}{2.67}\\\\\\\\ \u0026amp;\\approx 0.19 \\end{aligned}\\] with the area between the mean and \\(z=0.19\\) found to be \\(0.0753\\).\nSo the proportion of upper-class respondents with 12–16 years of education can be found by \\(0.3962 + 0.0753 =\\) \\(0.4715\\).\n We are looking the area under the curve greater than \\(16\\). For working-class respondents, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-12.05}{2.77}\\\\\\\\ \u0026amp;\\approx 1.06 \\end{aligned}\\] The area between \\(z=1.06\\) and the tail is \\(0.1446\\) thus implying the probability of a working-class respondent having more than 16 years of education is \\(14.46\\%\\).\nFor middle-class respondents, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-14.56}{2.95}\\\\\\\\ \u0026amp;\\approx 0.49 \\end{aligned}\\] The area between \\(z=0.49\\) and the tail is \\(0.3121\\) thus implying the probability of a middle-class respondent having more than 16 years of education is \\(31.21\\%\\).\n We are looking the area under the curve less than \\(10\\). For middle-class respondents, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-14.56}{2.95}\\\\\\\\ \u0026amp;\\approx 0.49 \\end{aligned}\\] The area beyond \\(z=0.63\\) and the tail is \\(0.2643\\) thus implying the probability of a lower-class respondent having more less 10 years of education is \\(26.43\\%\\).\n    6  We are looking the area under the curve greater than \\(625\\). For these high school graduates, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{625-536}{102}\\\\\\\\ \u0026amp;\\approx 0.87 \\end{aligned}\\] The area beyond \\(z=0.87\\) and the tail is \\(0.1992\\) thus implying the probability that a high school graduate having earning a score of more than 625 is \\(19.92\\%\\).\n We are looking the area under the curve between than \\(400\\) and \\(625\\). For these high school graduates, the \\(z\\)-score for 625 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{625-536}{102}\\\\\\\\ \u0026amp;\\approx 0.87 \\end{aligned}\\] The area between the mean and \\(z=0.87\\) is \\(0.3087\\).\nFor these high school graduates, the \\(z\\)-score for 400 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{400-536}{102}\\\\\\\\ \u0026amp;\\approx -1.33 \\end{aligned}\\] The area between\\(z=-1.33\\) and the mean is \\(0.4082\\).\nSumming these proportions yields \\(0.3087+0.4082=0.7168\\), or \\(71.68\\%\\) of the high school graduates earned a score between 400 and 625.\n The 20th percentile, or \\(0.2000\\) is equivalent to \\(z=-0.84\\). Thus the SAT ERW equivalency can be found by \\(536-0.84\\cdot 102=450.32\\), or a score of \\(450\\).\n    7  We have\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{150-100}{15} \u0026amp;\\approx 3.33 \\end{aligned}\\] or \\(z\\approx 3.33\\).\n We are looking the area under the curve greater than \\(150\\). The area beyond \\(z=3.33\\) is \\(0.0004\\) implying that the percentage of scores above 150 is \\(0.04\\%\\)1.\n We are looking at the area between \\(85\\) and \\(150\\). The \\(z\\)-score for a score of 85 is\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{85-100}{15}\\\\\\\\ \u0026amp;\\approx -1 \\end{aligned}\\] with the area between \\(z=-1\\) and the mean found to be \\(0.3413\\). The \\(z\\)-score for a score of 150 is\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{150-100}{15} \u0026amp;\\approx 3.33 \\end{aligned}\\] with the area between the mean and \\(z=3.33\\) found to be \\(0.4996\\).\nSo the \\(0.3413 + 0.4996 = 0.8409\\), or about \\(84\\%\\) of scores fall between \\(85\\) and \\(150\\).\n Scoring in the 95th percentile means that 95% of the sample scored below this level. This outcome can be calculated by \\(100 + 1.65\\cdot 15 = 124.75\\). So the IQ score that is associated with the 95th percentile is \\(124.75\\).\n    8  We are looking the area under the curve less than \\(400\\). For these high school graduates, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{400-531}{114}\\\\\\\\ \u0026amp;\\approx -1.15 \\end{aligned}\\] The area beyond \\(z=-1.15\\) and the tail is \\(0.1251\\) thus implying that about \\(12.51\\%\\)of high school graduates hearned a score less than 400.\n We are looking the area under the curve between than \\(600\\) and \\(700\\). For these high school graduates, the \\(z\\)-score for 600 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{600-531}{114}\\\\\\\\ \u0026amp;\\approx 0.61 \\end{aligned}\\] The area between the mean and \\(z=0.61\\) is \\(0.2291\\).\nFor these high school graduates, the \\(z\\)-score for 700 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{700-531}{114}\\\\\\\\ \u0026amp;\\approx 1.48 \\end{aligned}\\] The area between \\(z=1.48\\) and the mean is \\(0.4306\\).\nSubtracting these proportions yields \\(0.4306-0.2291=0.2015\\), or \\(20.15\\%\\) of the high school graduates earned a score between 600 and 700\n For an earned score of \\(725\\) which is greater than the mean, we have From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{725-531}{114}\\\\\\\\ \u0026amp;\\approx 1.70 \\end{aligned}\\] The area beyond \\(z=1.70\\) is \\(0.0466\\). So the percentile rank may be found by \\(1.000-0.0466 = 0.9535\\), or \\(95.35\\%\\) which implies the 96th percentile.\n    11  For an earned score of \\(990\\) which is greater than the mean, we have From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{990-981}{27.3}\\\\\\\\ \u0026amp;\\approx 0.33 \\end{aligned}\\] The area beyond \\(z=0.33\\) is \\(0.3707\\). So the percentile rank may be found by \\(1.000-0.3707 = 0.6293\\), or \\(62.93\\%\\) which implies the 62nd percentile. So the team is not in the upper quartile.\n For the top 25%, or \\(0.0025\\), the area beyond \\(z\\) is approximated by \\(z = 0.67\\). The cutoff score is then \\(981+0.67\\cdot 27.3\\approx\\) \\(999.29\\).\n As noted above, \\(z = 0.67\\).\n    12  For Team A an eligibility criterion score of \\(971\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{971-983}{33}\\\\\\\\ \u0026amp;\\approx -0.36 \\end{aligned}\\] For Team A a retention criterion score of \\(958\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{958-976}{34.9}\\\\\\\\ \u0026amp;\\approx -0.52 \\end{aligned}\\] For Team B an eligibility criterion score of \\(987\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{987-983}{33}\\\\\\\\ \u0026amp;\\approx 0.12 \\end{aligned}\\] For Team B a retention criterion score of \\(970\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{970-976}{34.9}\\\\\\\\ \u0026amp;\\approx -0.17 \\end{aligned}\\] So Team B is better on both eligibility and retention than Team A.\n For Team B a retention criterion score of \\(z=-0.17\\) is below the mean and the corresponding proportion can be viewed by which is the area between \\(z\\) and the tail, or \\(0.4325\\).\n For Team A an eligibility criterion score of \\(z=-0.36\\) is below the mean and the corresponding proportion can be viewed by which is the area between \\(z\\) and the tail, or \\(0.3594\\) implying about the 35th percentile.\n   :::\n   Which is the reason you don’t notice any shading in the plot↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"530a08282c6261f8515db43910630058","permalink":"/assignment/05-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/05-homeworks/","section":"assignment","summary":"Week 5 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 5 Problem Set Assignment Chapter 5 Exercises: 1, 2, 5, 6, 7, 8, 11, 12.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"    Week 7 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; } li { margin-left: 1.5em; } ul { margin-left: 1.5em; }  Week 7 Problem Set Assignment Chapter 6 Exercises: 1, 2, 3, 4, 5, 6, 7, 8, 9. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  Although there are problems with the collection of data from all Americans, the census is assumed to be complete, so the mean age would be a parameter.\n A statistic because it is estimated from a sample.\n A parameter because the school would have information on all its students.\n A statistic because it is estimated from a sample.\n A parameter because the school has information on all employees.\n    2  First, the population is not clearly defined. Are the population subscribers to the newspaper? Readers of the newspaper? Or some other set of people? Even given this uncertainty, the letters to the editor would only be a random sample of a population if, clearly, they came randomly from that population. But there is no reason to assume this to be the case. People with stronger opinions, who can write reasonably well, and who have the time to write are more likely to write the editor. Since these characteristics are not distributed randomly throughout the adult population, it is improbable that the letters are a random sample.\n The mayor might consider forming a coalition to randomly sample landline and cell phone numbers of city residents.\n    3  Assuming that the population is defined as all persons shopping at that shopping mall that day of the week, she is selecting a systematic random sample. A more precise definition might limit it to all persons passing by the department store at the mall that day.\n This is a stratified sample because voters were first grouped by county, and unless the counties have the same number of voters, it is a disproportionate stratified sample because the same number is chosen from each county. We can assume that it was a probability sample, but we are not told exactly how the 50 voters were chosen from the lists. However, assuming that the population is defined as all Americans, this sort of sampling technique would qualify as nonprobability sampling.\n This is neither a simple random sample nor a systematic random sample. It might be thought of as a sample stratified on last name, but even then, choosing the first 20 names is not a random selection process.\n This is not a probability sample. Instead, it is a purposive sample chosen to represent a cross section of the population in New York City.\n    4  There are 120 students where 57 are juniors who represent \\(57\\div120 = 47.5\\), or 47.5% of the class. This implies the probability of choosing a junior is \\(0.475\\).\n The probability that the student will be a freshman is \\(7\\div120 =\\) \\(0.058\\).\n The proportion of seniors in the class is \\(34/120 = 0.283\\). The proportion of sophomores is \\(22\\div120 = 0.183\\). Then the number of students to be chosen at each class level is:\n  Class Level\n Calculation\n Result\n     Freshman\n 0.058(30)\n 1.74\n   Sophomore\n 0.183(30)\n 5.49\n   Junior\n 0.475(30)\n 14.25\n   Senior\n 0.283(30)\n 8.49\n      or about 2 freshman, 5 sophomores, 14 juniors, and 8 seniors.\n For a disproportionate sample we choose the same number of students from each class level. For example, we could have 5 freshman, 5 sophomores, 5 juniors, and 5 seniors.\n    5  The relationship between the standard error and the standard deviation is \\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{N}}\\). Since \\(\\sigma\\) is divided by \\(\\sqrt{N}\\), \\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{N}}\\) must always be smaller than \\(\\sigma\\) barring the trivial case where \\(N = 1\\). Theoretically, the dispersion of the mean must be less than the dispersion of the raw scores. This implies that the standard error of the mean is less than the standard deviation.\n   6  First note that the standard error of the mean is proportional to \\(\\frac{1}{\\sqrt{N}}\\).  The standard error of the mean is\n\\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{100}}=\\frac{\\sigma}{10}\\) for a sample size of 100\n\\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{1600}}=\\frac{\\sigma}{40}\\) for a sample size of 16000\nWe compare the two by \\(\\frac{40}{10} =4\\). So \\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{N}}\\) is smaller by a factor of 4 when the sample size increases from 100 to 1600.\n The standard error of the mean is\n\\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{300}}\\approx\\frac{\\sigma}{17.32}\\) for a sample size of 300\n\\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{150}}\\approx\\frac{\\sigma}{12.25}\\) for a sample size of 150\nWe compare the two by \\(\\frac{17.32}{12.25} \\approx 1.41\\). So \\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{N}}\\) is greater by a factor of 1.41 when the sample size decreases from 300 to 150.\n For simplicity, assume an initial sample size of 100. We then have \\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{100}}=\\frac{\\sigma}{10}\\) for a sample size of 10 0\n\\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{4\\cdot100}}=\\frac{\\sigma}{20}\\) for a sample size of 4000\nWe compare the two by \\(\\frac{20}{10} = 2\\). So \\(\\sigma_{\\overline{Y}} =\\frac{\\sigma}{\\sqrt{N}}\\) decreases by a factor of 2.\n    7  Facebook ® polls are most definitely not probability samples. While many reasons exists, the two noted below are likely the most important: - the population from which data are gathered is limited to the friends on one’s Facebook ® profile–and such is certainly not a representative sample from the larger population. - due to Facebook ® algorithms, we can’t be sure that every Facebook ® friend would have the same opportunity to see, and thus participate, in the poll.\n The population is our Facebook ® friends who come across the poll.\n    8\nFirst we convert these to a vector and take the sum\nmilitary \u0026lt;- c(229634, 64564, 376034, 89826, 26384, 41441, 177336, 68440, 8579) total_mil \u0026lt;- sum(military)  The mean number of active military personnel per region in 2009 was\nmu_mil = mean(military) mu_mil ## [1] 120248.7 or about 120249. The standard deviation is\nsd_pop=function(x){sd(x)*sqrt((length(x)-1)/length(x))} sigma_mil = sd_pop(military) sigma_mil ## [1] 112966.1 or about 112966.\n We can use the sample command to get our 10 sample means of size 3. We can use the replcate and rnorm commands in R to do this\n# replicate(size #, rnorm(#of sample means, mu, sigma)) samples \u0026lt;- replicate(3,rnorm(10, mu_mil,sigma_mil)) samples ## [,1] [,2] [,3] ## [1,] 31422.38 65643.63 86411.50 ## [2,] 15714.03 363201.95 -11681.81 ## [3,] 15329.43 90086.78 43303.50 ## [4,] 341455.84 54319.25 98274.90 ## [5,] 228621.48 61548.34 109545.24 ## [6,] -96565.00 205915.43 70730.99 ## [7,] 241782.45 -57116.52 170698.90 ## [8,] -39273.16 285236.57 193548.81 ## [9,] 121682.40 72571.27 74652.67 ## [10,] 78516.03 228608.91 -16726.03 The mean of these 10 sets of means is\nmean(samples) ## [1] 104248.7 or about 105428. We notice that the population mean and the mean of the sampling distribution are somewhat close, a feature that we should come to expect given the fact that \\(\\mu_Y = \\mu_{\\overline{Y}}\\).\n The standard deviation of these 10 sets of means is\nsd(samples) ## [1] 114291.8 or about 122991.\n The population distribution is positively skewed and not close to normal. Since a very small sample size is used in this problem, the histogram for the samples of size 3 does not look normal. The distributions appear unimodal. The fact that the sample distribution of the means tends toward normality because of the Central Limit Theorem would become even more apparent if we took samples of size 5 or 6. To show this, look at the plots below     9  This is not a reliable sample. The students eating lunch on Tuesday are not necessarily representative of all students at the school, and you have no way of calculating the probability of inclusion of any student. Many students might, for example, rarely eat lunch at the student union and, therefore, have no chance of being represented in your sample. The fact that you selected all the students eating lunch on Tuesday makes your selection appear to be a census of a population, but that isn’t true either unless all the students ate at the student union on Tuesday.\n This is not a reliable sample. The majority of people in attendance at the rally would probably be in support of open borders. Students against such might not have any chance of selection.\n Answers will vary, but it seems the best route would be to obtain a list of all registered students from your university’s registrar. And then employ a known sampling strategy (simple random sampling, stratified sampling, etc.) to create your sample of 10% of the student body. From there, you would reach out to each person you’ve selected via e-mail, telephone, mail, and so on and ask them to complete your survey.\n   :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"59cf66c3ec57afe00b0487aabf6e9b2e","permalink":"/assignment/07-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/07-homeworks/","section":"assignment","summary":"Week 7 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; } li { margin-left: 1.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"  You can find information pertaining to Data Camp by heading over to lessons page. To simply get to Data Camp site, please click on their logo below to go to their sign in page (or directly to the modules if you are already signed in):\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"25d4817ae10e3ebf2a1eb5dead763b0e","permalink":"/assignment/datacamp-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/datacamp-assignment/","section":"assignment","summary":"You can find information pertaining to Data Camp by heading over to lessons page. To simply get to Data Camp site, please click on their logo below to go to their sign in page (or directly to the modules if you are already signed in):","tags":null,"title":"What is a Data Camp?","type":"docs"},{"authors":null,"categories":null,"content":"   Accessibility Colors Fonts Graphic assets Images Vectors Vectors, photos, videos, and other assets    Accessibility  Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness) Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account. Colorgorical: Create color palettes based on fancy mathematical rules for perceptual distance. Color Hex Color Codes: A collection of colors and palettes given only by hex identifiers. Colorpicker for data: More fancy mathematical rules for color palettes (explanation). ColourLovers: Like Facebook for color palettes. Comprehensive list of color palettes in r: A wided ranging collection of various palettes from the entire R community. These can now all be downloaded within a the package paleteer by running install.packages(\"paletteer\"). More information can be found here. iWantHue: Yet another perceptual distance-based color palette builder. Photochrome: Word-based color palettes. PolicyViz Design Color Tools: Large collection of useful color resources Scientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico. viridis: Perceptually uniform color scales. Wes Anderson Palettes: Palettes based off of Wes Anderson films and also one of my personal favorite sites to choose colors.   Fonts  Google Fonts: Huge collection of free, well-made fonts. The Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).   Graphic assets Images  Burst freephotos.cc Pexels Pixabay StockSnap.io Unsplash Use the Creative Commons filters on Google Images or Flickr   Vectors  aiconica: 1,000+ vector icons Noun Project: Thousands of free simple vector images Vecteezy: Thousands of free vector images   Vectors, photos, videos, and other assets  Stockio    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility Colors Fonts Graphic assets Images Vectors Vectors, photos, videos, and other assets    Accessibility  Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness) Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"    Introduction Preparation Sentiment Analysis Learning by Doing Extra Information Getting Data  Data Wrangling Exploring Data Lemmatize Words Using Stop Words Descriptive Statistics Wordclouds Bar Plots   Sentiment Analysis Basic Text Analysis Most Common Positive and Negative Words in a Bar Graph Most Common Positive and Negative Words in a WordCloud Positive and Negative Words in a Line Graph Positive and Negative Words in a Boxplot  Viewing Across all Lexicons    Introduction Preparation Download a script file of just the R chunks used in this walkthrough.\n Sentiment Analysis Without getting into the specifics of machine learning, a sentiment analysis is a quantitative process of determining whether a piece of writing is positive, negative or neutral. It entails using a mix of statistics (you know…that old chestnut), natural language processing (NLP), and machine learning to identify and extract subjective information from text files, for instance, a reviewer’s feelings, thoughts, judgments, or assessments within open text. If you want to know more about this method, Qualtrics provides a nice readable writeup about it.\n Learning by Doing We are essentially going to learn about a sentiment analysis using music…well music that I like anyway. In this session we’ll be looking at some Grateful Dead lyrics from two albums.\nWith that said, let’s load up some libraries. If you don’t have some of these (and you most likely don’t), remember to download them first using Tools \u0026gt; Install Packages.\n# We are using the tidyverse family which is essentially the gold standard for # data wrangling: library(tidyverse) library(scales) # text mining and annoying wordclouds based on the tidyverse family library(tidytext) library(ggwordcloud) library(textstem) library(textdata) library(wordcloud) # getting lyrics library(genius) # Allows you to download lyrics for an entire album in a tidy format from # genius.com # the aesthetics library(viridis) library(RColorBrewer) library(hrbrthemes) hrbrthemes::import_roboto_condensed()  Extra Information More information about * the tidyverse family of packages are introduced by selecting https://www.tidyverse.org/.\n genius.com can be found here https://genius.com/.\n the genius package can be found via this link https://github.com/josiahparry/genius.\n   Getting Data Grab the lyrics for American Beauty (1971) which was more about rock and roll and Shakedown Street (1978) which was influenced by disco (ugh yes…disco). Were going to see if the sentiments associated within each album changed over time. On a side note, feel free to grab albums and lyrics that you like instead but the example will be based on the two albums so I would suggest going through it with these first and then doing your own. We’ll compare the two using a sentiment analysis.\namerican_beauty \u0026lt;- genius_album(artist = \u0026quot;The Grateful Dead\u0026quot;, album = \u0026quot;***American Beauty***\u0026quot;) ## Joining, by = c(\u0026quot;album_name\u0026quot;, \u0026quot;track_n\u0026quot;, \u0026quot;track_url\u0026quot;) To see the results, just type in the following:\namerican_beauty ## # A tibble: 359 x 4 ## track_n line lyric track_title ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 Look out of any window Box of Rain ## 2 1 2 Any morning, any evening, any day Box of Rain ## 3 1 3 Maybe the sun is shining Box of Rain ## 4 1 4 Birds are winging or rain is falling from a heavy … Box of Rain ## 5 1 5 What do you want me to do Box of Rain ## 6 1 6 To do for you to see you through? Box of Rain ## 7 1 7 For this is all a dream we dreamed Box of Rain ## 8 1 8 One afternoon, long ago Box of Rain ## 9 1 9 Walk out of any doorway Box of Rain ## 10 1 10 Feel your way, feel your way like the day before Box of Rain ## # … with 349 more rows Now let’s do the same for Shakedown Street\nshakedown_street \u0026lt;- genius_album(artist = \u0026quot;The Grateful Dead\u0026quot;, album = \u0026quot;***Shakedown Street***\u0026quot;) ## Joining, by = c(\u0026quot;album_name\u0026quot;, \u0026quot;track_n\u0026quot;, \u0026quot;track_url\u0026quot;) And the results look like:\nshakedown_street ## # A tibble: 304 x 4 ## track_n line lyric track_title ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 \u0026quot;Well, I was feeling so bad, asked my family docto… Good Lovin\u0026#39; ## 2 1 2 \u0026quot;I said, \\\u0026quot;Doctor, Doctor, Mister M.D., (doctor) c… Good Lovin\u0026#39; ## 3 1 3 \u0026quot;He said, \\\u0026quot;Yeah, yeah, yeah, yeah, yeah, yeah, ye… Good Lovin\u0026#39; ## 4 1 4 \u0026quot;All you need, all you really need: good loving\u0026quot; Good Lovin\u0026#39; ## 5 1 5 \u0026quot;Because you got to have loving (good loving)\u0026quot; Good Lovin\u0026#39; ## 6 1 6 \u0026quot;Everybody got to have loving (good loving)\u0026quot; Good Lovin\u0026#39; ## 7 1 7 \u0026quot;A little good loving now baby, good loving\u0026quot; Good Lovin\u0026#39; ## 8 1 8 \u0026quot;So come on baby, squeeze me tight\u0026quot; Good Lovin\u0026#39; ## 9 1 9 \u0026quot;Don\u0026#39;t you want your daddy to be alright?\u0026quot; Good Lovin\u0026#39; ## 10 1 10 \u0026quot;I said baby, now it\u0026#39;s for sure\u0026quot; Good Lovin\u0026#39; ## # … with 294 more rows To find out the internal structure of one of your data sets, run this:\nstr(shakedown_street) ## tibble [304 × 4] (S3: tbl_df/tbl/data.frame) ## $ track_n : int [1:304] 1 1 1 1 1 1 1 1 1 1 ... ## $ line : int [1:304] 1 2 3 4 5 6 7 8 9 10 ... ## $ lyric : chr [1:304] \u0026quot;Well, I was feeling so bad, asked my family doctor about what I had\u0026quot; \u0026quot;I said, \\\u0026quot;Doctor, Doctor, Mister M.D., (doctor) can you tell me (doctor), what\u0026#39;s ailing me? (doctor)\\\u0026quot;\u0026quot; \u0026quot;He said, \\\u0026quot;Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah\\\u0026quot;\u0026quot; \u0026quot;All you need, all you really need: good loving\u0026quot; ... ## $ track_title: chr [1:304] \u0026quot;Good Lovin\u0026#39;\u0026quot; \u0026quot;Good Lovin\u0026#39;\u0026quot; \u0026quot;Good Lovin\u0026#39;\u0026quot; \u0026quot;Good Lovin\u0026#39;\u0026quot; ... This gives you a good bit of information about the structure of your data set. * The class of the data are in multiple formats: tbl_df (tabular data frame), tbl (table), and data.frame (normal data frame). This implies that you can use common commands to wrangle the data within this variable.\n You have four (4) internal variables and 304 lines in this data frame (think an excel document which has 4 columns and 304 rows).\n Notice that the track_title (track title) and lyric (lyrics) columns are character vectors while track_n (track number) and line (line number) are integer vectors. These are important because what you can do with any vector is mostly dependent on its type.\n    Data Wrangling If we’re going to compare the two albums, we should probably put them in the same data frame. But to make sure we can distinguish each, we’re going to have to tag them by album name. To do this, we need to add a column. The package dplyr let’s you mess around with the data without destroying the data frame format. It also lets you deal with pipes (%\u0026gt;%) which is a fantastic thing. During the dark days of R before pipes, you had to go back and rerun any R code that you made changes to which meant you had to keep track of everything. In any case, when using pipes, to do (nearly) anything to the data frame, you must use the command mutate.\nRemember we are adding a column that denoted the album name:\namerican_beauty_tagged \u0026lt;- american_beauty %\u0026gt;% mutate(album = \u0026quot;***American Beauty***\u0026quot;) %\u0026gt;% select(album, track_title, track_n, line, lyric)  \nOk so let’s take a moment to get our bearings straight. Currently we are\nusing the american_beauty data set\n adding a column named album and then populating it with the text American Beauty (in quotes); and\n rearranging the columns. By default, any new columns are put at the end of a data frame. Just to satisfy my obsessive nature, we’ll rearrange them in order of scope by putting the album name in front.\n   \nGood? Ok then let’s take a look at the data set now:\namerican_beauty ## # A tibble: 359 x 4 ## track_n line lyric track_title ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 Look out of any window Box of Rain ## 2 1 2 Any morning, any evening, any day Box of Rain ## 3 1 3 Maybe the sun is shining Box of Rain ## 4 1 4 Birds are winging or rain is falling from a heavy … Box of Rain ## 5 1 5 What do you want me to do Box of Rain ## 6 1 6 To do for you to see you through? Box of Rain ## 7 1 7 For this is all a dream we dreamed Box of Rain ## 8 1 8 One afternoon, long ago Box of Rain ## 9 1 9 Walk out of any doorway Box of Rain ## 10 1 10 Feel your way, feel your way like the day before Box of Rain ## # … with 349 more rows Sure enough, there’s the column with the album name. So now let’s do the same for the other album\nshakedown_street_tagged \u0026lt;- shakedown_street %\u0026gt;% mutate(album = \u0026quot;***Shakedown Street***\u0026quot;) %\u0026gt;% select(album, track_title, track_n, line, lyric) and verify:\nshakedown_street ## # A tibble: 304 x 4 ## track_n line lyric track_title ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 \u0026quot;Well, I was feeling so bad, asked my family docto… Good Lovin\u0026#39; ## 2 1 2 \u0026quot;I said, \\\u0026quot;Doctor, Doctor, Mister M.D., (doctor) c… Good Lovin\u0026#39; ## 3 1 3 \u0026quot;He said, \\\u0026quot;Yeah, yeah, yeah, yeah, yeah, yeah, ye… Good Lovin\u0026#39; ## 4 1 4 \u0026quot;All you need, all you really need: good loving\u0026quot; Good Lovin\u0026#39; ## 5 1 5 \u0026quot;Because you got to have loving (good loving)\u0026quot; Good Lovin\u0026#39; ## 6 1 6 \u0026quot;Everybody got to have loving (good loving)\u0026quot; Good Lovin\u0026#39; ## 7 1 7 \u0026quot;A little good loving now baby, good loving\u0026quot; Good Lovin\u0026#39; ## 8 1 8 \u0026quot;So come on baby, squeeze me tight\u0026quot; Good Lovin\u0026#39; ## 9 1 9 \u0026quot;Don\u0026#39;t you want your daddy to be alright?\u0026quot; Good Lovin\u0026#39; ## 10 1 10 \u0026quot;I said baby, now it\u0026#39;s for sure\u0026quot; Good Lovin\u0026#39; ## # … with 294 more rows Great those look good. Now to merge the data sets, we are going to use a command called rbind. The single requirement is that both data frames have the same column names since it needs to know what columns go with what (but notice that having the same column names implies that the columns do not have to be in the same order!)\nall_albums \u0026lt;- rbind(american_beauty_tagged, shakedown_street_tagged) If you simply do the following\nall_albums ## # A tibble: 663 x 5 ## album track_title track_n line lyric ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 ***American Be… Box of Rain 1 1 Look out of any window ## 2 ***American Be… Box of Rain 1 2 Any morning, any evening, any day ## 3 ***American Be… Box of Rain 1 3 Maybe the sun is shining ## 4 ***American Be… Box of Rain 1 4 Birds are winging or rain is falli… ## 5 ***American Be… Box of Rain 1 5 What do you want me to do ## 6 ***American Be… Box of Rain 1 6 To do for you to see you through? ## 7 ***American Be… Box of Rain 1 7 For this is all a dream we dreamed ## 8 ***American Be… Box of Rain 1 8 One afternoon, long ago ## 9 ***American Be… Box of Rain 1 9 Walk out of any doorway ## 10 ***American Be… Box of Rain 1 10 Feel your way, feel your way like … ## # … with 653 more rows Notice that you can’t actually observe both data frames were stacked, or bound. To do this, or at least get an indicator that the binding worked, you can find the unique values in a column by doing\nunique(all_albums$album) ## [1] \u0026quot;***American Beauty***\u0026quot; \u0026quot;***Shakedown Street***\u0026quot; Sure enough, it looks like both albums are there.\nExploring Data OK now to do some exploratory data analysis (EDA) which have to be displayed in a useless format on par with pie charts - that is wordclouds. Let’s tidy the data sets.\nFirst we are going to tokenize the lyrics into a tidy dataframe\ntidy_lyrics\u0026lt;- all_albums %\u0026gt;% group_by(album) %\u0026gt;% unnest_tokens(word, lyric)  So here we are 1. using the all_albums data set we just created;\ngrouping by album name which means all operations are done within the context of each album; and\n tokenizing the data set which basically is a fancy way of saying we’re splitting up lyrics into words within each album.\n  Take a look at what this data frame now looks like by using\ntidy_lyrics %\u0026gt;% head() ## # A tibble: 6 x 5 ## # Groups: album [1] ## album track_title track_n line word ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 ***American Beauty*** Box of Rain 1 1 look ## 2 ***American Beauty*** Box of Rain 1 1 out ## 3 ***American Beauty*** Box of Rain 1 1 of ## 4 ***American Beauty*** Box of Rain 1 1 any ## 5 ***American Beauty*** Box of Rain 1 1 window ## 6 ***American Beauty*** Box of Rain 1 2 any Or simply use View(tidy_lyrics) to get a full view in a separate tab.\n Lemmatize Words For grammatical reasons, lyrics are going to use different forms of a word known as morphological derivations such as drink, drinks, and drinking to name a few. Additionally, there are families of derivationally related words with similar meanings known as differentiated inflections such as democracy, democratic, and democratization. In many situations including lyrics, it is useful to use the basic term for each of these variants. For this we have stemming and lemmatization.\n Stemming refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n Lemmatization refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n  The textstem package can do both, but for reasons associated with robustness, we’ll use lemmatization.\ntidy_lyrics$word \u0026lt;- lemmatize_words(tidy_lyrics$word) The command (right) above takes the column word in tidy_lyrics, lemmatizes it, and then replaces the column word in tidy_lyrics (left) with the results. Take a look using View(tidy_lyrics).\n Using Stop Words Now that we have a list in a tidy format, let’s do something with it. First let’s remove all of the stop words. Recall that these are terms that are commonly used (e.g. the, an, in, etc) and for this type of analysis, are noise and basically useless. To do this, we’re going to use some logic! The approach that is the most efficient besides making someone else do it is to utilize joins. If you want to know more about joins in R - which you definitely should get familiar with as it will make merging of data sets so much easier in your life - check out Dr. Jenny Bryant’s Stat545 course.\nIn our case, we’ll be using an anti_join which basically gives you an output by finding the rows of the first table cannot that cannot be matched in the second table. But before that, let’s pull out the stop words from the tidytext package by doing\ndata(\u0026quot;stop_words\u0026quot;) and then we’ll remove those words from our tidy data frame using an anti_join\ntidy_lyrics_nsw \u0026lt;- tidy_lyrics %\u0026gt;% ungroup() %\u0026gt;% anti_join(stop_words) ## Joining, by = \u0026quot;word\u0026quot;  \nOk so let’s take another moment to get our bearings straight. Currently we are\nusing the tidy_lyrics data set that we created;\n ungrouping the data set; and\n removing all of the stop words from that data frame.\n   \nNow you can take a look by using View(tidy_lyrics_nsw) or just look at the top six using the following syntax\ntidy_lyrics_nsw %\u0026gt;% head() ## # A tibble: 6 x 5 ## album track_title track_n line word ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 ***American Beauty*** Box of Rain 1 1 window ## 2 ***American Beauty*** Box of Rain 1 2 morning ## 3 ***American Beauty*** Box of Rain 1 2 day ## 4 ***American Beauty*** Box of Rain 1 3 sun ## 5 ***American Beauty*** Box of Rain 1 3 shine ## 6 ***American Beauty*** Box of Rain 1 4 bird  Not seeing what you want? How about the bottom six then?\ntidy_lyrics_nsw %\u0026gt;% tail() ## # A tibble: 6 x 5 ## album track_title track_n line word ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 ***Shakedown Street*** Stagger Lee (Live) 14 29 lee ## 2 ***Shakedown Street*** Stagger Lee (Live) 14 30 song ## 3 ***Shakedown Street*** Stagger Lee (Live) 14 30 delia ## 4 ***Shakedown Street*** Stagger Lee (Live) 14 30 sing ## 5 ***Shakedown Street*** Stagger Lee (Live) 14 30 stagger ## 6 ***Shakedown Street*** Stagger Lee (Live) 14 30 lee We can remove additional words manually but for the purposes of this walkthrough, let’s not.\n Descriptive Statistics Until recently, the quantitative approach to analyze open text was in the use of word clouds. For all of my griping, this isn’t necessarily a flawed approach since it will indicate if some words are more important than others but they are descriptive at best. Historically, the issue has been related to certain scholars making overarching claims, or inferences from what are essentially visual representations of frequency counts which you should know by now is nonsense. However, with the advent of machine learning as a methodological tool, we now use inferential statistics to derive usable outcomes though there are still some people who stick by frequencies. In any case, let’s calculate some frequencies to get an idea of how our data looks.\nfrequencies_lyrics \u0026lt;- tidy_lyrics_nsw %\u0026gt;% group_by(album) %\u0026gt;% count(word, sort = TRUE) and then plot them on a wordcloud. At this point, if you are unfamiliar or uncomfortable with ggplot, I suggest going through or reviewing the first ggplot section on datacamp. With that said, there will also be a brief explanation below of what’s used below the plot.\nOK if you’re ready, run the following but be forewarned, it may take some time!\nWordclouds set.seed(99) ggplot(frequencies_lyrics, aes(label = word, size = n, color = album)) + geom_text_wordcloud() + scale_radius(range = c(0, 20)) + theme_minimal() ## Warning: Removed 2 rows containing missing values (geom_text_wordcloud). So what’s going on above? Well let’s break it down:\nggplot(frequencies_lyrics, aes(label = word, size = n, color = album)) tells R to use the ggplot2 package by looking at the data set frequencies_lyrics with the aesthetics (aes) of labeling the data by the word, sizing each by its corresponding frequency (n), and color by album title (album).\n geom_text_wordcloud() tells R to use the ggwordcloud package and that text will be used to represent the data and to put it in a spiral.\n scale_radius(range = c(0, 20)) tells ggplot that the smallest a data point can be is 0 points and the largest is 20 points.\n theme_minimal is a ggplot theme that removes all default(background, axes, etc.) information and just plots what you ask for.\n  You may notice that I did not go over set.seed. That is for another time because it deals with probabilities that are contingent on how data is analyzed and/or rendered by each package in R.\nOK great but that takes too long to render, its not very pretty nor does it do a great job of splitting the words apart - we do want to compare them after all! Let’s make it better:\nset.seed(99) frequencies_lyrics %\u0026gt;% filter(n \u0026gt; 1) %\u0026gt;% ggplot(aes(label = word, size = n, color = album)) + scale_color_manual(values = c(\u0026quot;#5bc0de\u0026quot;, \u0026quot;#5cb85c\u0026quot;)) + geom_text_wordcloud(rm_outside = TRUE, shape = \u0026quot;circle\u0026quot;) + scale_radius(range = c(4, 15)) + theme_minimal() + facet_grid(.~ album) + theme(panel.spacing = unit(0.5, \u0026quot;cm\u0026quot;)) ## Warning: Removed 1 rows containing missing values (geom_text_wordcloud). ## Some words could not fit on page. They have been removed. ## Some words could not fit on page. They have been removed. That looks better and we know what words come from what album. So what’s different here? Well first of all, it is worth noting that not all of the syntax from above will be explained here simply due to the fact they are included for aesthetics and have nothing to do with the function creating the actual wordcloud. However, you are given the entire code so that the modified output is or will be at some point understandable. If you would like to know what this looks like without the extra aesthetics for comparison, run the following in your companion R script:\nset.seed(99) frequencies_lyrics %\u0026gt;% filter(n \u0026gt; 1) %\u0026gt;% ggplot(aes(label = word, size = n, color = album)) + scale_color_manual(values = c(\u0026quot;#5bc0de\u0026quot;, \u0026quot;#5cb85c\u0026quot;)) + geom_text_wordcloud(rm_outside = TRUE, shape = \u0026quot;circle\u0026quot;) + scale_radius(range = c(4, 15)) + theme_minimal() + facet_grid(album ~ .) Now back to the pending question that has been updated a bit: What are the major differences here that aren’t related to the aesthetics?\nIf you call frequencies_lyrics, you’ll notice that there are 684 rows implying that R has to plot 684 words. That takes some time! To reduce the time and before even calling ggplot, let’s get rid of the “one offs”, or those terms that only appear once. Here we   called the original data set,\n filtered the frequency n by telling R to only look at values greater than 1, and\n then called ggplot. One side note, notice that we don’t called the data set again in ggplot since we already did that when filtering. The %\u0026gt;% gives you the control to pass information through pipes. Much like anything Mario…pipes make everything better.\n  I have manually assigned colors by using the scale_color_manual command and inputting hexadecimal, or hex codes. These are one of two ways internet browsers determine what colors you see on screen (the other is called RBG) and most computer languages recognize them too. You can Google hex codes and find all sorts of examples. I suggest using this site which also includes user generated palettes, but obviously there is no mandate: https://www.color-hex.com/.\n (same as before)\n To make some of the text appear readable, the range of scale_radius was changed to reflect a minimum of 2 points.\n (same as before)\n facet_grid (and its counterpart facet_wrap - not shown here) is a way of splitting up a visualization by a value in a column within your data frame. If you were confused as to why the album name was important and that we binded the original data frames, this is the reason! There are ways to customize a facet but we’ll get to that later.\n  NOTE: If you see some blurred or overlapping text, just push the Zoom option right above the graphics window.\nYou can find more information, examples, and usable syntax/lines of code for ggwordcloud by selecting going to this vignette.\n Bar Plots One avenue to look at when comparing sets of open ended text is to compare terms, in that are common terms used? We can do this or we can dance if we want to. Let’s say we don’t do the latter:\nfrequencies_lyrics %\u0026gt;% filter(n \u0026gt; 5) %\u0026gt;% group_by(album) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ungroup() %\u0026gt;% mutate(album = reorder(album, n)) %\u0026gt;% ggplot(aes(word, n)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + facet_grid(~ album) + xlab(NULL) + coord_flip() + theme(axis.text.y = element_text(size = 8)) There is a lot going on here! While some of these items may be repetitive, let’s break it down anyway:\nThe frequencies_lyrics data set has been called.\n We use filter(n \u0026gt; 5) to filter out all of the word counts that are 5 or less. If you are using your own data set, this will vary.\n Since we are looking at albums, we should first pair those out by using group_by(album).\n Using count(word, sort = TRUE), we count the number of words. Now you may be saying something like “Wait! didn’t we already do that?” Well we did but we counted them across both albums basically ignoring which word came from what album. This time, when we used group_by(album), the system now performs all operations by album. So words are now counted within each album.\n If you run the entire pipe chain up to this count(word, sort = TRUE), you may have noticed that the words are in order by counts. We can reorder the column word by a particular word and then their respective counts n by using mutate(word = reorder(word, n)).\n When you group something, that structure is maintained indefinitely. To remove it, we use the common ungroup().\n Similar to step 5, we can reorder the column album by a particular album and then their respective counts n by using mutate(album = reorder(album, n)).\n  At this point, you may realize we are starting to plot. The great thing about the tidyverse universe is that you can do operations and plot in one fell swoop.\nggplot(aes(word, n)) tells R that we’re going to plot something using ggplot using the a word on the x-axis and its corresponding count n on the y-axis.\n geom_bar(stat = \"identity\") indicates that we are going to use as bar plot. For the time being, we won’t discuss the stat = identity part except to tell you that when a y variable is declared, geom_bar requires stat = identity.\n facet_grid(~ albums) simply means we are going to facet, or ploy by albums.\n xlab(NULL) tells the system to ignore the label on the x-axis which in this case is word.\n Running the command up to this point yields vertical bar plots which are somewhat difficult to compare. They would probably be easier to interpret if the bars were horizontal. Well coord_flip() does just that! This flips the x- and y-axes as well.\n We will discuss themes at a later point but so you can simply read the vertical axis with the words, we are going to make the font size 8 (pixels) by stating theme(axis.text.y = element_text(size = 8)).\n     Sentiment Analysis Now we’ll investigate the various sentiments and emotions expressed in the lyrics by using three sentiment dictionaries or lexicons, included with the tidytext package: Bing, NRC, and AFINN. I won’t cover what these are in great depth but in a nutshell\n  lexicon  description  variable type  sentiments  more information      Bing  A backend of Microsoft Bing), this lexicon may be used to assesses open text for its polarities in sentiments.  categorical  positive, negative  Bing    NRC  One of the most utilized and researched lexicons that provides information about emotional context.  categorical  anger, anticipation, disgust, fear, joy, sadness, surprise, trust  NRC    AFINN  A lexicon used for measuring psychological valence by assigning a level of severity to a term.  numerical  -5,-4,-3,-2,-1,0,1,2,3,4,5  AFINN     We can categorize the lyrics and then perform some text based transformations and manipulations to construct some visualizations.\nUsing the tidy_lyrics_nsw data set that we created earlier, we will filter out any numbers in our data set and\nGet the Bing lexicon with the get_sentiments() function.\n Join the Bing lexicon to the tokenized data set, specify by = “word”\n  emotions_lyrics_bing \u0026lt;- tidy_lyrics_nsw %\u0026gt;% filter(!grepl(\u0026#39;[0-9]\u0026#39;, word)) %\u0026gt;% left_join(get_sentiments(\u0026quot;bing\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% group_by(album) %\u0026gt;% mutate(sentiment = ifelse(is.na(sentiment), \u0026#39;neutral\u0026#39;, sentiment)) emotions_lyrics_bing ## # A tibble: 1,605 x 6 ## # Groups: album [2] ## album track_title track_n line word sentiment ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 ***American Beauty*** Box of Rain 1 1 window neutral ## 2 ***American Beauty*** Box of Rain 1 2 morning neutral ## 3 ***American Beauty*** Box of Rain 1 2 day neutral ## 4 ***American Beauty*** Box of Rain 1 3 sun neutral ## 5 ***American Beauty*** Box of Rain 1 3 shine positive ## 6 ***American Beauty*** Box of Rain 1 4 bird neutral ## 7 ***American Beauty*** Box of Rain 1 4 wing neutral ## 8 ***American Beauty*** Box of Rain 1 4 rain neutral ## 9 ***American Beauty*** Box of Rain 1 4 fall negative ## 10 ***American Beauty*** Box of Rain 1 4 heavy neutral ## # … with 1,595 more rows Again, there is a lot going on here! Let’s break it down line by line:\nThe tidy_lyrics_nsw data set has been called. We are using this rather than tidy_lyrics because the stop words were removed.\n We use filter(!grepl('[0-9]', word)) to filter out any numbers in our data set, namely in the word column.\n To get the Bing lexicon and compare it to our list of words, we use left_join(get_sentiments(\"bing\"), by = \"word\").\n Like before, we used group_by(album) to perform all operations by album (or within each album if you prefer).\n To perform most any operation on a data set when using %\u0026gt;%, we use mutate. In this case, we are using a logic statement to tell the program if it sees an entry with NA in the sentiment column, change it to the term neutral.\n  For a total count, we delineate the sentiments and count how many are in each by album using count(sentiment) which uses the sentiment column.\nemotions_lyrics_bing %\u0026gt;% count(sentiment) ## # A tibble: 6 x 3 ## # Groups: album [2] ## album sentiment n ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 ***American Beauty*** negative 62 ## 2 ***American Beauty*** neutral 703 ## 3 ***American Beauty*** positive 62 ## 4 ***Shakedown Street*** negative 76 ## 5 ***Shakedown Street*** neutral 619 ## 6 ***Shakedown Street*** positive 83 Basic Text Analysis Most Common Positive and Negative Words in a Bar Graph We can associate terms tagged with a negative sentiment with negative numbers to better visualize them. To accomplish this, we can create a logic statement called an if-else statement that assigns -n to a word with a negative sentiment and to everything else, it provides an n count. We’ll then order the words in these groups by the number of times they appear in the plot.\nword_count \u0026lt;- emotions_lyrics_bing %\u0026gt;% count(word, sentiment, sort = TRUE) word_count ## # A tibble: 693 x 4 ## # Groups: album [2] ## album word sentiment n ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 ***Shakedown Street*** love positive 45 ## 2 ***Shakedown Street*** lovin neutral 25 ## 3 ***Shakedown Street*** fire neutral 24 ## 4 ***Shakedown Street*** doctor neutral 17 ## 5 ***American Beauty*** home neutral 16 ## 6 ***Shakedown Street*** stagger neutral 16 ## 7 ***American Beauty*** friend neutral 14 ## 8 ***American Beauty*** time neutral 14 ## 9 ***Shakedown Street*** lee neutral 14 ## 10 ***Shakedown Street*** mountain neutral 14 ## # … with 683 more rows In the above, we are going to\nUse the emotions_lyrics_bing data set.\n Count up the words by sentiment and then sort them by count(word, sentiment, sort = TRUE)\n  top_sentiments_bing \u0026lt;- word_count %\u0026gt;% filter(sentiment != \u0026#39;neutral\u0026#39;) %\u0026gt;% group_by(sentiment) %\u0026gt;% top_n(10, n) %\u0026gt;% mutate(num = ifelse(sentiment == \u0026quot;negative\u0026quot;, -n, n)) %\u0026gt;% select(-n) %\u0026gt;% mutate(word = reorder(word, num)) %\u0026gt;% ungroup() top_sentiments_bing ## # A tibble: 21 x 4 ## album word sentiment num ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 ***Shakedown Street*** love positive 45 ## 2 ***American Beauty*** easy positive 13 ## 3 ***American Beauty*** sweet positive 9 ## 4 ***American Beauty*** devil negative -7 ## 5 ***Shakedown Street*** dark negative -6 ## 6 ***American Beauty*** fall negative -5 ## 7 ***American Beauty*** lie negative -5 ## 8 ***American Beauty*** numb negative -5 ## 9 ***Shakedown Street*** dead negative -5 ## 10 ***Shakedown Street*** miracle positive 5 ## # … with 11 more rows Let’s break it down line by line:\nThe word_count data set has been called.\n We use filter(sentiment != 'neutral') to filter out any term in the sentiment column that is neutral.\n To get the top 10 of words used in each album use top_n(10, n).\n We use a logic statement mutate(num = ifelse(sentiment == \"negative\", -n, n)) to tell R that if it sees the term neutral in the sentiment column, to change the the count associated with it to a negative in a new column called num.\n Using select(-n) tells the program that we want to get rid of the the column n. You can think of the negative as a way to deselect a column.\n We simply put the terms from greatest to least disregarding album by mutate(word = reorder(word, num)).\n  ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) + geom_bar(stat = \u0026#39;identity\u0026#39;, alpha = 0.75) + scale_fill_manual(guide = FALSE, values = c(\u0026quot;#d9534f\u0026quot;, \u0026quot;#428bca\u0026quot;)) + scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7)) + labs(x = \u0026#39;\u0026#39;, y = \u0026quot;Number of Occurrences\u0026quot;, title = \u0026#39;Top Sentiments of Lyrics\u0026#39;, subtitle = \u0026#39;Most Common Positive and Negative Words\u0026#39;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), plot.subtitle = element_text(hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1, size = 14 , face = \u0026quot;bold\u0026quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 1.1), panel.spacing = unit(0.5, \u0026quot;cm\u0026quot;)) + facet_wrap(album ~ ., scales = \u0026quot;free_x\u0026quot;) Now there a bunch of things going on in this bar plot too!\nHere we are using the top_sentiments_bing data set but reordering the way its plotted by greatest to least and filling by positive or negative sentiment by aes(reorder(word, num), num, fill = sentiment).\n geom_bar is a bar graph as we saw before, but here the transparency level is set to 75% by alpha = 0.75.\n The positive and negative colors are set to a specific blue (#d9534f) and red (#428bca), respectively by scale_fill_manual(guide = FALSE, values = c(\"#d9534f\", \"#428bca\")). Additionally, the legend has been turned off as well.\n The y-axis is limited to plotting between -10 and 55 with an even number of breaks given by scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7)).\n labs provides the opportunity to name the axes, title and subtitle.\n One of the default themes that can be used by ggplot is theme_bw() or a black and white theme.\n The values along the x-axis can be aesthetically manipulated to be at a 45 degree angle in size 14 bold print right-adjusted by axis.text.x = element_text(angle = 45, hjust = 1, size = 14, face = \"bold\").\n   Most Common Positive and Negative Words in a WordCloud To compare the words within each album, we can initially use a comparison wordcloud. Now we can create this using ggwordcloud as before, but if you just want a quick look, an older package named wordcloud does a pretty good job and it has a built in function for comparisons.\nLet’s take a look at the album American Beauty first:\nemotions_lyrics_bing %\u0026gt;% filter(album == \u0026quot;***American Beauty***\u0026quot;) %\u0026gt;% filter(sentiment != \u0026quot;neutral\u0026quot;) %\u0026gt;% count(word, sentiment, sort = TRUE) %\u0026gt;% spread(sentiment, n, fill = 0L) %\u0026gt;% as.data.frame() %\u0026gt;% remove_rownames() %\u0026gt;% column_to_rownames(\u0026quot;word\u0026quot;) %\u0026gt;% select(-album) %\u0026gt;% comparison.cloud(colors = c(\u0026quot;#d9534f\u0026quot;, \u0026quot;#428bca\u0026quot;), title.size = 1.5) and then Shakedown Street:\nemotions_lyrics_bing %\u0026gt;% filter(album == \u0026quot;***Shakedown Street***\u0026quot;) %\u0026gt;% filter(sentiment != \u0026quot;neutral\u0026quot;) %\u0026gt;% count(word, sentiment, sort = TRUE) %\u0026gt;% spread(sentiment, n, fill = 0L) %\u0026gt;% as.data.frame() %\u0026gt;% remove_rownames() %\u0026gt;% column_to_rownames(\u0026quot;word\u0026quot;) %\u0026gt;% select(-album) %\u0026gt;% comparison.cloud(colors = c(\u0026quot;#d9534f\u0026quot;, \u0026quot;#428bca\u0026quot;), title.size = 1.5) Without being repetitive, the only command you haven’t seen is spread which essentially takes a long data set and converts it into a wide one. Both spread and its counterpart gather are commands that are used often and worth your time getting to know a bit about, though the current incarnations of these known as pivot_wider and pivot_longer. If interested, take a look at the online version of the text R for Data Science.\n Positive and Negative Words in a Line Graph Now that we’ve looked at the most common words for either positive or negative sentiment, what proportion of these sentiments are present in within the entire lyrical data set?\npos_neg_bing_album \u0026lt;- tidy_lyrics_nsw %\u0026gt;% filter(!grepl(\u0026#39;[0-9]\u0026#39;, word)) %\u0026gt;% left_join(get_sentiments(\u0026quot;bing\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% mutate(sentiment = ifelse(is.na(sentiment), \u0026#39;neutral\u0026#39;, sentiment)) %\u0026gt;% group_by(album, sentiment) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(percent = n / sum(n)) %\u0026gt;% select(-n) %\u0026gt;% ungroup()  ## `summarise()` regrouping output by \u0026#39;album\u0026#39; (override with `.groups` argument) pos_neg_bing_album ## # A tibble: 6 x 3 ## album sentiment percent ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ***American Beauty*** negative 0.0750 ## 2 ***American Beauty*** neutral 0.850 ## 3 ***American Beauty*** positive 0.0750 ## 4 ***Shakedown Street*** negative 0.0977 ## 5 ***Shakedown Street*** neutral 0.796 ## 6 ***Shakedown Street*** positive 0.107 pos_neg_bing_album %\u0026gt;% filter(sentiment != \u0026quot;neutral\u0026quot;) %\u0026gt;% ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + geom_line(size = 1) + geom_point(size = 3) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + labs(x = \u0026quot;Album\u0026quot;, y = \u0026quot;Emotion Words Count (as %)\u0026quot;) + scale_color_manual(values = c(positive = \u0026quot;#d9534f\u0026quot;, negative = \u0026quot;#428bca\u0026quot;)) + ggtitle(\u0026quot;Proportion of Positive and Negative Words by Album\u0026quot;, subtitle = \u0026quot;Bing lexicon\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), plot.subtitle = element_text(hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = \u0026quot;bold\u0026quot;), axis.title.x = element_blank(), axis.text.y = element_text(size = 11, face = \u0026quot;bold\u0026quot;)) Well while this isn’t overtly interesting, it does provide some indication that while Shakedown Street is slightly more positive, it is also more negative as well. Maybe we’ll get a better idea by looking at the individual songs:\npos_neg_bing_track \u0026lt;- tidy_lyrics_nsw %\u0026gt;% filter(!grepl(\u0026#39;[0-9]\u0026#39;, word)) %\u0026gt;% left_join(get_sentiments(\u0026quot;bing\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% mutate(sentiment = ifelse(is.na(sentiment), \u0026#39;neutral\u0026#39;, sentiment)) %\u0026gt;% group_by(album, track_title, track_n, sentiment) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(percent = n / sum(n)) %\u0026gt;% select(-n)  ## `summarise()` regrouping output by \u0026#39;album\u0026#39;, \u0026#39;track_title\u0026#39;, \u0026#39;track_n\u0026#39; (override with `.groups` argument) pos_neg_bing_track ## # A tibble: 64 x 5 ## # Groups: album, track_title, track_n [22] ## album track_title track_n sentiment percent ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ***American Beauty*** Attics of My Life 9 negative 0.04 ## 2 ***American Beauty*** Attics of My Life 9 neutral 0.92 ## 3 ***American Beauty*** Attics of My Life 9 positive 0.04 ## 4 ***American Beauty*** Box of Rain 1 negative 0.0741 ## 5 ***American Beauty*** Box of Rain 1 neutral 0.889 ## 6 ***American Beauty*** Box of Rain 1 positive 0.0370 ## 7 ***American Beauty*** Brokedown Palace 7 negative 0.0392 ## 8 ***American Beauty*** Brokedown Palace 7 neutral 0.892 ## 9 ***American Beauty*** Brokedown Palace 7 positive 0.0686 ## 10 ***American Beauty*** Candyman 5 negative 0.0615 ## # … with 54 more rows pos_neg_bing_track %\u0026gt;% filter(sentiment != \u0026quot;neutral\u0026quot;) %\u0026gt;% ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, group = sentiment)) + geom_line(size = 1) + geom_point(size = 3) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + labs(x = \u0026quot;Album\u0026quot;, y = \u0026quot;Sentiment Count (as %)\u0026quot;) + scale_color_manual(values = c(positive = \u0026quot;#d9534f\u0026quot;, negative = \u0026quot;#428bca\u0026quot;)) + ggtitle(\u0026quot;Proportion of Positive and Negative Words by Track\u0026quot;, subtitle = \u0026quot;Bing lexicon\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), plot.subtitle = element_text(hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = \u0026quot;bold\u0026quot;), axis.title.x = element_blank(), axis.text.y = element_text(size = 11, face = \u0026quot;bold\u0026quot;)) + facet_wrap(. ~ album, ncol = 1, scales = \u0026quot;free_x\u0026quot;) Firstly, you may notice that geom_line, geom_point, and reorder(track_title, track_n). The first two are fairly obvious as they are parts of the visualization that render the dots and lines. In the latter, we arrange the tracks by their original order since many artists tend to itemize tracks in a preferred way. Sometimes this is intended to tell a story1 while other times it is simply a personal or studio preference.\nVisually, the ebbs and flows of the sentiments within each album are relatively consistent and are reflections of each other as one would expect. In fact they both have an instance where the sentiments have a declining slop. The primary differential is the area between the positive and negative curves which is greater in the album Shakedown Street than American Beauty. In fact, it is this disparity that is the main contribution to the earlier variant of this plot which looked at the albums as a whole.\n Positive and Negative Words in a Boxplot No we switch our attention to something that may be of greater interest, that is the NRC lexicon. Recall this dictionary not only has positive and negative categories, it also has eight different emotional classifications: Anger, Anticipation, Disgust, Fear, Joy, Sadness, Surprise, and Trust.\nIn this case we will disregard the positive and negative tags but please note that what Bing classifies by either tag may not necessarily be the same as how NRC groups them. In a nutshell, both Bing and NRC are different lexicons from two different sources so it would make sense that there would be a discrepancy between classifications.\nUsing a similar approach as before, we first look at the differentials by album:\nemotions_album_nrc \u0026lt;- tidy_lyrics_nsw %\u0026gt;% left_join(get_sentiments(\u0026quot;nrc\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% filter(!(sentiment == \u0026quot;negative\u0026quot; | sentiment == \u0026quot;positive\u0026quot;)) %\u0026gt;% mutate(sentiment = as.factor(sentiment)) %\u0026gt;% group_by(album, sentiment) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(percent = n / sum(n)) %\u0026gt;% select(-n) %\u0026gt;% ungroup()  ## `summarise()` regrouping output by \u0026#39;album\u0026#39; (override with `.groups` argument) emotions_album_nrc  ## # A tibble: 16 x 3 ## album sentiment percent ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ***American Beauty*** anger 0.0611 ## 2 ***American Beauty*** anticipation 0.195 ## 3 ***American Beauty*** disgust 0.0611 ## 4 ***American Beauty*** fear 0.0656 ## 5 ***American Beauty*** joy 0.208 ## 6 ***American Beauty*** sadness 0.143 ## 7 ***American Beauty*** surprise 0.0905 ## 8 ***American Beauty*** trust 0.176 ## 9 ***Shakedown Street*** anger 0.0892 ## 10 ***Shakedown Street*** anticipation 0.140 ## 11 ***Shakedown Street*** disgust 0.0255 ## 12 ***Shakedown Street*** fear 0.178 ## 13 ***Shakedown Street*** joy 0.219 ## 14 ***Shakedown Street*** sadness 0.110 ## 15 ***Shakedown Street*** surprise 0.104 ## 16 ***Shakedown Street*** trust 0.134 and then the corresponding plot.\nemotions_album_nrc %\u0026gt;% ggplot() + geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + scale_fill_brewer(palette = \u0026quot;Spectral\u0026quot;) + ggtitle(\u0026quot;Distribution of Sentiments by Album\u0026quot;) + labs(x = \u0026quot;Sentiment\u0026quot;, y = \u0026quot;Percentage\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), legend.position = \u0026quot;none\u0026quot;, axis.text.x = element_text(size = 11, face = \u0026quot;bold\u0026quot;), axis.text.y = element_text(size = 11, face = \u0026quot;bold\u0026quot;)) + facet_wrap(. ~ album, ncol = 1, scales = \u0026quot;free\u0026quot;) The lack of boxes isn’t surprising considering we only have one value for each sentiment per album. What is interesting is that the percent of terms associated with anger, anticipation, fear and trust are elevated in Shakedown Street.\nNow a bit of social commentary: These emotions tend to reflect the inner turmoil going on within the band where two of the staple members (Keith and Donna Jean Godchaux) left the band after it was reportedly found that Donna Jean was having an affair with another band member by the name of Bob Weir. Let’s see if this is reflected in the songs themselves.\nemotions_tracks_nrc \u0026lt;- tidy_lyrics_nsw %\u0026gt;% left_join(get_sentiments(\u0026quot;nrc\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% filter(!(sentiment == \u0026quot;negative\u0026quot; | sentiment == \u0026quot;positive\u0026quot;)) %\u0026gt;% mutate(sentiment = as.factor(sentiment)) %\u0026gt;% group_by(album, track_title, track_n, sentiment) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(percent = n / sum(n)) %\u0026gt;% select(-n) %\u0026gt;% ungroup()  ## `summarise()` regrouping output by \u0026#39;album\u0026#39;, \u0026#39;track_title\u0026#39;, \u0026#39;track_n\u0026#39; (override with `.groups` argument) emotions_tracks_nrc  ## # A tibble: 149 x 5 ## album track_title track_n sentiment percent ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ***American Beauty*** Attics of My Life 9 anger 0.0625 ## 2 ***American Beauty*** Attics of My Life 9 anticipation 0.25 ## 3 ***American Beauty*** Attics of My Life 9 disgust 0.0625 ## 4 ***American Beauty*** Attics of My Life 9 joy 0.125 ## 5 ***American Beauty*** Attics of My Life 9 sadness 0.188 ## 6 ***American Beauty*** Attics of My Life 9 surprise 0.0625 ## 7 ***American Beauty*** Attics of My Life 9 trust 0.25 ## 8 ***American Beauty*** Box of Rain 1 anger 0.0345 ## 9 ***American Beauty*** Box of Rain 1 anticipation 0.276 ## 10 ***American Beauty*** Box of Rain 1 fear 0.103 ## # … with 139 more rows and then the corresponding plot.\nemotions_tracks_nrc %\u0026gt;% ggplot() + geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + scale_fill_brewer(palette = \u0026quot;Spectral\u0026quot;) + ggtitle(\u0026quot;Distribution of Sentiments by Album Aggregated by Track\u0026quot;) + labs(x = \u0026quot;Detected Sentiments\u0026quot;, y = \u0026quot;Percentage\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), legend.position = \u0026quot;none\u0026quot;, axis.text.x = element_text(size = 11, face = \u0026quot;bold\u0026quot;), axis.text.y = element_text(size = 11, face = \u0026quot;bold\u0026quot;)) + facet_wrap(. ~ album, ncol = 1, scales = \u0026quot;free_x\u0026quot;) So when we look at the scores by track and then aggregate, we see the the original findings seem to be correct though there is great deal more variability in joy within Shakedown Street that wasn’t detected earlier (maybe it was the disco). Additionally there appears to be a greater association with sadness in American Beauty that was not found earlier as well. While you may not be familiar with the album, the new findings here are also not a bug surprise. The album is known for its polar track sequencing where an extremely upbeat song (reflected in joy) would be followed by one that is sorrowful (reflected almost equally by sadness).\nWhat if we wanted to see how the songs changed by track order? We can use a bump chart to visualize this\nemotions_tracks_nrc %\u0026gt;% ggplot(aes(reorder(track_title, track_n), percent, color = sentiment, group = sentiment)) + geom_line(size = 1.5) + geom_point(size = 3.5) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + xlab(\u0026quot;Album\u0026quot;) + ylab(\u0026quot;Proportion of Sentiments\u0026quot;) + ggtitle(\u0026quot;Sentiments by Albums\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = \u0026quot;bold\u0026quot;), axis.title.x = element_blank(), axis.text.y = element_text(size = 11, face = \u0026quot;bold\u0026quot;)) + scale_color_brewer(palette = \u0026quot;Spectral\u0026quot;) + facet_wrap(. ~ album, ncol = 1, scales = \u0026quot;free_x\u0026quot;) Well that got messy. Even with the colors, it is hard to discern what is going on. This is a good instance where you can use facet_grid. Until now, we have been using facet_wrap to separate the album variable into its distinct entries. However the functionality of facet_wrap is limited and will return a symmetrical matrix of plots for the number of unique levels or factors of a given variable. facet_grid serves another purpose by return facets equal to the levels or factors of a given variable. Think of it more this way -\n facet_wrap essentially splits your plot across the categories you want.\n facet_grid does a similar thing but instead of creating different plots it creates different grids and then plots each plot in the grids.\n  Still confused? Well let’s apply it and see.\nemotions_tracks_nrc %\u0026gt;% ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, group = sentiment)) + geom_line(size = 1.5) + geom_point(size = 3.5) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + xlab(\u0026quot;Album\u0026quot;) + ylab(\u0026quot;Proportion of Sentiments\u0026quot;) + ggtitle(\u0026quot;Individual Sentiments by Album\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = \u0026quot;bold\u0026quot;), axis.title.x = element_blank(), axis.text.y = element_text(size = 8, face = \u0026quot;bold\u0026quot;)) + scale_color_brewer(palette = \u0026quot;Spectral\u0026quot;) + facet_grid(sentiment ~ album, scales = \u0026quot;free_x\u0026quot;) We only changed facet_wrap(. ~ album, ncol = 1, scales = \"free_x\") in the messy plot to facet_grid(sentiment ~ album, scales = \"free_x\") in the new one. Sure it could still use some work on the aesthetics side of things but the line graphs are now provided by type, or facet which makes a visual comparison much easier.\nWith a few exceptions, you may notice that the sentiments share a common shape when comparing tracks in order. This consistency is not an outlier. One of the reasons you may enjoy a follow-up album by an artist you liked before is hypothesized to having a similar line of best fit. Don’t believe me? Try it yourself and hopefully you remember some basic terms from Algebra.\nFirst let’s define a binomial function\nbinomial_smooth \u0026lt;- function(...) { geom_smooth(method = \u0026quot;glm\u0026quot;, method.args = list(family = \u0026quot;binomial\u0026quot;), ...) } and then apply it as a third degree polynomial\nemotions_tracks_nrc %\u0026gt;% ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, group = sentiment)) + geom_line(size = 1.5) + geom_point(size = 3.5) + binomial_smooth(formula = y ~ splines::ns(x, 3), color = \u0026quot;#000000\u0026quot;) + scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) + xlab(\u0026quot;Album\u0026quot;) + ylab(\u0026quot;Proportion of Sentiments\u0026quot;) + ggtitle(\u0026quot;Individual Sentiments by Album\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = \u0026quot;bold\u0026quot;), axis.title.x = element_blank(), axis.text.y = element_text(size = 8, face = \u0026quot;bold\u0026quot;)) + scale_color_brewer(palette = \u0026quot;Spectral\u0026quot;) + facet_grid(sentiment ~ album, scales = \u0026quot;free_x\u0026quot;) Change the number 3 in binomial_smooth(formula = y ~ splines::ns(x, 3), color = \"#000000\") to amend the degree as you see fit. In any case, you may notice that the line of best fit is relatively consistent between the two albums when assessing sentiments in track order.\n  Viewing Across all Lexicons Let’s compare the lexicons themselves on how many positive and negative words they each categorize.\nget_sentiments(\u0026quot;bing\u0026quot;) %\u0026gt;% count(sentiment) ## # A tibble: 2 x 2 ## sentiment n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 negative 4781 ## 2 positive 2005 get_sentiments(\u0026quot;nrc\u0026quot;) %\u0026gt;% count(sentiment) ## # A tibble: 10 x 2 ## sentiment n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 anger 1247 ## 2 anticipation 839 ## 3 disgust 1058 ## 4 fear 1476 ## 5 joy 689 ## 6 negative 3324 ## 7 positive 2312 ## 8 sadness 1191 ## 9 surprise 534 ## 10 trust 1231  In the bing lexicon, there are 4781 negative and 2005 positive terms.\n In the nrc lexicon, there are 3324 negative and 2312 positive terms.\n  get_sentiments(\u0026quot;afinn\u0026quot;) %\u0026gt;% count(value) ## # A tibble: 11 x 2 ## value n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 -5 16 ## 2 -4 43 ## 3 -3 264 ## 4 -2 966 ## 5 -1 309 ## 6 0 1 ## 7 1 208 ## 8 2 448 ## 9 3 172 ## 10 4 45 ## 11 5 5  In the AFINN lexicon, the measure with regards to the severity of a term being positive or negative, rather than just treating a word as one or the other. However, we can just treat the negative measures as negative sentiments and the positive measures as positive sentiments just to get a general idea of the total number. With that in mind, we can do the following:  get_sentiments(\u0026quot;afinn\u0026quot;) %\u0026gt;% select(value) %\u0026gt;% mutate(sentiment = if_else(value \u0026gt; 0, \u0026quot;positive\u0026quot;, \u0026quot;negative\u0026quot;, \u0026quot;NA\u0026quot;)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(sum = n()) %\u0026gt;% filter(sentiment == \u0026quot;positive\u0026quot; | sentiment == \u0026quot;negative\u0026quot;) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sentiment sum ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 negative 1599 ## 2 positive 878  In the AFINN lexicon, there are (generally) 1599 negative and 877 positive terms.  Now taking a look at our data set, let’s see how our terms get tagged.\nemotions_lyrics_bing %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(sum = n()) %\u0026gt;% filter(sentiment == \u0026quot;positive\u0026quot; | sentiment == \u0026quot;negative\u0026quot;) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sentiment sum ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 negative 138 ## 2 positive 145 bing tags 134 of our terms as negative and 143 as positive.\ntidy_lyrics_nsw %\u0026gt;% left_join(get_sentiments(\u0026quot;nrc\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(sum = n()) %\u0026gt;% filter(sentiment == \u0026quot;positive\u0026quot; | sentiment == \u0026quot;negative\u0026quot;) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sentiment sum ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 negative 145 ## 2 positive 264 However, nrc tags 140 of our terms as negative and 258 as positive.\nemotions_lyrics_afinn \u0026lt;- tidy_lyrics_nsw %\u0026gt;% left_join(get_sentiments(\u0026quot;afinn\u0026quot;), by = \u0026quot;word\u0026quot;) %\u0026gt;% filter(!grepl(\u0026#39;[0-9]\u0026#39;, word)) emotions_lyrics_afinn %\u0026gt;% select(value) %\u0026gt;% mutate(sentiment = if_else(value \u0026gt; 0, \u0026quot;positive\u0026quot;, \u0026quot;negative\u0026quot;, \u0026quot;NA\u0026quot;)) %\u0026gt;% group_by(sentiment) %\u0026gt;% summarize(sum = n()) %\u0026gt;% filter(sentiment == \u0026quot;positive\u0026quot; | sentiment == \u0026quot;negative\u0026quot;) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sentiment sum ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 negative 116 ## 2 positive 155 Finally AFINN (generally) tags 113 of our terms as negative and 153 as positive.\nSo you can see that the sentiments are dependent on lexicons. Is this a bad thing? Well it depends on what you are using it for.\nNow let’s put all of the lexicons together and see how they compare. Note that there is a lot below which will not have a line-by-line explanation. At this point, try reading the code to figure out what is occurring. You may find this to be easier when using albums of artists that you enjoy. In any case, please ask questions if needed!\n First we calculate AFINN sentiment scores and compare them to the list of terms we already have.\nafinn_scores \u0026lt;- emotions_lyrics_afinn %\u0026gt;% replace_na(replace = list(value = 0)) %\u0026gt;% group_by(index = album, track_title) %\u0026gt;% summarize(sentiment = sum(value)) %\u0026gt;% mutate(lexicon = \u0026quot;AFINN\u0026quot;) ## `summarise()` regrouping output by \u0026#39;index\u0026#39; (override with `.groups` argument) We then combine both the bing and nrc lexicons into one data frame and calculate the sentiment scores for each.\nbing_nrc_scores \u0026lt;- bind_rows( tidy_lyrics_nsw %\u0026gt;% inner_join(get_sentiments(\u0026quot;bing\u0026quot;)) %\u0026gt;% mutate(lexicon = \u0026quot;Bing\u0026quot;), tidy_lyrics %\u0026gt;% inner_join(get_sentiments(\u0026quot;nrc\u0026quot;) %\u0026gt;% filter(sentiment %in% c(\u0026quot;positive\u0026quot;, \u0026quot;negative\u0026quot;))) %\u0026gt;% mutate(lexicon = \u0026quot;NRC\u0026quot;)) %\u0026gt;% # from here we count the sentiments, spread on positive/negative, # then create the final sentiment score: count(lexicon, index = album, track_title, sentiment) %\u0026gt;% spread(sentiment, n, fill = 0) %\u0026gt;% mutate(lexicon = as.factor(lexicon), sentiment = positive - negative) ## Joining, by = \u0026quot;word\u0026quot; ## Joining, by = \u0026quot;word\u0026quot; Finally we compile a list of all three lexicons using the bind_rows command.\nall_lexicons \u0026lt;- bind_rows(afinn_scores, bing_nrc_scores) %\u0026gt;% select(-negative, -positive) Try View(all_lexicons) if you want to view the resulting data frame. To make the final plot more interesting, let’s assign a palette using hex colors.\nlexicon_cols \u0026lt;- c(\u0026quot;AFINN\u0026quot; = \u0026quot;#ae5a41\u0026quot;, \u0026quot;NRC\u0026quot; = \u0026quot;#559e83\u0026quot;, \u0026quot;Bing\u0026quot; = \u0026quot;#1b85b8\u0026quot;) Finally we plot all of the lexicons by album and score.\nall_lexicons %\u0026gt;% ggplot(aes(track_title, sentiment, fill = lexicon)) + geom_col(show.legend = FALSE) + facet_wrap(~lexicon, ncol = 1, scales = \u0026quot;free_y\u0026quot;) + scale_fill_manual(values = lexicon_cols) + ggtitle(\u0026quot;Comparison of Sentiments\u0026quot;, subtitle = \u0026quot;by track order\u0026quot;) + labs(x = \u0026quot;Index of All Songs\u0026quot;, y = \u0026quot;Sentiment Score\u0026quot;) + theme_bw() + theme(plot.title = element_text(hjust=0.5), plot.subtitle = element_text(hjust=0.5), axis.text.x = element_blank()) + facet_grid(lexicon ~ index, scales = \u0026quot;free_x\u0026quot;) Well that is a lot to take in but this is essentially one way statistics can be applied! We have taken open text, broken it down into its basic terms, filtered out common terms, and used natural language processing (NLP) including various lexicons to derive sentiments.\nThis work is registered under the CC BY-NC-SA 3.0 Creative Commons License. Any items including, but not limited to lyrics, logos, and references associated with the Grateful Dead are licensed under Grateful Dead Productions unless otherwise noted. All rights reserved.\n   like Dark Side of the Moon by Pink Floyd or maybe that reference just dated me and you are even more confused.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"32c7f742d7f18c6f24d401ca43eb4c46","permalink":"/lesson/sentimentanalysisintro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/sentimentanalysisintro/","section":"lesson","summary":"Introduction Preparation Sentiment Analysis Learning by Doing Extra Information Getting Data  Data Wrangling Exploring Data Lemmatize Words Using Stop Words Descriptive Statistics Wordclouds Bar Plots   Sentiment Analysis Basic Text Analysis Most Common Positive and Negative Words in a Bar Graph Most Common Positive and Negative Words in a WordCloud Positive and Negative Words in a Line Graph Positive and Negative Words in a Boxplot  Viewing Across all Lexicons    Introduction Preparation Download a script file of just the R chunks used in this walkthrough.","tags":null,"title":"Week 15: Something Fun - An Introduction to Sentiment Analysis Using (My Favorite Band): The Grateful Dead","type":"docs"},{"authors":null,"categories":null,"content":"    Preparation Purpose Objectives Packages  The Tidyverse Package Tidy Data Basics The Pipe %\u0026gt;% Operator Gapminder mutate() adds new variables Try These Out group_by() operates on groups summarize() with group_by() arrange() orders columns  NFL Data Set Task Logical Operators Data Science Acknowledgements Don’t Get Bogged Down!   Preparation Download a script file of just the R chunks used in this walkthrough.\n Purpose What are some common things you like to do with your data? Maybe remove rows or columns, do calculations and maybe add new columns? This is called data wrangling. It’s not data management or data manipulation: you keep the raw data raw and do these things programatically in R with the tidyverse.\nYou are going to be introduced to data wrangling in R without using the base package, or “Base R.” The tidyverse is a suite of packages that match a philosophy of data science developed by Hadley Wickham and the RStudio team. It is a more straight-forward way to learn R. I encourage you to take a look around the tidyverse web page just to see everything it can do. You can fine it here: https://www.tidyverse.org/\nNow that you have had two weeks of utter frustration with “Base R”, which means, in R without using any additional packages (though we have used a bit of tidyverse), I will show you by comparison what code will look like in “Base R”. For some things, base-R is more straightforward and where tat is apparent, I will note it. Whenever we use a function that is from the tidyverse, we will prefix it so you’ll know for sure.\nObjectives  discuss tidying data read data from a csv file into R explore gapminder data with base-R functions wrangle gapminder data with dplyr from the tidyverse family of functions   Packages Please load up the following packages\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.4 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::group_rows() masks kableExtra::group_rows() ## x dplyr::lag() masks stats::lag() library(gapminder) Remember to download them if you receive an error:\ninstall.packages(\u0026quot;tidyverse\u0026quot;) install.packages(\u0026quot;gapminder\u0026quot;)   The Tidyverse Package The tidyverse package is actually a family of packages that have been constructed to take all kinds of data sets in multiple formats and to tidy them. Before we get into that concept, the typical path that we take when working with real world data sets is relatively simple…\n…yet the process can be extremely complex and time consuming as described below:\n Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information (NYTimes, 2014).\n tidyverse provides packages - given in green - that address all of these notions, but we won’t even be able to scratch the surface of most of them.\nIn this walk through, we’ll be concentrating on the use of pipes using the dplyr.\n  Tidy Data Let’s start off discussing tidy data which has simple convention: put variables in the columns and observations in the rows - amazing right?\nThere are three interrelated rules which make a data set tidy:\nEach variable must have its own column. Each observation must have its own row. Each value must have its own cell.  We are going to wrangle - yup that is a real term for messing with the structure of data - a tidy-ish data set (the Mutate part of the cycle), and then come back to tidying messy data using tidyr once we’ve gotten it into proper form.\nConceptually, making data tidy first is really critical. Instead of building your analyses around whatever format your data are in, we’ll take deliberate steps to make your data tidy. When your data are in this format, you can use a growing assortment of powerful analytical and visualization tools instead of inventing home-grown ways to accommodate your data. This will save you time since you aren’t reinventing the wheel, and will make your work more clear and understandable to your collaborators. Additionally after struggling with Base R and its arduous process, pipes will probably be something you welcome!\n Basics There are six functions in dplyr that you will primarily use to wrangle data. Remember that variables are the column names while observations are the values within a column.\n the filter() command which let’s you pick observations by their values.    the select() command which let’s you pick variables by their names.\n   the mutate() command which let’s you create new variables with functions of existing variables.   the summarise() command which let’s you collapse multiple values to a single summary.    the group_by() command which let’s you perform operations with respect to a variable.   the arrange() command which let’s you reorder the rows of a data frame.  (nope there isn’t a picture for this) Should you memorize these? NO! NEVER! DON’T DO IT! That’s what the internet is for! As with any data set, the main objective is to logically think about what you can do to achieve a goal. The commands are simply paths you could take to get there.\n The Pipe %\u0026gt;% Operator Pipes are a logical operator from the magrittr package that allows you to pass logic down a chain. Find that confusing? Let’s try to explain it another way:\nI %\u0026gt;% woke up %\u0026gt;% showered %\u0026gt;% got dressed %\u0026gt;% ate breakfast %\u0026gt;% showed up for work Here every act is dependent on all of the previous acts. This essentially signifies what pipes do, in that you can fit multiple commands in a row without having to do them one by one as in Base R. Pipes are given by the %\u0026gt;% symbol. In RStudio, the keyboard shortcut for a pipe is\n Cmd + Shift + M (Mac or Linux)\n Ctrl + Shift + M (Windows)\n  Gapminder In this walk through, we’ll be using Gapminder data, which represents the health and wealth of nations. It was pioneered by Hans Rosling, who is famous for describing the prosperity of nations over time through famines, wars and other historic events with an interactive data visualization in his 2006 TED Talk: The best stats you’ve ever seen which you can access by selecting the image on the next page.\n\nLet’s take a look at the gapminder data set.\nhead(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. Now using pipes, we could have done\ngapminder %\u0026gt;% head() ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. Here are some others using the filter and select commands with multiple steps:\n No Pipes gapusa_filter \u0026lt;- filter(gapminder, country == \u0026quot;United States\u0026quot;) gapusa_filter ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 United States Americas 1952 68.4 157553000 13990. ## 2 United States Americas 1957 69.5 171984000 14847. ## 3 United States Americas 1962 70.2 186538000 16173. ## 4 United States Americas 1967 70.8 198712000 19530. ## 5 United States Americas 1972 71.3 209896000 21806. ## 6 United States Americas 1977 73.4 220239000 24073. ## 7 United States Americas 1982 74.6 232187835 25010. ## 8 United States Americas 1987 75.0 242803533 29884. ## 9 United States Americas 1992 76.1 256894189 32004. ## 10 United States Americas 1997 76.8 272911760 35767. ## 11 United States Americas 2002 77.3 287675526 39097. ## 12 United States Americas 2007 78.2 301139947 42952. gapusa_select \u0026lt;- select(gapusa_filter, -continent, -lifeExp) gapusa_select ## # A tibble: 12 x 4 ## country year pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 United States 1952 157553000 13990. ## 2 United States 1957 171984000 14847. ## 3 United States 1962 186538000 16173. ## 4 United States 1967 198712000 19530. ## 5 United States 1972 209896000 21806. ## 6 United States 1977 220239000 24073. ## 7 United States 1982 232187835 25010. ## 8 United States 1987 242803533 29884. ## 9 United States 1992 256894189 32004. ## 10 United States 1997 272911760 35767. ## 11 United States 2002 287675526 39097. ## 12 United States 2007 301139947 42952.  With Pipes gapusa \u0026lt;- gapminder %\u0026gt;% filter(country == \u0026quot;United States\u0026quot;) %\u0026gt;% select(-continent, -lifeExp) gapusa ## # A tibble: 12 x 4 ## country year pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 United States 1952 157553000 13990. ## 2 United States 1957 171984000 14847. ## 3 United States 1962 186538000 16173. ## 4 United States 1967 198712000 19530. ## 5 United States 1972 209896000 21806. ## 6 United States 1977 220239000 24073. ## 7 United States 1982 232187835 25010. ## 8 United States 1987 242803533 29884. ## 9 United States 1992 256894189 32004. ## 10 United States 1997 272911760 35767. ## 11 United States 2002 287675526 39097. ## 12 United States 2007 301139947 42952. By using multiple lines you can actually read this like a story and there aren’t temporary variables that get confusing. This reads like:\n “start with the gapminder data, and then filter for the United States, and lastly drop the variables continent and lifeExp.”\n   mutate() adds new variables Let’s say we needed to add an index column so we know which order these data came in. Let’s not make a new variable, let’s add a column to our gapminder data frame. How do we do that? With the mutate() function.\nImagine we want to know each country’s annual GDP. We can multiply pop by gdpPercap to create a new column named gdp.\ngapminder %\u0026gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdp ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows  Try These Out On Your Own 1 Try to figure this out by yourself using pipes and only the filter, mutate, and select commands. Compare your syntax with the syntax below. Remember! There are multiple ways to go about finding the outcome.\n Calculate the population in thousands for all European countries in the year 2007 and add it as a new column.   Possible solution\ngapminder %\u0026gt;% filter(continent == \u0026quot;Europe\u0026quot;, year == 2007) %\u0026gt;% mutate(pop_thousands = pop/1000) %\u0026gt;% select(country, year, pop_thousands) #this cleans up the dataframe but isn\u0026#39;t necessary ## # A tibble: 30 x 3 ## country year pop_thousands ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania 2007 3601. ## 2 Austria 2007 8200. ## 3 Belgium 2007 10392. ## 4 Bosnia and Herzegovina 2007 4552. ## 5 Bulgaria 2007 7323. ## 6 Croatia 2007 4493. ## 7 Czech Republic 2007 10229. ## 8 Denmark 2007 5468. ## 9 Finland 2007 5238. ## 10 France 2007 61084. ## # … with 20 more rows  \nIf you got it, that’s great! However, it’s absolutely fine if you did not. The best way to learn is to practice (and possibly yell at your computer a few times if that helps).\n  group_by() operates on groups What if we wanted to know the total population on each continent in 2002? Answering this question requires a grouping variable. By using group_by() we can set our grouping variable to continent and create a new column called cont_pop that will add up all country populations by their associated continents.\ngapminder %\u0026gt;% filter(year == 2002) %\u0026gt;% group_by(continent) %\u0026gt;% mutate(cont_pop = sum(pop)) ## # A tibble: 142 x 7 ## # Groups: continent [5] ## country continent year lifeExp pop gdpPercap cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 2002 42.1 25268405 727. 3601802203 ## 2 Albania Europe 2002 75.7 3508512 4604. 578223869 ## 3 Algeria Africa 2002 71.0 31287142 5288. 833723916 ## 4 Angola Africa 2002 41.0 10866106 2773. 833723916 ## 5 Argentina Americas 2002 74.3 38331121 8798. 849772762 ## 6 Australia Oceania 2002 80.4 19546792 30688. 23454829 ## 7 Austria Europe 2002 79.0 8148312 32418. 578223869 ## 8 Bahrain Asia 2002 74.8 656397 23404. 3601802203 ## 9 Bangladesh Asia 2002 62.0 135656790 1136. 3601802203 ## 10 Belgium Europe 2002 78.3 10311970 30486. 578223869 ## # … with 132 more rows Sure this great but what if we don’t care about the other columns and only want each continent and their population in 2002? That leads us to the next function:\n summarize() with group_by() We want to operate on a group, but actually collapse or distill the output from that group. The summarize() function will do that for us.\ngapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarize(cont_pop = sum(pop)) %\u0026gt;% ungroup() ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 6187585961 ## 2 Americas 7351438499 ## 3 Asia 30507333901 ## 4 Europe 6181115304 ## 5 Oceania 212992136 summarize() will actually only keep the columns that are grouped_by or summarized. So if we wanted to keep other columns, we’d have to do have a few more steps. ungroup() removes the grouping and it’s good to get in the habit of using it after a group_by() because R remembers! We can use more than one grouping variable. Let’s get total populations by continent and year.\ngapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarize(cont_pop = sum(as.numeric(pop))) %\u0026gt;% ungroup() ## `summarise()` regrouping output by \u0026#39;continent\u0026#39; (override with `.groups` argument) ## # A tibble: 60 x 3 ## continent year cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 1952 237640501 ## 2 Africa 1957 264837738 ## 3 Africa 1962 296516865 ## 4 Africa 1967 335289489 ## 5 Africa 1972 379879541 ## 6 Africa 1977 433061021 ## 7 Africa 1982 499348587 ## 8 Africa 1987 574834110 ## 9 Africa 1992 659081517 ## 10 Africa 1997 743832984 ## # … with 50 more rows  arrange() orders columns This is ordered alphabetically, which is helpful in certain circumstances. But let’s say we wanted to order it in ascending order for year. The dplyr function to do that is arrange().\ngapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarize(cont_pop = sum(as.numeric(pop))) %\u0026gt;% arrange(year) %\u0026gt;% ungroup() ## `summarise()` regrouping output by \u0026#39;continent\u0026#39; (override with `.groups` argument) ## # A tibble: 60 x 3 ## continent year cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 1952 237640501 ## 2 Americas 1952 345152446 ## 3 Asia 1952 1395357351 ## 4 Europe 1952 418120846 ## 5 Oceania 1952 10686006 ## 6 Africa 1957 264837738 ## 7 Americas 1957 386953916 ## 8 Asia 1957 1562780599 ## 9 Europe 1957 437890351 ## 10 Oceania 1957 11941976 ## # … with 50 more rows On Your Own 2 Try to figure this out by yourself using pipes and some if not all of the commands introduced above. Compare your syntax with the script On Your Own (Pipes and the NFL) Syntax Set.R. Remember! There are multiple ways to go about finding the outcome.\n Now what is the maximum GDP per continent across all years?   Possible solution\ngapminder %\u0026gt;% mutate(gdp = pop * gdpPercap) %\u0026gt;% group_by(continent) %\u0026gt;% mutate(max_gdp = max(gdp)) %\u0026gt;% filter(gdp == max_gdp) %\u0026gt;% select(-country) ## # A tibble: 5 x 7 ## # Groups: continent [5] ## continent year lifeExp pop gdpPercap gdp max_gdp ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Oceania 2007 81.2 20434176 34435. 7.04e11 7.04e11 ## 2 Asia 2007 73.0 1318683096 4959. 6.54e12 6.54e12 ## 3 Africa 2007 71.3 80264543 5581. 4.48e11 4.48e11 ## 4 Europe 2007 79.4 82400996 32170. 2.65e12 2.65e12 ## 5 Americas 2007 78.2 301139947 42952. 1.29e13 1.29e13     NFL Data Set Grab the data set1: 2014-average-ticket-price\n Save it in the same place as your script.\n Load it up in R2:\n  nfl2014 \u0026lt;- read_csv(\u0026quot;2014-average-ticket-price.csv\u0026quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Event = col_character(), ## Division = col_character(), ## `Avg TP, $` = col_double() ## ) To make a good habit, let’s check the file’s characteristics. This is a good habit and can alleviate some pains down the line. It is recommended you run these simple checks on any data set.\nCheck that your set is in a data frame format and get information about your columns and their types (character, numeric, or factor) by running  str(nfl2014) ## tibble [108 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Event : chr [1:108] \u0026quot;Baltimore Ravens at Pittsburgh Steelers Tickets on 02-Nov-2014 (9037819)\u0026quot; \u0026quot;Pittsburgh Steelers at Baltimore Ravens Tickets on 11-Sep-2014 (9037835)\u0026quot; \u0026quot;Cleveland Browns at Pittsburgh Steelers Tickets on 07-Sep-2014 (9037806)\u0026quot; \u0026quot;Cincinnati Bengals at Pittsburgh Steelers Tickets on 28-Dec-2014 (9037828)\u0026quot; ... ## $ Division : chr [1:108] \u0026quot;AFC North\u0026quot; \u0026quot;AFC North\u0026quot; \u0026quot;AFC North\u0026quot; \u0026quot;AFC North\u0026quot; ... ## $ Avg TP, $: num [1:108] 202 199 196 164 148 137 135 102 89 83 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. Event = col_character(), ## .. Division = col_character(), ## .. `Avg TP, $` = col_double() ## .. ) or you can do\nglimpse(nfl2014) ## Rows: 108 ## Columns: 3 ## $ Event \u0026lt;chr\u0026gt; \u0026quot;Baltimore Ravens at Pittsburgh Steelers Tickets on 02-No… ## $ Division \u0026lt;chr\u0026gt; \u0026quot;AFC North\u0026quot;, \u0026quot;AFC North\u0026quot;, \u0026quot;AFC North\u0026quot;, \u0026quot;AFC North\u0026quot;, \u0026quot;AFC … ## $ `Avg TP, $` \u0026lt;dbl\u0026gt; 202, 199, 196, 164, 148, 137, 135, 102, 89, 83, 83, 81, 2… So we do have a data frame with 108 rows and three columns as well as two columns that are made up of characters (i.e. letters) and one that is numeric, or made of numbers. These columns are actually called vectors. Please keep this mind as we’ll be referring to the columns as vectors from now on.\nCheck that its a tibble, which is essentially a data frame but tweaked to make it easier to wrangle.  is_tibble(nfl2014) The differences between the two are worth a brief discussion in a data science course, but not here. Some older functions don’t work with tibbles. If you encounter one of these functions, use the command as.data.frame() to turn a tibble back to a data.frame like so:\nnfl2014_dfonly \u0026lt;- as.data.frame(nfl2014) Anyway, if you test for both of those and they pass, you will be able to use all of the commands listed above.\nWe’ll tweak the second question on the task list a bit to say: “What was the highest and lowest average ticket price for all 2014-2015 NFL games by division?” For this session, we’ll disregard the histogram and answer the question that was asked for on the original.\nTo tackle this, let’s think about it logically. Originally you had to\ninspect the data frame which I called nfl2014,\n remove the columns with NA,\n create eight different data frames, one for each division, and\n derive the mean for each.\n put the values in order.\n  That is a lot of steps! Let’s try using pipes to do this instead:\nnfl2014 %\u0026gt;% select(Division, `Avg TP, $`) %\u0026gt;% group_by(Division) %\u0026gt;% na.omit() %\u0026gt;% summarize(Mean_by_Division = mean(`Avg TP, $`, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(Mean_by_Division)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 2 ## Division Mean_by_Division ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 NFC North 179. ## 2 NFC East 173. ## 3 NFC West 165. ## 4 AFC North 135. ## 5 AFC East 127. ## 6 AFC West 125. ## 7 NFC South 95.4 ## 8 AFC South 83.3 Well that was MUCH EASIER! Now what happened? Let’s go through it line by line:\nI called the data frame nfl2014. This was the name given to the csv file when it was brought in.\n I selected the two columns I want to know more about. In this case, I can’t compute anything by Division if I don’t have the numerical data that explains ticket prices Avg TP, $.\n Since I need the mean by each Division, grouping by that vector will allow me to find perform any and all operations by each unique value. In this case we have eight divisions, so well have outcomes for each.\n NAs are a special type of value in R. New users tend to disregard them as blanks or sometimes zeros. However, the name itself NA means not available aka a missing value. When you see this, it means that R has recognized that a value should be there but its not and depending on the command, it may bring up an error or worst case scenario, an analysis runs and your output is skewed. The command na.omit() will remove any row where an NA is found. Please note this differs from removing only rows with NAs. The distinction, while minute, can have lasting ramifications.\n summarize tells R that I’m going to reduce the data set using some summary technique. In this case the mean is needed so I constructed a new column named Mean_by_Division which took the mean of `Avg TP, $. Note that since we are using pipes, we do not have to tell R about the name of the data frame. It has been understood since step 1.\n As a good rule of thumb, it is always good to ungroup() your data because the grouping is information that R retains. R remembers!\n Arrange the Mean_by_Division vector in decreasing order. The default is increasing.\n  On Your Own 3  Now try doing the same as the above except only for AFC teams.  \nThere are a lot of ways to accomplish this, but I would suggest filtering with a string. Take a look at the link and see if you can figure out the command to use.\nFilter with Text data\nOf course, a solution is below. Again you are learning how to do this so take some time and for many of you, it may be oddly satisfying to find the output you want after being frustrated.  Possible solution\nnfl2014 %\u0026gt;% select(Division, `Avg TP, $`) %\u0026gt;% filter(str_detect(Division, \u0026quot;AFC\u0026quot;)) %\u0026gt;% na.omit() %\u0026gt;% summarize(AFC_mean = mean(`Avg TP, $`, na.rm = TRUE)) ## # A tibble: 1 x 1 ## AFC_mean ## \u0026lt;dbl\u0026gt; ## 1 117.    Task For this task first download the drinks2010andBeyond data set3. This is an amended version of the average serving sizes per person by country as reported by the World Health Organization (WHO), Global Information System on Alcohol and Health (GISAH) since 2010. If you are interested, please visit the WHO GISAH site.\nIn a typical submission4, you must include\nYour name and task in the top of the script.\n Separated solutions in proper numerical order.\n Your code.\n An answer to the question.\n  Remember you can leave text that R will ignore by putting a hashtag # in front of it.\nAs an example, if I were to submit the altered item 2 from above, my script would read:\n# Abhik Roy # EDP 613 # Drinks Task # 1 blah blah blah # 2 nfl2014 %\u0026gt;% select(Division, `Avg TP, $`) %\u0026gt;% group_by(Division) %\u0026gt;% na.omit() %\u0026gt;% summarize(Mean_by_Division = mean(`Avg TP, $`, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(Mean_by_Division)) # According to the output, the highest average price paid was $170.00 per ticket paid by # fans the NFC North whereas the lowest average price ticket was paid by attendees who # went to games in the AFC South at $83.30. # 3 blah blah blah Now please answer the following questions. Please note that all consumption measurements are in liters:\n First step  We have to bring in the data set:\ndrinks \u0026lt;- read_csv(\u0026quot;drinks2010andBeyond.csv\u0026quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## `Beverage Types` = col_character(), ## `2016` = col_double(), ## `2015` = col_double(), ## `2014` = col_double(), ## `2013` = col_double(), ## `2012` = col_double(), ## `2011` = col_double(), ## `2010` = col_double() ## )  \nOK now we can move on with the questions! Which country or countries had the highest average consumption of wine and beer in each year?   Possible solution  drinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Beer\u0026quot; | `Beverage Types` == \u0026quot;Wine\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% group_by(year, Country) %\u0026gt;% summarize(avg = mean(measure)) %\u0026gt;% top_n(n=1, avg) ## `summarise()` regrouping output by \u0026#39;year\u0026#39; (override with `.groups` argument) ## # A tibble: 7 x 3 ## # Groups: year [7] ## year Country avg ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Portugal 5.42 ## 2 2011 Croatia 5.40 ## 3 2012 Equatorial Guinea 5.82 ## 4 2013 Equatorial Guinea 5.08 ## 5 2014 Austria 5.25 ## 6 2015 Slovenia 5.32 ## 7 2016 Czechia 4.84 Breakdown line-by-line:\nWe call the data frame drinks.\n We filter the column Beverage Types by “Beer” OR “Wine”.5\n The gather() command is used to go from a wide data set to a long one while ignoring the application of the command on the columns Country, and Beverage Types. To see what occurs, first run the first two lines by highlighting them and ending before the %\u0026gt;% (pipe) on the second line:\n  That simply runs through steps 1 and 2 above. Now run the first three lines in the same way ending prior to the %\u0026gt;% (pipe) in the third line\nYou can find a multitude of resources for gather() and its complement spread() but my advice is simply to play around with the key, value, and columns you don’t want to make long. However if you like some reference material while you explore these commands, try Data Science for R. If the link is not work, the address is https://garrettgman.github.io/tidying/.\nIn our case all of the data is now in relation to the columns Country and Beverage Types which gives us the remaining columns (the key) as data points that have values (the value) which I have named year (via key = \"year\") and measure (via value = \"measure\"). Those that do not have associated values become NAs (for example the country of Afghanistan did not have any values for Beer or Wine in 2016). Please take some time to investigate the gather() command as R works will with long data sets and not so well with wide ones!\nna.omit() is a command you have seen before and it simply sees if there is an NA in a row and then deleted said row. It performs this recursively throughout your entire data frame.\n The group_by() command notifies R that all operations must be performed with respect to the columns indicated which in this case implies that  the summarize() command used here to find the mean of the column measure is performed by year and Country. We assign the results to a new column avg.\n Finally the top_n() which gives us the top value of the average avg column.\n  Again to see how this layering works, I suggest that you run this line by line to observe the results. This is the benefit of the tidyverse approach.\n \nWhich country or countries had the highest average consumption of spirits in each year?   Possible solution  The process here is exactly the same as the one described in item 1 except the Beverage Types are \"Spirits\" rather than \"Beer\" or \"Wine\".\ndrinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Spirits\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% group_by(year, Country) %\u0026gt;% summarize(avg = mean(measure)) %\u0026gt;% top_n(n=1, avg) ## `summarise()` regrouping output by \u0026#39;year\u0026#39; (override with `.groups` argument) ## # A tibble: 7 x 3 ## # Groups: year [7] ## year Country avg ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Estonia 7.53 ## 2 2011 Estonia 8.18 ## 3 2012 Estonia 8.53 ## 4 2013 Estonia 8.93 ## 5 2014 Estonia 8.7 ## 6 2015 Estonia 8.37 ## 7 2016 Estonia 7.72  \nWhich country had the highest average consumption of all alcohol across time?   Possible solution  Again, the process here is exactly the same as the one described in items 1 and 2 except the Beverage Types are \"All types\" rather than any individual type.\ndrinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;All types\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% group_by(year, Country) %\u0026gt;% summarize(avg = mean(measure)) %\u0026gt;% top_n(n=1, avg) ## `summarise()` regrouping output by \u0026#39;year\u0026#39; (override with `.groups` argument) ## # A tibble: 7 x 3 ## # Groups: year [7] ## year Country avg ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Estonia 15.0 ## 2 2011 Estonia 16.3 ## 3 2012 Estonia 17.0 ## 4 2013 Estonia 17.8 ## 5 2014 Estonia 17.3 ## 6 2015 Estonia 16.6 ## 7 2016 Estonia 15.4  \nBased on each years average consumption of beer across all counties, how far does the United States deviate from the mean, if at all? Is the mean an appropriate measure of the average here? Why or why not?\nThis one was fairly difficult. A deviation from the mean essentially indicates how a single measure differs from an established value of a mean. In our case, how the mean of the United States differs from that of the mean derived from the rest of the countries. Your first thought may be to find the difference between both the means by subtracting them. This would be fine if both had the same standard deviation, but they do not.   Possible solution  # USA only drinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Beer\u0026quot; \u0026amp; Country == \u0026quot;United States of America\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% summarize(avg = mean(measure), stanDev = sd(measure))  ## # A tibble: 1 x 2 ## avg stanDev ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4.22 0.0699 # Remaining countries drinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Beer\u0026quot; \u0026amp; Country != \u0026quot;United States of America\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% summarize(avg = mean(measure), stanDev = sd(measure))  ## # A tibble: 1 x 2 ## avg stanDev ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2.06 1.80 Note that our filter() command again uses an AND command rather than an OR and rather than filtering using one column, we use two (Beverage Types AND Country) but in the latter we we want to exclude the United States of America by using Country != \"United States of America\". So interpreting this, we can say that we’re looking for figures for all Beer consumption around the world except for the United States of America.\nSo what can we do? At this point not much! In the future we should be able to address this issue by using a statistical test known as a t-test but we need to satisfy some initial prerequisites first. We will explore this in R5 but for now, we’ll leave it here.\nOn a side note, it is absolutely fine if you subtracted the means! That would be the most logical choice and you also did not have much of a choice at the current time. Consider that the issue of subtracting means with differing standard deviations is that the spread differs between each and while a mean is a mean, they’re not measures of the mean on the same footing - ergo all things are not equal.  \nSince we measure beer in the United States in ounces (for some reason) rather than in liters, convert the 2016 measures into ounces using the conversion 1 Liter = 33.814022 oz.   Possible solution  One of the nicer aspects of using %\u0026gt;% is in the fact you can selectively perform operations and general augmentations on the columns of your choice without ever having to worry about how any other columns are affected. This is actually accomplished using the command mutate() which in a nutshell allows for column wise operations.\nIn this task, we are informed that the column 2016 is given in liters. However since we and a handful of other countries have chosen to use a measurement that are nowadays based off the metric system (yes that’s true..for example an inch is officially defined as 2.54 centimeters) and not the metric system itself (base 10 seems pretty easy but what do I know), we have to convert on order to report. In this case we are given the easy to remember 1 Liter = 33.814022 (fluid) ounces. To convert this, we use the mutate() command by drinks %\u0026gt;% mutate(`2016` = 33.814022 * `2016`) %\u0026gt;% rename(`2016 (using garbage measurements)` = `2016`) ## # A tibble: 954 x 9 ## Country `Beverage Types` `2016 (using ga… `2015` `2014` `2013` `2012` `2011` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghan… All types NA 0.02 0.03 0.03 0.04 0.04 ## 2 Afghan… Beer NA 0.01 0.01 0.01 0.01 0.01 ## 3 Afghan… Wine NA 0 0 0 0 0 ## 4 Afghan… Spirits NA 0.02 0.02 0.02 0.03 0.03 ## 5 Afghan… Other alcoholic… NA 0 0 0 0 0 ## 6 Albania All types 171. 4.77 4.81 5.06 5.43 5.65 ## 7 Albania Beer 58.8 1.57 1.58 1.82 1.81 1.88 ## 8 Albania Wine 45.3 1.17 1.12 1.06 1.3 1.27 ## 9 Albania Spirits 64.2 1.94 2.02 2.09 2.22 2.4 ## 10 Albania Other alcoholic… 3.04 0.08 0.09 0.09 0.1 0.1 ## # … with 944 more rows, and 1 more variable: `2010` \u0026lt;dbl\u0026gt; with the last line being dedicated to renaming the column that was just converted from 2016 to 2016 (using garbage measurements). Notice that the rename() command requires that the new name of a column precede the current name separated by and =. I realize this appears to be counterintuative and there is a reason for it, but unless you love discussing how R compiles code, I’ll leave it be.\nOn a side note, we only converted the column 2016. What if we wanted to convert every column? Well we’d be using a lot of %\u0026gt;% and wasted energy. This is one of many instances where gather() is a great command to know. Switching from a wide data frame to a long one allows you to convert all years at once. We can do this by\ndrinks %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% mutate(`measure` = 33.814022 * `measure`) %\u0026gt;% rename(`Measures (in ounces because that\u0026#39;s apparently easy)` = `measure`) ## # A tibble: 6,016 x 4 ## Country `Beverage Types` year `Measures (in ounces because that\u0026#39;s app… ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania All types 2016 171. ## 2 Albania Beer 2016 58.8 ## 3 Albania Wine 2016 45.3 ## 4 Albania Spirits 2016 64.2 ## 5 Albania Other alcoholic bever… 2016 3.04 ## 6 Algeria All types 2016 18.9 ## 7 Algeria Beer 2016 10.8 ## 8 Algeria Wine 2016 4.73 ## 9 Algeria Spirits 2016 3.38 ## 10 Algeria Other alcoholic bever… 2016 0 ## # … with 6,006 more rows Unlike the piping we used earlier, we aren’t filtering for anything (though we could if we wanted by putting the filters back in). The rest is pretty much a combination of previous column-wise wrangling. Try running the first three lines. You will once again notice that the measure column is the associated liter count by year. With that, we mainly need to use mutate() on the measure column to convert it.\nLearning both gather() and to some degree spread() which is not covered in this walk-through will save you a great deal of time. It is worth taking the time now to explore what you can do with it.\n Explore the data set a bit and play around with pipes. Provide one outcome you derived not already asked about that you (hopefully) found interesting.   Possible solution  Of course answers will vary here but we can see the values for each country by year\ncoalesce_by_column \u0026lt;- function(df) { return(dplyr::coalesce(!!! as.list(df))) } drinks %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% rowid_to_column(\u0026quot;Country ID\u0026quot;) %\u0026gt;% spread(`Beverage Types`, value = `measure`) %\u0026gt;% select(-`Country ID`) %\u0026gt;% group_by(Country, year) %\u0026gt;% summarise_all(coalesce_by_column) %\u0026gt;% na_if(0) ## # A tibble: 1,202 x 7 ## # Groups: Country [190] ## Country year `All types` Beer `Other alcoholic beverages` Spirits Wine ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan 2010 0.03 0.01 NA 0.02 NA ## 2 Afghanistan 2011 0.04 0.01 NA 0.03 NA ## 3 Afghanistan 2012 0.04 0.01 NA 0.03 NA ## 4 Afghanistan 2013 0.03 0.01 NA 0.02 NA ## 5 Afghanistan 2014 0.03 0.01 NA 0.02 NA ## 6 Afghanistan 2015 0.02 0.01 NA 0.02 NA ## 7 Albania 2010 5.53 1.72 0.12 2.39 1.3 ## 8 Albania 2011 5.65 1.88 0.1 2.4 1.27 ## 9 Albania 2012 5.43 1.81 0.1 2.22 1.3 ## 10 Albania 2013 5.06 1.82 0.09 2.09 1.06 ## # … with 1,192 more rows where NA represents values not reported.\nOn a side note, there is an easier way to run spread() and it is called pivot_wider().\n  Logical Operators    R syntax  Idea  Logic      !  NOT  !a    \u0026amp;  vector-based AND (value-wise)  a \u0026amp; b    \u0026amp;\u0026amp;  value-based AND (single value)  a \u0026amp;\u0026amp; b    |  vector-based inclusive OR (value-wise)  a | b    ||  value-based inclusive OR (single value)  a || b    xor  vector-based exclusive OR (value-wise)  xor(a,b)    \u0026lt;  LESS than  a \u0026lt; b    \u0026gt;  GREATER than  a \u0026gt; b    ==  exactly EQUALS  a == b    \u0026lt;=  LESS than or EQUAL to  a \u0026lt;= b    \u0026gt;=  GREATER than or EQUAL to  a \u0026gt;= b    !=  NOT EQUAL to  a != b    %in%  included in?  a %in% b    isTRUE  test if something is TRUE  isTRUE(a)      The use of %\u0026gt;% will get easier. Some people think of what steps they use when using a spreadsheet software (e.g. Microsoft Excel ©) when learning the logic of R. In fact, many of the formulas and syntax used in spreadsheet softwares parallel those in R. If you are used to Excel for example, try checking out R for Excel Users. If the link is broken for some reason, head over to https://www.rforexcelusers.com/. Please note that I do not receive any compensation in any form for recommending this site nor do I take responsibility for its content. For assistance regarding the site, please contact the authors\n Data Science Data science uses the statistics you will learn in this course and to some degree, in the follow-up course to tackle real life data. According to Glassdoor, the average salary for a data scientist is 113,436 USD (range: 95,000 USD - 250,000 USD; Burtch-Works, 2018). If you are interested in learning more about Data Science, a course is tentatively scheduled for the Fall 2021 term in addition to Data Visualization in Spring 2021. Both of these courses run very differently from the statistics courses and are focused on exploring data sets of interest to students. If interested, please let me know or stay tuned.\n Acknowledgements A great deal of material presented here was adapted from two online texts: Introduction to Open Data Science and R for Data Science. If interested in how R is used in data science, try taking a look.\n Don’t Get Bogged Down! Just get a feel for the items above. There is a whole bunch of commands and ridiculous syntax above but that’s not the point. Concentrate on knowing what to do (e.g. “Is this best represented by a bar plot?”) rather than how to do it (“The ggplot command is…”). You can ALWAYS search for the syntax associated with whatever in R but if you don’t how to conceptually solve a problem, software won’t help. Remember computers are stupid!\n  From the FiveThirtyEight NFL Ticket Prices Github Page↩︎\n If you are getting an error, remember to point R to the right location by Session \u0026gt; Working Directory \u0026gt; To Source File Location in the menu.↩︎\n From the FiveThirtyEight Alcohol Consumption Github Page↩︎\n This is just for practice so you do not have to turn anything in!↩︎\n Note that OR here means what is known as an inclusive or…aka it this or that or both. Using this idea, the second line search reads like Beverage Types for “Beer” OR “Wine” OR both. The other type of OR is called an exclusive or ..aka it this or that but NOT both. If you ever have to use it, it is given by the command xor(). Included on the last page of this document are some standard logical operators for your reference.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"966b0ddce3ea132adea1ea1f15ef073e","permalink":"/lesson/nflpipes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/nflpipes/","section":"lesson","summary":"Preparation Purpose Objectives Packages  The Tidyverse Package Tidy Data Basics The Pipe %\u0026gt;% Operator Gapminder mutate() adds new variables Try These Out group_by() operates on groups summarize() with group_by() arrange() orders columns  NFL Data Set Task Logical Operators Data Science Acknowledgements Don’t Get Bogged Down!","tags":null,"title":"Dplyr, Pipes, and the NFL","type":"docs"},{"authors":null,"categories":null,"content":"   RStudio.cloud RStudio on your computer Install R Install RStudio Install tidyverse    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (as of this writing, it’s R-4.0.0.pkg; as of right now but download the most current version.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n   It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"/resource/install/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"RStudio.cloud RStudio on your computer Install R Install RStudio Install tidyverse    You will do all of your work in this class with the open source (and free!","tags":null,"title":"Installing R, RStudio, and tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   Quiz    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Quiz Assignment Quiz covering Chapters 1-3. Please turn it in to the Submission Portal on ecampus by 11:59 PM next this Friday\n Solutions Posted on ecampus after the submission deadline.\n:::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"969c1469241b653a68a7c0c41e4bfc3b","permalink":"/assignment/quiz1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/quiz1/","section":"assignment","summary":"Quiz    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Quiz Assignment Quiz covering Chapters 1-3.","tags":null,"title":"Quiz","type":"docs"},{"authors":null,"categories":null,"content":"   Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses a language called \\(\\LaTeX\\) to create fancy mathematical equations. Be forewarned, this is syntax heavy so please do not get bogged down in it. That’s not to say its not extremely useful - \\(\\LaTeX\\) takes the need to format anything mathematics out of the equation1 which makes life much simpler but the language hasn’t been updated since 1994 which makes it older than some or many of you. The current version of the language \\(\\LaTeX2e\\) is both archaic and extremely dependent on external packages which can make both a complex and messy script. While the next generation of \\(\\LaTeX\\) is in the works2, there is a long way to go. With that said, this is worth reviewing if for no other reason than general curiosity and possibly to see how some of the course material is created.\nThere are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago Manual of Style 27th edition (author-date) APA 7th edition MLA 8th edition   Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, J.D. \u0026amp; Pischke, J (2015). Mastering ’Metrics: The Path from Cause to Effect. Princeton University Press.\nRohrer, J.M. (2018). “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.    No pun intended but if you’ve ever tried to use the Equation Editor in Microsoft Word, the formatting is a nightmare.↩︎\n Codenamed \\(\\LaTeX3\\). Mathematicians are generally not very good at naming things.↩︎\n   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"/resource/markdown/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Exam 2    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Exam 2 Assignment Exam 2 - data portion covering inferential statistics. - paper portion covering Chapters 7-10.\nPlease turn both to the Submission Portal as separate tasks on ecampus by 11:59 PM next Wednesday.\n Solutions Posted on ecampus after the submission deadline.\n:::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"178b899765364e9ed356a8601036609f","permalink":"/assignment/exams2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/exams2/","section":"assignment","summary":"Exam 2    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Exam 2 Assignment Exam 2 - data portion covering inferential statistics.","tags":null,"title":"Exam II","type":"docs"},{"authors":null,"categories":null,"content":"   Exam 1    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Exam 1 Assignment Exam 1 - data portion covering descriptive statistics. - paper portion covering Chapters 1-6.\nPlease turn both to the Submission Portal as separate tasks on ecampus by 11:59 PM next Wednesday.\n Solutions Posted on ecampus after the submission deadline.\n:::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d62bcbfc820e72f07efbd120db46ad2c","permalink":"/assignment/exams1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/exams1/","section":"assignment","summary":"Exam 1    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Exam 1 Assignment Exam 1 - data portion covering descriptive statistics.","tags":null,"title":"Exam 1","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R R in the Wild Sample of R Visualizations   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with the tidyverse family of packages, it’s often easier to just search for that instead of the letter “R” (e.g. “tidy pivoting”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio. LHS 610: Exploratory Data Analysis for Health: A comprehensive way to learn R using health based data sets by Dr. Karandeep Singh out of the University of Michigan. R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. RStudio Conference cheat sheets: A comprehensive collection of all recognized RStudio cheat sheets from rstudio::conf 2019. In fact if you are interested in R, check out the conference workshop materials. Rstudio Education: A fantastic beginner’s site for those of you who want to learn, teach, or wish to become a certified trainer. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. Stats Illustrations: Dr. Allison Horst created and continues to build beautiful illustrations of many R commands you will probably use. Teacup Giraffes: A fantastic and visually stunning way to learn R using animations accessible to children but for adults by Dr. Desirée De Leon at RStudio.   R in the Wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\n Sample of R Visualizations  Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test A decade(ish) of listening to Sigur Rós Disproving Approval R color Palettes by Emil Hvitfeldt is a very comprehensive list of pre packaged color palettes you can get. He has even compiled all of them into an R package called paletteer General (Attys) Distributions Health care indicators in Utah counties Mapping Fall Foliage Personal Blog from Isabella Benabaye with some fun and colorful R and Python data visualizations Sexism on the Silver Screen: Exploring film’s gender divide Song lyrics across the United States Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Who came to vote in Utah’s caucuses?    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the original source code @ andrewheiss). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R R in the Wild Sample of R Visualizations   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   Background Exploratory Steps Answer the Question What to Submit Final check    My cats did not care about this data set but they did yawn!  Background The data set included with this task is derived from the Mythbusters Episode Is Yawning Contagious?. The file yawning_data.csv contains all of the actual data they collected.\n Exploratory Steps Before getting to the question at hand, please do the following:\n As usual, please open the data set and code book to familiarize yourself with the (1) information and (2) layout, both of which are important.\n Go watch the Mythbusters Episode Is Yawning Contagious?.\n   Answer the Question Is yawning really contagious? Remember that you are basing your decision on the Mythbusters data set.\n What to Submit R file. You may choose to upload a script or Rmarkdown1 document. For a curated and biased list of resources that you can use to learn R Markdown, head over to the R Markdown section on the course Resources page.   While the layout somewhat differs between a script and Rmarkdown document, you must supply a basic explanation of each line of code. If you get stuck, stay calm and think about what is happening to the data. Remember that you have the resources and a week. Worst case scenario, you have this tool called the Internet so please utilize it! Please make sure you follow these general guidelines\n Write out your name in the upper left hand corner. Please put a # --- between each item.2 Number each problem. As noted earlier, explain each line of code. Indicate your answer by # Solution: Always answer the question in context.  Findings. Answer the question in a Word document. If you are unfamiliar with the format, USCLibraries has a fantastic site with additional resources3 that can help you now and in the future.   In general, please submit the following document: A formally written APA 7th edition formatted paper up to five (5) pages in length with the following sections: - Introduction - Literature Review4 - Methods - Analysis - Findings\n  Why is there no lower limit on the page length? Good question! In general, page length is no way a determining factor for quality. You should concentrate on trying to convince a reader that you have enough evidence to have answered the question. This will come primarily from statistical tests but to a lesser degree, prior literature as well.\n  Some advice:\n  Stick to the facts and evidence. You may have thoughts and views but that should not be included in your report. Avoid first person perspective. In general, avoid using I, me, and any other term that implies the narrative is coming from your point-of-view.   Final check R file. Once you are satisfied with the work but prior to submission, save your script or Rmarkdown document and restart R. Run your script from start to bottom by highlighting all of the contents within the entire file. If it runs smoothly without errors, then you should be fine to submit.\n Findings. Please read over the report prior to submission. Make sure your language is appropriate and that at minimum, it is devoid of grammatical and spelling errors.\n    Remember that submitting the latter may earn you up to an extra 5% on this submission, though this policy applies to the paper portion as well.↩︎\n Does not apply to Rmarkdown documents.↩︎\n In fact, the USCLibraries site is a great source of information with guides for nearly everything you may need in the social sciences.↩︎\n Use only relevant studies which should keep this section relatively short.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f92fe7e99503d91cde7127601a1e3dbe","permalink":"/assignment/de1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/de1/","section":"assignment","summary":"Background Exploratory Steps Answer the Question What to Submit Final check    My cats did not care about this data set but they did yawn!  Background The data set included with this task is derived from the Mythbusters Episode Is Yawning Contagious?","tags":null,"title":"Yawn","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R Markdown Source Materials Overview of Sorts Key Terms Adding Chunks Naming Chunks Chunk options Tackling Inline Chunks  Output formats   You may have guessed that R Markdown is regular Markdown with R code and output sprinkled in. Simply put, its\n R + Markdown = R Markdown  \nYou can do everything you can with regular Markdown, but you can also incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. The website for this course was created with R Markdown (and a package named blogdown) and is updated using the same platform.\nLearning R Markdown  Data Camp has a good module on learning how to create R Markdown documents. Getting started in R and RStudio is created by Andy Fields and is a great step-by-step introduction to both R and R Markdown and you can skip around using a Table of Contents. R Markdown Basics is a very comprehensive primer solely focused on R Markdown. Andy Lin, the author of the walkthrough assumes that you have some working knowledge of R, namely RStudio.   Source Materials The documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent. Use them as resources.\nHere are the most important things about R markdown that will likely be beneficial:\n Overview of Sorts Key Terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Adding Chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Naming Chunks You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Tackling Inline Chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n   Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;August 19, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"/resource/rmarkdown/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Learning R Markdown Source Materials Overview of Sorts Key Terms Adding Chunks Naming Chunks Chunk options Tackling Inline Chunks  Output formats   You may have guessed that R Markdown is regular Markdown with R code and output sprinkled in.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"  R Templates Here are a few R templates you can use for submissions1. If you do not know how to use Rmarkdown, please use the script for submissions where R may be utilized or is required.\n Example Script Example RMarkdown    You may have to righ click and download each depending on your browser settings.↩︎\n   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"114d788d2e764035929d62ae9207e751","permalink":"/resource/templates/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/templates/","section":"resource","summary":"R Templates Here are a few R templates you can use for submissions1. If you do not know how to use Rmarkdown, please use the script for submissions where R may be utilized or is required.","tags":null,"title":"R Templates","type":"docs"},{"authors":null,"categories":null,"content":"  Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"   Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer The Stories Behind a Line   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n The Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R. Emery’s Essentials: Descriptions and examples of 26 different chart types. From Data to Viz: A decision tree for dozens of chart types with links to R and Python code. The Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations. The Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.). R Graph Catalog:{target=\"_blank\"} R code for 124 ggplot graphs.   General resources  Ann K. Emery’s blog: Blog and tutorials by Ann Emery. Data Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway. The Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic. Evergreen Data: Helpful resources by Stephanie Evergreen. FlowingData: Blog by Nathan Yau. Info We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field. Information is Beautiful: Blog by David McCandless. PolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch. Visualising Data:{target=\"_blank\"} Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk. Storytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic. Junk Charts: Blog by Kaiser Fung. WTF Visualizations: Visualizations that make you ask “wtf?” Seeing Data: A series of research projects about perceptions and visualizations.   Visualization in Excel  How to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel. Ann Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.   Visualization in Tableau Because it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  Australia as 100 people: You can make something like this with d3 and the potato project.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"   This is me This is what I am This is where I come from This is what I research This is how you can contact me This is Sparta   This is me     This is what I am Professor of Educational Psychology and head of the Program Evaluation Certificate at WVU.\n This is where I come from  PhD in Program Evaluation from Western Michigan University. MS in Mathematical Sciences from Michigan Technological University. BS in Mathematics from West Virginia Wesleyan College.   This is what I research  Transference of Foundational Evaluation Concepts Modeling using Machine Learning Applications of Predictive Hierarchical Social Networks  which are just fancy ways of saying\n How people learn basic concepts in evaluation Programming a type of artificial intelligence and then training it to do some of my work for me1 Figuring out how things are related to each other and then making judgments2  You can check out more about what I do here:    ORCID   GitHub   ResearchGate  This is how you can contact me    Abhik.Roy   @DrTheoreticalPE  This is Sparta     No its not Skynet. I said it in the syllabus and I’ll say it here: computers are stupid.↩︎\n Which is a very evaluation like thing to do!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"779259905d42fe327a85b980891c96bc","permalink":"/professor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/professor/","section":"","summary":"This is me This is what I am This is where I come from This is what I research This is how you can contact me This is Sparta   This is me     This is what I am Professor of Educational Psychology and head of the Program Evaluation Certificate at WVU.","tags":null,"title":"Professor","type":"page"},{"authors":null,"categories":null,"content":"  Every class session may be comprised of four important sections with tasks denoted in a fifth area. You should read about the details for each using the main menu at the top of this webpage.\n Readings (): This page contains the readings for the topic. Read these first each time you visit. Lesson (): This page contains interactive lessons, slides, and/or recorded walkthroughs that teaches you the principles and code you need to know. Go through these after doing the content. Example (): This page contains extra items that are picked or created to help you understand the current material. You do not have anything to complete in this section. From time to time I may post videos of performing something in R that may be beneficial. Assignment (): This page contains the instructions for all student related tasks including information about the tasks and the final. Due (): This page contains the instructions what to submit by week. Assignments are due by Wednesday at 11:59 PM or as noted.  ::: note  tl;dr: I recommend that you follow these lessons in the following order:\n        \n or in terms that aren’t as ambiguous as emojis, follow this process: read the material, complete data camp with other possible items, and then use the example (if given) along with the assignment to help you with what is due. :::\n   Concentration Readings Lesson Example Assignment Due   September 2, 2020 The What and Why of Statistics        September 9, 2020 The Organization and Graphic Presentation of Data        September 16, 2020 Measures of Central Tendency        September 23, 2020 Measures of Variability        September 30, 2020 The Normal Distribution        October 7, 2020 Sampling and Sampling Distributions Part I         October 14, 2020 Sampling and Sampling Distributions Part II          October 21, 2020 Exam I           October 28, 2020 Estimations         November 4, 2020 Testing Hypotheses        November 11, 2020 Bivariate Tables         November 18, 2020 The Chi-Square Test and Measures of Association         November 25, 2020 Chi-Square and ANOVAs        December 2, 2020 Exam II           December 9, 2020 Sentiment Analysis            ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Every class session may be comprised of four important sections with tasks denoted in a fifth area. You should read about the details for each using the main menu at the top of this webpage.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"    Course objectives FAQ Is the course content difficult? Is R difficult? What if I find a mistake?  Course materials Texts Software Note: Online help  Assignments and Grades R, Rmarkdown, Extra Credit and Showing Work Your hours Class conduct and expectations Learning and knowledge during a pandemic  Course policies COVID-19 Statement Psychological and Psychiatric Services CARE Team Lauren’s Promise Academic Integrity Inclusivity Statement Incomplete Grades Sale of Course Materials Student Evaluation of Instruction (SEI) University Attendance Policy Course Netiquette Response Time Technical Requirements Technical Support    Instructor  Dr. Abhik Roy  Allen Hall 504O  Abhik.Roy@mail.wvu.edu  @DrTheoreticalPE  Schedule an appointment   Course details  Forever (for the next 13 weeks)  August 26 - December 11  Thursdays from 4:00-6:50 PM  Agricultural Sciences building Rm. 2004  Slack   Contacting me Slack is the best way to get in contact with me. Response times are typically much quicker than the standard 24-48 hours given to emails. Please remember that life is somewhat chaotic at this time!\n  Course objectives Students will become proficient and be able to describe in detail the following aspects:\nAssessing and describing counterfactuals and scientific argumentation. Defining the underlying constructs for appropriate statistical techniques. Describing and graphing data. Explaining and drawing inferences from data. Gaining real world experience using R to analyze data sets. Obtaining an understanding of ethical, social, political, and cultural issues confronted by those who perform statistical analyses. Understanding and critically assessing differing analytical methods.   FAQ Is the course content difficult? You’ve probably heard an answer like this before: At times some of the material can be dense. Well that isn’t much of a response. Difficulty is not the issue here since as humans with differing educational background, we will have strengths in some areas more than others. Instead the question is can you identify areas that need strengthened and communicate them? If you can, then you have a good shot at succeeding in this class.\n Is R difficult? Learning R can be especially challenging at first—it’s akin to learning a new language like Spanish or even mathematics. Even experienced R users get frustrated…and so much so that some of us have swear jars. However as silly as it sounds one of the best feelings is to overcome a roadblock. With that said, if you find yourself getting irritated, try the following: take a break, go let some frustration out, sleep, discuss with a peer, etc. If you are at your limit, take a few breaths and contact me!\n What if I find a mistake? Tell me! I strive to be error free but like everyone else, make silly mistakes. This includes grammar and spelling errors as well!\n  Course materials There are two texts and three software packages necessary for this course. With that said, you will receive some supplementary materials in the course as well.\nTexts We’ll rely on the texts below:\n Frankfort-Nachmias, C. and Leon-Guerrero, A. (2020). Social Statistical for a Diverse Society (9th ed.). Sage.\n American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\n  There will occasionally be additional articles and videos to read and watch. When this happens, links to these other resources will be included on the content page for that session.\n Software R and RStudio You will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard — R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code. Please note that\nyou do not need to have any programming experience to use R\n R is free, but it can sometimes be a task to install and configure. To make life easier, you can opt to use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This is recommended for those of you who do not want to install programs right now but please note that you will most likely have to in the near future.\nThe service is convenient, but please keep in mind that it can be slow and is not designed to handle large data sets or more complicated analysis and graphics. You also cannot customize much with RStudio.cloud. Over the course of the term, you’ll want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here.\n  Note: Online help Data science and statistical programming can be difficult. Computers are stupid and its always the tiny errors in your coding can result in tons of headache. People working in any syntax based software package at any level experience this!\nBut there are multiple resources online to help.\n First you will have access to Data Camp which allows you to relieve professional training that otherwise costs a lot of money. Is it perfect and flashy? Nope but it is interactive and if you pay attention, then R’s initial steep learning curve won’t feel like an uphill battle.\n Second two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)). I freely admit that StackOverflow has saved me multiple hours of frustration. Just note if you have a syntax or process issue, both sites require you to produce a minimally reproducible example.\n Third using Twitter you can post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\n Fourth searching for help with R on Google (or another indexed search site) can sometimes be tricky because the program name is,for a lack of a better explanation, a single letter.\n Fifth we have a class chatroom which may be accessed via Slack where you can poise a question. I will monitor Slack regularly and will respond quickly. (This is a rare instances where I keep notifications on so please utilize it!) Ask questions about the readings, exercises, and mini projects. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too.\n Sixth and lastly you can send items by Slack or email me with questions about content, R or whatever (after giving a graduate student effort of course). Please include the subject header EDP 613 “the title of your email” where the title of your email is given without the quotes. Know that I will not just give you the answer, but am happy to push you in the right direction. If you do email me, please include the following:\n A brief description of the problem and what you have done thus far including the line number of the issue (if applicable). A copy of your data set (if applicable). A copy of your script or Rmarkdown document with comments throughout so I know what you’ve tried historically leading up to the issue.   I have a mail filter that will automatically bring your email to a top priority status.1\nYou should absolutely expect to struggle at times, but there is no better and more satisfying feeling than figuring things out for yourself! In the long run, you’re more likely to remember things you’ve figured out rather than those you’ve been shown or told.\n  Assignments and Grades You can find descriptions for all the assignments on the assignments page.\n   Percent  Assignment      10  Data Camp    10  Data Explorations    10  Homeworks    20  Quizzes    25  Exams    25  Final Exam      \n   Grade  Range      A  90–100%    B  80%-89%    C  70–79%    D  60–69%    F  \u0026lt; 60%      R, Rmarkdown, Extra Credit and Showing Work If you submit your work in Rmarkdown, you may earn up to an extra five percent on any task (this includes quizzes and exams!). Additionally in lieu of showing step by step work, you may submit R code instead. In these instances, both the code used and output must be presented but you still must provide a written or typed interpretation in context. Do note that certain items will explicitly ask for work to be shown by hand. In those cases, work in R will not be accepted.\n Your hours Please watch this video:\n Your hours (formerly known as office hours) are set times dedicated to you the student!2). This means that I will be in my office at home waiting for you to come by talk to me remotely with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns.\nBecause of the pandemic, we cannot meet in person. I can meet you online via Zoom. You can request a meeting through either e-mail or Slack.\n Class conduct and expectations Here are the rules, expectations, and policies that I came up with or stole from other professors:\n Late work: Barring the exams and quizzes, past due deliverables will only be accepted up to 48 hours after the initial time and due date. For each full day an assignment is late, 10% of the final grade will be deducted. All submissions must be made via eCampus. There are no exceptions to this policy. Please note that I will not accept coursework by email or any other means.\n Showing work: On any submission, you must show all relevant work where applicable. I don’t know what you know so its your job to provide all of the necessary evidence to convince me that you do know what you say you know. While you’ve probably heard this multiple times over your life, think of it this way: if someone tells you that not only is the Earth flat, but the core is actually made out of styrofoam and Moscato, the first question you should probably ask is what’s your evidence? and possibly the second may be is the core delicious?.\n Participation: Please ensure that you are engaged and participate in class. Engagement is mostly defined by you—if that means commenting and answering questions, neat; if it means sitting quietly and being focused, also neat; but if it means being being disrespectful or flaking off, not so neat.\n Rubrics: While there are valid reasons for the utilization of a rubric in undergraduate classes, at the graduate level, I do not (often) provide nor use a such an item to guide or evaluate your submission due to four primary concerns:\nWhen writing anything in academia that is pivotal (a thesis, dissertation, journal article, report, etc.), there is (typically) no such document as a rubric. If you write within the limitations as defined in a rubric, then creativity may be stifled (i.e. writing to the rubric rather than constructing a product from the ground up). Feedback can only be given along the criteria listed within a rubric which limits your learning as a student and constrains me as the instructor. Unless you are in a very specific area, the real world does not use rubrics!   Technology use: Use phones, computers, etc. responsibly. You’re all adults…well I am most of the time.\n My philosophy:\nJust assume that all submissions are formal and must be submitted with the appropriate use of language, grammar, syntax, etc. and follow standard APA 7th edition formatting guidelines where applicable. People who are easily offended by content, believe their work to be flawless or are generally unable to handle criticism should consider looking at another course. If you want rainbows and ponies, consider scrapbooking. If you care about data and learning highly marketable skills, you’ve come to the right place. There is a great deal of content in this course and you will likely struggle with some at times. Given that, there is also something to be said about the satisfaction a person gets when figuring something out, but nowhere is it written that has to be on your own. You may find that a nudge here or there elicits the same feeling so please reach out for help.    Learning and knowledge during a pandemic When course objectives are written explicitly and clearly, they provide the information you need to figure out what a student should be able to do by the end of a given term. In fact, professors often test your proficiency in an area through multiple assessments such as exams, papers, presentations, etc where you are essentially asked to show us what you have learned. However learning is not the same as knowledge .\nTo save you from a long philosophical narrative on epistemology, in a nutshell we humans aren’t that good at evaluating a person’s knowledge mainly because its not a well-defined concept. With that said, we believe one indicator of knowledge is in a person’s ability to successfully explain a high level concept in such a way that the lay person can understand it. Every so often, consider asking yourself this:\nCan I describe whatever using conversational language so that a child could understand it?\n On top of what’s noted above and I’m not sure how to articulate this any better - life sucks right now! It is likely by now you know people who have been hospitalized or passed away, lost their jobs, and/or tested positive for COVID-19. Additionally stresses in life are up. Given this, here is my promise to you:\nIf you keep an open line, show initiative, and let me know ahead of time if something is going not according to plan3, I will do everything I can to help you learn everything you were hoping to learn from this class!\n   Course policies Its pretty simple: Be nice. Be honest. Don’t cheat. Stay in touch. Be a good human.\n We will also follow WVU’s Code of Conduct.\nThis syllabus reflects a plan for the term but things change and plans change. so deviations may become necessary as we move along during the term. Note that I reserve the right to alter or amend this syllabus and will send notifications if course tasks are affected.\n COVID-19 Statement WVU is committed to maintaining a safe learning environment for all students, faculty, and staff. Should campus operations change because of health concerns related to the COVID-19 pandemic, it is possible that this course will move to a fully online delivery format. If that occurs, students will be advised of technical and/or equipment requirements, including remote proctoring software.\nIn a face-to-face environment, our commitment to safety requires students, staff, and instructors to observe the social distancing and personal protective equipment (PPE) guidelines set by the University at all times. While in class, students will sit in assigned seats when applicable and wear the required PPE. Should a student forget to bring the required PPE, PPE will be available in the building for students to acquire. Students who fail to comply will be dismissed from the classroom for the class period and may be referred to the Office of Student Conduct for further sanctions.\nIf a student becomes sick or is required to quarantine during the semester, they should notify the instructor. The student should work with the instructor to develop a plan to receive the necessary course content, activities, and assessments to complete the course learning outcomes.\nPsychological and Psychiatric Services Life at WVU can be complicated and challenging, especially during a pandemic! You might feel overwhelmed, experience anxiety or depression, or struggle with relationships or family responsibilities. Psychological and Psychiatric Services provides free, confidential support for students who are struggling with mental health and emotional challenges. The office is staffed by professional counselors and psychiatrists who are attuned to the needs of all types of college and professional students. Please do not hesitate to contact them for assistance—getting help is a smart and courageous thing to do.\n CARE Team If you or anyone you know may be at-risk such as those listed here, please make a CARE referral. You may do so directly at the main WVU CARE TEAM site.\n Lauren’s Promise I will listen and believe you if someone is threatening you.\n Lauren McCluskey, a 21-year-old honors student athlete, was murdered on October 22, 2018 by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or the Campus Police at 304-293-3136.\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or call/text a private Title IX On-Call Line 304-906-9930.\nAny form of sexual harassment or violence will not be excused or tolerated at West Virginia University. WVU has instituted procedures to respond to violations of these laws and standards, programs aimed at the prevention of such conduct, and intervention on behalf of the victims.\n Academic Integrity The integrity of the classes offered by any academic institution solidifies the foundation of its mission and cannot be sacrificed to expediency, ignorance, or blatant fraud. Therefore, I will enforce rigorous standards of academic integrity in all aspects and assignments of this course. For the detailed policy of West Virginia University regarding the definitions of acts considered to fall under academic dishonesty and possible ensuing sanctions, please see the West Virginia University Academic Catalog at http://catalog.wvu.edu/undergraduate/coursecreditstermsclassification/#academicintegritytext. Should you have any questions about possibly improper research citations or references, or any other activity that may be interpreted as an attempt at academic dishonesty, please see me before the assignment is due to discuss the matter.\n Inclusivity Statement The West Virginia University community is committed to creating and fostering a positive learning and working environment based on open communication, mutual respect, and inclusion.\nIf you are a person with a disability and anticipate needing any type of accommodation in order to participate in this class, please advise me and make appropriate arrangements with the Office of Accessibility Services (304-293-6700).\nFor more information on West Virginia University’s Diversity, Equity, and Inclusion initiatives, please see http://diversity.wvu.edu.\n Incomplete Grades Students who want to be considered for an Incomplete must apply to their instructor prior to the end of the term. If the instructor agrees, the instructor and the student must negotiate the conditions under which the grade of I will be changed to a letter grade and sign a contract. The date to submit the incomplete work should not be set beyond the last day of class of the following semester. If the student does not complete the terms of contract, then the instructor should submit a grade of F. All incomplete contracts must be filed with the department and Dean’s Office. See the policy at [Students who want to be considered for an Incomplete must apply to their instructor prior to the end of the term. If the instructor agrees, the instructor and the student must negotiate the conditions under which the grade of I will be changed to a letter grade and sign a contract. The date to submit the incomplete work should not be set beyond the last day of class of the following semester. If the student does not complete the terms of contract, then the instructor should submit a grade of F. All incomplete contracts must be filed with the department and Dean’s Office. See the policy at http://catalog.wvu.edu/undergraduate/enrollmentandregistration/#gradestext.\n Sale of Course Materials All course materials, including lectures, class notes, quizzes, exams, handouts, presentations, and other materials provided to students for this course are protected under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Please review the sharing and editing restrictions prior to distributing or amending any material on this site. As such, the unauthorized purchase or sale of these materials may result in disciplinary sanctions under the Campus Student Code. Basically you can share what you like but don’t try to make a buck.\n Student Evaluation of Instruction (SEI) Effective teaching is a primary mission of West Virginia University. Student evaluation of instruction provides the university and the instructor with feedback about your experiences in the course for review and course improvement. Your participation in the evaluation of course instruction is both strongly encouraged and highly valued. Results are strictly confidential, anonymous, and not available to the instructor until after final grades are released by Admissions and Records. Information about how you can complete this evaluation will be provided later.\n University Attendance Policy At West Virginia University, class attendance contributes significantly to academic success. Students who attend classes regularly tend to earn higher grades and have higher passing rates in courses. Excessive absences may jeopardize students’ grades or even their ability to continue in their courses. There is a strong correlation between regular class attendance and academic success.\n Course Netiquette The basic premise is that the etiquette expected of students in the online environment is the same as that expected in a classroom. Common courtesy is the guiding rule of Internet communications. Be prepared to communicate effectively when taking an online course. Following these simple netiquette rules in your online class or education environment will ensure your success:\n Include a professional salutation. In this case, “Hello Dr. Roy” or “Dear Dr. Roy” is appropriate. Include a proper ending such as “Thank you” or “With regards.” Then type in your full name. Never type in ALL CAPS, because it reads as if you ARE SHOUTING AT PEOPLE. Act as professionally, via your writing, as you would in a face to face classroom. Refrain from inappropriate language and derogatory or personal attacks. Do not dominate any discussion. Give other students the opportunity to join in the discussion. Disagree with ideas but avoid challenges that may be interpreted as a personal attack. Check that you are replying to the specific person you intend, and not to the entire class. Never give your password to another person. Respect the virtual classroom. Never forward in-class communications or posts by others outside of this virtual space. Never spam your classmates. If you quote someone’s previous post, only quote enough to make your point.  Be aware of the University’s Academic Integrity and Dishonesty Policy http://catalog.wvu.edu/undergraduate/coursecreditstermsclassification/#academicintegritytext. You can review the rules, regulations, and procedures concerning student conduct and discipline for the main campus of West Virginia University, at http://campuslife.wvu.edu/r/download/1802350.\n Response Time I generally respond to Slack queries in the same day while responses to emails and discussion posts are within 48 hours, except during holidays. Often, I will reply much more quickly but you should not count on a immediate. Please plan accordingly so that you don’t miss deadlines! I generally return assignments within one to two weeks after a final submission date.\n Technical Requirements Students need to have access to a computer for word processing, e-mail and access to eCampus. Access to the Internet is necessary for completion of this course. Run the Browser Check. This tool will check that you are using a supported Internet browser and have a valid Java version installed. The required technical skills to participate in this course are:\nNavigate the web Use email with attachments Create and submit files in commonly used word processing program formats Copy and paste Download and install software Consult software tutorials and other online sources as a method of learning software features Use syntax when necessary  Notice that programming is not on here!\n Technical Support Technical support regarding your use of eCampus is available by contacting 304-293-4444 (telephone), 1-877-327-9260 (toll free number), itshelp@mail.wvu.edu (email), and/or http://it.wvu.edu (website).\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n   In this new normal, I receive an overwhelming number of emails per day and may miss yours if the header is not formatted properly.↩︎\n There is some misunderstanding about what office hours actually are! For some reason that is not clear, particular graduate students have noted in my course evaluations that they believe these to be the times I should not be disturbed. This is not just a local issue!, which is the exact opposite of what they are for!↩︎\n I realize sometimes this just isn’t possible so contact me as soon as you can.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course objectives FAQ Is the course content difficult? Is R difficult? What if I find a mistake?  Course materials Texts Software Note: Online help  Assignments and Grades R, Rmarkdown, Extra Credit and Showing Work Your hours Class conduct and expectations Learning and knowledge during a pandemic  Course policies COVID-19 Statement Psychological and Psychiatric Services CARE Team Lauren’s Promise Academic Integrity Inclusivity Statement Incomplete Grades Sale of Course Materials Student Evaluation of Instruction (SEI) University Attendance Policy Course Netiquette Response Time Technical Requirements Technical Support    Instructor  Dr.","tags":null,"title":"Syllabus","type":"page"}]