[{"authors":["admin"],"categories":null,"content":"I hold a Ph.D. in Program Evaluation and am an Assistant Professor of Educational Psychology within the Department of Counseling and Learning Sciences in the College of Education and Human Services at West Virginia University. I currently perform research on the teaching of developmental evaluation, modeling of social science concepts using machine learning, and in the construction of predictive social networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0ebaba4b5b8aaba6189e8c7aaa6409a6","permalink":"/author/dr.-abhik-roy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-abhik-roy/","section":"authors","summary":"I hold a Ph.D. in Program Evaluation and am an Assistant Professor of Educational Psychology within the Department of Counseling and Learning Sciences in the College of Education and Human Services at West Virginia University.","tags":null,"title":"Dr. Abhik Roy","type":"authors"},{"authors":["admin"],"categories":null,"content":"Abhik Roy holds a Ph.D. in Program Evaluation and is an Assistant Professor of Educational Psychology within the Department of Learning Sciences and Human Development in the College of Education and Human Services at West Virginia University. He currently performs research on the teaching of developmental evaluation, modeling of social science concepts using machine learning, and in the construction of predictive social networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/dr.-abhik-roy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-abhik-roy/","section":"authors","summary":"Abhik Roy holds a Ph.D. in Program Evaluation and is an Assistant Professor of Educational Psychology within the Department of Learning Sciences and Human Development in the College of Education and Human Services at West Virginia University.","tags":null,"title":"Dr. Abhik Roy","type":"authors"},{"authors":null,"categories":null,"content":"  Here are multiple resources and guides related to data, R and other relevant topics. None of these are required but could be helpful in or beyond the course!\n","date":1594771200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1594771200,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"Here are multiple resources and guides related to data, R and other relevant topics. None of these are required but could be helpful in or beyond the course!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"   Lessons via Data Camp Homeworks Data Tasks Quizzes and Exams A Note About Discussion Board Postings   You will get the most of out this class if you:\nengage with the readings and materials, use R to complete any data wrangling or analysis, ask for help immediately after giving it (whatever it is) a good faith effort, and keep an open and honest line of communication with me and your peers if possible.  Each type of assignment in this class helps with at least one one of these criteria.\nLessons via Data Camp Most class sessions have interactive lessons via Data Camp. Your task is simply to go through these.\nI will grade these short exercises using a check system:\n ✔+: (11.5 points (115%) in gradebook) Modules are 100% completed. Every task was attempted and answered, and most answers are correct. These are not earned often. ✔: (10 points (100%) in gradebook) Modules are 70–99% complete and most answers are correct or on point. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Modules are less than 70% complete and/or most answers are incorrect or off-point. This indicates that you need to improve next time. Hopefully people will not earn this often.  Otherwise 0 points (0%) in gradebook.\n Homeworks On most weeks, you will be asked to submit a set of problems assigned after the prior week’s class. These must be submitted as a single document in the correct numerical order and submitted on the Submission Portal within ecampus as a single cohesive document (in either in docx or pdf format). If you learn how to type your homework in Rmarkdown, you will earn an extra 5% on each submission where this is done.\n Data Tasks Some classes also have fully annotated examples of performing tasks in R code that teach and demonstrate how to do specific concepots we cover here or elsewhere and on Data Camp.\nI will grade these short exercises using a check system:\n ✔+: (11.5 points (115%) in gradebook) Exercises are 100% completed. Every task was attempted and answered, and most answers are correct. Knitted document are clean and easy to follow. This indicates that your work is exceptional. These are not earned often. ✔: (10 points (100%) in gradebook) Exercises are 70–99% complete and most answers are correct or on point. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect or off-point. This indicates that you need to improve next time. Hopefully people will not earn this often.  Otherwise 0 points (0%) in gradebook.\nAgain, I am not grading your coding ability, checking each line of code or syntax to make sure it produces the exact output, and most importantly, I am not looking for perfection! Try hard, get frustrated, do good work and you’ll get a ✓ or even a ✔+.\nI encourage you to work together on the courses if possible, but you must turn in your own work on eCampus.\n Quizzes and Exams Exact dates will be posted here soon but you will have a announced quizzes, three exams and a final. Please note if you have an A going into the final exam, you do not need to take it!\n A Note About Discussion Board Postings While you may like or even love them, I hate discussion board posts. I hated them as a graduate student and still hate them as a professor. I recently made the decision to never use discussion board posts again for many reasons which are not useful to discuss here.\nI always prefer face to face or at least conferencing if needed. With that said, we have Zoom and a Slack channel so please use both!\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Lessons via Data Camp Homeworks Data Tasks Quizzes and Exams A Note About Discussion Board Postings   You will get the most of out this class if you:","tags":null,"title":"About the Assignments","type":"docs"},{"authors":null,"categories":null,"content":"  This section contains extra material to help you understand a concepts covered for that week. Sometimes this is just a few links for you to look at and other times its fully annotated R code that you can use as a reference for helping you along with a task. Please access the material on this site this section after you have finished the reading(s).\nFrom time to time, I may also contains video of coding some examples pieces so you can see what it looks like to work with R in real time. You’ll definitely notice that I make all sorts of errors, which is absolutely normal and happens to everyone!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains extra material to help you understand a concepts covered for that week. Sometimes this is just a few links for you to look at and other times its fully annotated R code that you can use as a reference for helping you along with a task.","tags":null,"title":"About the Examples","type":"docs"},{"authors":null,"categories":null,"content":"  Each class session has an interactive lesson that you will work through after doing the readings and watching a lecture (if applicable). These lessons are a central part of the class—they will teach you how to use R and other packages eventually leading to the tidyverse family.\nInteractive training sections are provided on Data Camp.\nHere is some advice. Carve out some time everyday to go through these. If you try to complete everything in one sitting, it will probably be overwhelming! However if you have familiarity with some modules, please feel free to work ahead.\nPlease note that if you have (1) used Data Camp before and (2) are logged in with the same username, then any module that was successfully completed will not have to be done again.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"45e63e789e3cb381d68e4ca47e0a453c","permalink":"/lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/","section":"lesson","summary":"Each class session has an interactive lesson that you will work through after doing the readings and watching a lecture (if applicable). These lessons are a central part of the class—they will teach you how to use R and other packages eventually leading to the tidyverse family.","tags":null,"title":"About the Lessons","type":"docs"},{"authors":null,"categories":null,"content":"  The Course Text Each class session has a set of required readings that you should complete before doing anything else. It also allows you to ask questions prior to moving on! With that said, think of the text as a reference, in that\n it contains a lot of stuff that you may use but probably won’t right now, and it gives you a basis to ask intelligent questions.  So you should absolutely read it to get a basis but in the long run, you will learn more from this course by doing.\n The Course Nontext Many of us are visual learners with me included, so a lot of what I provide outside of the text is visually based. Resources such as presentations, videos, visualizations, walkthroughs, etc. take a lot of time to construct so please take some time to go through them. Are they Hollywood? No because I’m a professor at a public university, have three children and drive a car with duct tape on it BUT they’ll be decent. I am always open to suggestions so if you have any, send them along! Pictures are better than words because some words are big and hard to understand. - Peter Griffin\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"0aa019bdc1e0c98e24563159b8fa2f91","permalink":"/readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/","section":"readings","summary":"The Course Text Each class session has a set of required readings that you should complete before doing anything else. It also allows you to ask questions prior to moving on!","tags":null,"title":"About the Readings","type":"docs"},{"authors":null,"categories":null,"content":"  This section will let you know what must be completed for the purposes of assessment1 or preparation2. You should still do everything else because those items are still expected and you will be scored on those skills eventually on some other course related task.\n What you have to do after a current class ends.↩︎\n What you have to do before the next class begins.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"140e53d0243061ae64e96f1d35188d6a","permalink":"/due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/","section":"due","summary":"This section will let you know what must be completed for the purposes of assessment1 or preparation2. You should still do everything else because those items are still expected and you will be scored on those skills eventually on some other course related task.","tags":null,"title":"So What's Due?","type":"docs"},{"authors":null,"categories":null,"content":"  You can download a BibTeX file of all the readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows/macOS, or Zotero or Mendeley online.\nIf you are comfortable with the terminal in a Mac, then the bib2ris may be easy to implement.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"68be32a8da6a38dd54a9e724ab3904a0","permalink":"/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows/macOS, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"    Class Slack Account Access Week 1 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Class Slack Account Access Please sign up for Slack!\nGo to the course Slack account Register if you already do not have an account. Note that you do not have to use your WVU account, but its not a bad idea. Head to to the channel #introduce-yourself and well introduce yourself by tell everyone about yourself, what you hope to achieve out of this course, and something about you that really defines who you are. For example, here is something about me:\nI teach data visualization BUT I am also about 40% colorblind! How do I know what color to use? Take EDP 693C: Data Visualization in the Spring to find out! 1\n (optional) Provide a picture if you are willing. I am a very visual person as are many other people so pictures help a great deal. However, you are not mandated to do so!   Week 1 Problem Set Assignment Chapter 1 Exercises: 3, 4, 6, 7, and 8. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  3  Interval–ratio Interval–ratio Nominal Ordinal Nominal Interval–ratio Ordinal    4  Discrete Continuous Continuous    6  Unemployment records could be used to determine the actual number of unemployed; a descriptive statistic based upon the population. A survey is taken to estimate student opinions about the quality of food; inferential statistic. National health records can be used to determine the incidence rate of breast cancer among all Asian women, so this would be a descriptive statistic. The ratings will be gathered from a survey, so this is an inferential statistic. A university should be able to report GPA by major, so this is a descriptive statistic based upon the population. In theory, the United States records all immigrants to this country. Therefore, the number of South East Asian immigrants would be a descriptive statistic. However, because of illegal immigration, surveys are also taken to estimate the total number of legal and unauthorized immigrants. In that event, the number of immigrants would be an inferential statistic.    7  Annual income Gender: nominal\nNumber of hours worked per week: interval ratio\nYears of education: interval ratio\nJob title: nominal. This is an application of inferential statistics. The researcher is using information based on her sample to predict the annual income of a larger population of young graduates.    8  At the nominal level, a simple measure of political participation is whether or not someone voted in the most recent general election. This variable would be coded either yes or no.\nAt the ordinal level, a composite measure could be constructed of both voting and political party membership, like this:\n  Behavior  Code      Didn’t vote; No membership  0    Voted; No membership OR Membership; Didn’t vote  1    Voted; Membership  2     These codes are ordinal in scale because the amount of political participation can be ranked from high to low. Other possible ordinal variables can be constructed from other sets of behaviors, such as working in a candidate’s campaign and signing a petition. The key points are to create a variable whose values can be ranked and whose values are not on an interval–ratio scale.\nAt the interval–ratio level, political participation could be measured by the percentage of elections in which a person has voted since becoming eligible to vote, or the amount of money a person donated to political candidates during some specified time period.\n :::\n   Shameless plug↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0c685bccb51eb8b27996aa20967c387f","permalink":"/assignment/01-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/01-homeworks/","section":"assignment","summary":"Class Slack Account Access Week 1 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Class Slack Account Access Please sign up for Slack!","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 1 after class ends What to do for Week 2 before coming to class   What to do for Week 1 after class ends1   eCampus?  Description  Location         Start a module in Data Camp that addresses the basic elements of R  Link       Complete problems 3, 4, 6, 7, and 8 in Chapter 1.  Link     \n What to do for Week 2 before coming to class2   Area  Description  Location         Read chapter 2 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on Social Explorer and the WVU Libraries Database.  Link       Watch a video on Data Visualization.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ac82a4ccda585b1b2dd58a008ca65ad","permalink":"/due/01-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/01-due/","section":"due","summary":"What to do for Week 1 after class ends What to do for Week 2 before coming to class   What to do for Week 1 after class ends1   eCampus?","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Nothing!   Nothing! There are no examples this week. Go do something!\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"059bb398e999a9d10b388c3df2b5644f","permalink":"/example/01-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/01-example/","section":"example","summary":"   Nothing!   Nothing! There are no examples this week. Go do something!\n ","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes   Data Camp The first Data Camp module is due the Wednesday following the completion of the Week 1 class. It provides an introduction to the structure in and elementary use of R with an end goal of getting you familiar with its functionality. In particular the sections address:\n Intro to basics Vectors Matrices Factors Data frames Lists  As noted last week, there is a lot on there but please refer to the syllabus for the grading policy regarding those tasks. Just remember, this is not a programming class!\n Book Materials Download the textbook\n PowerPoint and Lecture Notes.   Class Notes Download the in-class slides via\n PDF and Outline.   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2248ef62e0fddbf88b2832cdafb38b9a","permalink":"/lesson/01-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/01-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes   Data Camp The first Data Camp module is due the Wednesday following the completion of the Week 1 class. It provides an introduction to the structure in and elementary use of R with an end goal of getting you familiar with its functionality.","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Get Some Advice Review Syllabus Book Stuff   Get Some Advice First things first: Read the syllabus. Far be it from us to disagree with Snoop, so you should probably go read the syllabus as soon as possible!\n Review Syllabus Review the overviews of the other areas as well: content, lessons, examples, and assignments pages for this class.\n Book Stuff Read Chapter 1: The What and the Why of Statistics in Frankfort-Nachmias and Leon-Guerrero.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ca270d2bbdb77aeeab9c41859c1ca7a","permalink":"/readings/01-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/01-readings/","section":"readings","summary":"Get Some Advice Review Syllabus Book Stuff   Get Some Advice First things first: Read the syllabus. Far be it from us to disagree with Snoop, so you should probably go read the syllabus as soon as possible!","tags":null,"title":"The What and Why of Statistics","type":"docs"},{"authors":null,"categories":null,"content":"    Week 2 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 2 Problem Set Assignment Chapter 2 Exercises: 1, 2, 3, 4, 5, 6 and 7. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  Race is a nominal. variable; Since categories can be ordered, Class is an ordinal variable; Trauma is an interval variable.\n Frequency table for Race:\n     Race  Frequency      White  17    Non-White  13    Total  30      We have     Classification  Proportion      White  17/30 = 0.57    Non-White  13/30 = 0.43    Total  30/30 = 1.00        2  Frequency and Percentage Distribution Table for Class:\n   Class  Frequency  Percent      Lower  3  10    Working  15  50    Middle  11  36.7    Upper  1  3.3    Total  30  100      The smallest perceived class group is the upper class, making up only 3.3% of the survey.\n Together, the working and middle class make up 50% + 36.7% = 86.7% of the survey.\n    3  Frequency table for Traumas:\n   Number of Traumas  Frequency      0  15    1  11    2  4    Total  30      Trauma is an interval- or ratio-level variable, since it has a real zero point and a meaningful numeric scale.\n People in this survey are more likely to have experienced no traumas last year (50% of the group).\n The proportion who experienced one or more traumas is calculated by first adding 36.7% and 13.3% equaling 50% and then divide that number by 100 to obtain the proportion, 0.50, or half the group.\n    4  Unfortunately since other visualizations have not been introduced, pie charts are likely the best method approach to present these data.\n  5  Yes because 79.2% had “hardly any” confidence in the press, compared to 19.3% of those who voted for Clinton.\n  6     E-mail Hours Per Week  Frequency  Cumulative Frequency  Percent  Cumulative Percent      0  19  19  19  19    1  20  39  20  39    2  13  52  13  52    3  5  57  5  57    4  2  59  2  59    5  6  65  6  65    6  5  70  5  70    7  2  72  2  72    8  3  75  3  75    9  1  76  1  76    10 or more  23  99  23  99    Total  99    99          7  According to this time series chart, rates of voting have consistently varied by race and Hispanic origin. The group with the largest increase in voting rates is Blacks, from 53% in 1996 to 66.6% in 2012. However, in 2016, the voting rates for Blacks declined to 59.6%–the lowest it has been since the 2000 election when it was 56.9%. Hispanic voting rates have, for the most part, remained stable, although with some fluctuation most notably from the 1992 election (51.6%) to the 1996 election (44%). As noted in the exercise, in the 2012 presidential election, Blacks had the highest voting rates for all groups, followed by non-Hispanic Whites, non-Hispanic other races, and Hispanics. White voting rates increased by 1.2% from 2012 to 2016. The highest voting rate for Whites was in 1992 (70.2%), 1992 for Hispanics (51.6%), 2012 for Blacks (66.6%), and 1992 for non-Hispanic other races (54%).\n :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dbe504b058cb2e5a3ee53153a9fa3a5b","permalink":"/assignment/02-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/02-homeworks/","section":"assignment","summary":"Week 2 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 2 Problem Set Assignment Chapter 2 Exercises: 1, 2, 3, 4, 5, 6 and 7.","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 2 after class ends What to do for Week 3 before coming to class   What to do for Week 2 after class ends1   eCampus?  Description  Location         Start a module in Data Camp that addresses some essential tidyverse commands.  Link       Review the R Walkthrough on the importance of data frames.  Link       Complete problems 1, 2, 3, 4, 5, 6 and 7 in Chapter 2.  Link     \n What to do for Week 3 before coming to class2   Area  Description  Location         Read chapter 3 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on the Measures of Central Tendency.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac09a96a006ea325c7f2e74716cc60ae","permalink":"/due/02-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/02-due/","section":"due","summary":"What to do for Week 2 after class ends What to do for Week 3 before coming to class   What to do for Week 2 after class ends1   eCampus?","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics Social Explorer   Crash Course Statistics1 Crash Course Statistics is a freely available channel on YouTube which covers a range of topics in..well statistics. While I encourage you to watch all of their videos, the ones that really applies to Chapter 2 is one of two discussions on visualizations which you can see below.\n \n Social Explorer Social Explorer is a paid site that allows people to visualize public data sets pretty easily and without the need for coding anything. Luckily WVU has a site license for the site but it can be a bit difficult to locate. Follow the video below to get an idea of Social Explorer and the WVU Database.\n     Most weeks I’ll post some external resources that may help you get the overall idea of what we are covering. These are optional, but the presentations are generally engaging and the visuals are created by people who are more talented than me so it may be worth your time.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"/example/02-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"Crash Course Statistics Social Explorer   Crash Course Statistics1 Crash Course Statistics is a freely available channel on YouTube which covers a range of topics in..well statistics.","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough   Data Camp The second Data Camp module is due the Wednesday following the completion of the Week 2 class. It covers the tidyverse family of packages in R which we will be using throughout the remainder of the course. In particular the sections address:\n Data wrangling1 Data visualization Grouping and summarizing Types of visualizations   Book Materials Download the textbook\n PowerPoint and Lecture Notes.   Class Notes Download the in-class slides via\n PDF and Outline.   R Walkthrough First click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download a PDF of the presentation above.\n  Please get used to this term!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"911ebe9376618b75c6ca9ac02110b8b9","permalink":"/lesson/02-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/02-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough   Data Camp The second Data Camp module is due the Wednesday following the completion of the Week 2 class.","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 2: The Organization and Graphic Presentation of Data in Frankfort-Nachmias and Leon-Guerrero.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"510696b7bb47e51213a79e8cd09e243d","permalink":"/readings/02-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/02-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 2: The Organization and Graphic Presentation of Data in Frankfort-Nachmias and Leon-Guerrero.\n ","tags":null,"title":"The Organization and Graphic Presentation of Data","type":"docs"},{"authors":null,"categories":null,"content":"    Week 3 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 3 Problem Set Assignment Chapter 3 Exercises: 1, 2, 3, 4, 5, 6, 10, 11. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  The mode is Routine.\n The median is Routine.\n Based on the mode and median for this variable, most respondents indicate that their lives are Routine.\n A mean score is nonsense for nominal measurements.\n    2  The is ordinal.\n By using either figure related to the highest frequency or the highest percentage, the outcome is strongly agree.\n By using either related to the highest frequency or the highest percentage, the outcome is strongly agree for both years this indicating a high level of support. When combined agree, more than half of the 2014 (32.5% + 25.3% = 57.5%) and (39.5% + 27.0% = 66.5%) 2018 GSS samples agree that homosexuals should have the right to marry.\n    3  This has a level of an interval-ratio. Using frequencies1, we have\n\\[\\begin{aligned} \\dfrac{N+1}{2}\u0026amp;= \\dfrac{32+1}{2}\\\\\\\\ \u0026amp;= 16.5 \\end{aligned}\\] Adding up the frequencies until locating the 16th and 17th cases yields the category 40 hr worked last week.\n We have\n   25th percentile = \\((32 × 0.25)\\) implying the 8th case, or 30 hr worked last week. 50th percentile is simply the median implying the category 40 hr worked last week. 75th percentile = \\((32 × 0.75)\\) implying the 24th case, or 40 hr worked last week.    4  The mode category is agree.\n Using frequencies, we find that the median is 2, or agree.\n    20th percentile = \\((835 × 0.20)\\) implying the 167th case, or strongly agree. 80th percentile = \\((835 × 0.80)\\) implying the 668th case, or agree.    5       Race  Mode  Median      Black  Seldom  Sometimes    Hispanic  Everyday  Nearly everyday    White  Everyday  Most days      Teens’ breakfast habits vary by race/ethnicity. Out of the three racial/ethnic groups, Black students were more likely to report seldom or sometimes eating breakfast. On the other hand, White and Hispanic students eat breakfast more frequently. The mode for White and Hispanic students is everyday.    6  The mode for both Males and Females is Working full-time.\nUsing frequencies, we have\n Males:\n\\[\\begin{aligned} \\dfrac{N+1}{2}\u0026amp;= \\dfrac{596+1}{2}\\\\\\\\ \u0026amp;= 298.5 \\end{aligned}\\] Adding up the frequencies until locating the 298th and 299th cases yields the category Working Full Time.\n Females:\n\\[\\begin{aligned} \\dfrac{N+1}{2}\u0026amp;= \\dfrac{781+1}{2}\\\\\\\\ \u0026amp;= 391 \\end{aligned}\\] Adding up the frequencies until locating the 391th case yields the category Working Full Time. Using either the mode or median implies that there are no substantial differences between males and females.\n    10  \n  Hours Worked Last Week  Frequency  Hours Worked Last Week x Frequency      20  3  60    25  2  50    28  1  28    29  1  29    30  3  90    32  1  32    40  14  560    50  2  100    52  1  52    55  1  55    60  1  60    64  1  64    70  1  70    Total  32  1250      so we have \\[\\begin{aligned} \\overline{Y}\u0026amp;=\\dfrac{1250}{32}\\\\\\\\ \u0026amp;\\approx 39.06 \\end{aligned}\\] or about 39 hours.\nSince the median was 40 hr worked last week, the distribution is slightly skewed in a positive direction.    11  Since Clinton and Sanders’ middle-class income amount is higher than the U.S. Census estimated mean or median, their definition of middle class is not based on the statistical middle. It is unclear what they were using but it was not based on the actual measure.  :::\n   You can also do this using cumulative percentages.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ef8cddedad708f695e6676e0c06e973a","permalink":"/assignment/03-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/03-homeworks/","section":"assignment","summary":"Week 3 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 3 Problem Set Assignment Chapter 3 Exercises: 1, 2, 3, 4, 5, 6, 10, 11.","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 3 after class ends What to do for Week 4 before coming to class   What to do for Week 3 after class ends1   eCampus?  Description  Location         Complete a module in Data Camp focused on arguably the most important tidyverse package: dplyr.  Link       Go over an R Walkthrough on distributions and measures of central tendency.  Link       Complete problems 1, 2, 3, 4, 5, 6, 10, and 11 in Chapter 3.  Link     \n What to do for Week 4 before coming to class2   Area  Description  Location         Read chapter 4 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on the Measures of Variability.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2a9ebfd3c92be577bc4d52f8871ae75c","permalink":"/due/03-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/03-due/","section":"due","summary":"What to do for Week 3 after class ends What to do for Week 4 before coming to class   What to do for Week 3 after class ends1   eCampus?","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Central Tendency. Think about the terms, in that something wants to be gauged towards the middle. But as you’ll learn soon, the middle isn’t always where you think it is!\n \nRemember you can always access a Crash Course Statistics playlist using the following link.\n  --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"73664027ae41c739f0c70a62c901a4c5","permalink":"/example/03-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/03-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Central Tendency.","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough   Data Camp The third Data Camp module is due the Wednesday following the completion of the Week 3 class. It covers a particular tidyverse package called dplyr which is by far the one of the most useful tools you can use. In particular the sections address:\n Transforming Data with dplyr Aggregating Data Selecting and Transforming Data Case Study: The babynames Dataset  This now sets us up to do meaningful statistics in R.\n Book Materials Download the textbook\n PowerPoint and Lecture Notes.   Class Notes Download the class\n PDF and Outline.   R Walkthrough Download the data sets1 and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.   Week 3 Walkthrough Materials  Click on the presentation itself and then you may   Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download a PDF of the presentation above.\n  You will have to unzip this file. If you are unfamilair with this process, please check the Unzipping files section under Resources for assistance.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e88a62444161c21a7f4779be93acbf33","permalink":"/lesson/03-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/03-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough   Data Camp The third Data Camp module is due the Wednesday following the completion of the Week 3 class.","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 3: Measures of Central Tendency in Frankfort-Nachmias and Leon-Guerrero.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4e1bd317fe84602507de5f25991f630a","permalink":"/readings/03-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/03-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 3: Measures of Central Tendency in Frankfort-Nachmias and Leon-Guerrero.\n ","tags":null,"title":"Measures of Central Tendency","type":"docs"},{"authors":null,"categories":null,"content":"    Week 4 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 4 Problem Set Assignment Chapter 4 Exercises: 1, 2, 3, 7, 8, 12, 13. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  There are seven response categories for political views.\n We can calculate the percentage square by\n   Political Views\n Percentage \\(c\\)\n Percentage Squared \\(c^2\\)\n     Extremely liberal\n 5.5\n 30.25\n   Liberal\n 12.0\n 144.00\n   Slightly liberal\n 12.3\n 151.29\n   Moderate\n 37.8\n 1428.84\n   Slightly conservative\n 13.2\n 174.24\n   Conservative\n 15.0\n 225.00\n   Extremely conservative\n 4.2\n 17.64\n   Total\n 100.0\n 2171.26\n     yields \\(\\sum c^2 = 2171.26\\).\n We can calculate the IQV by\n\\[\\begin{aligned} IQV\u0026amp;= \\dfrac{7\\cdot(100^2-2171.26)}{100^2\\cdot (7-1)}\\\\\\\\ \u0026amp;= \\dfrac{54801.18}{60000}\\\\\\\\ \u0026amp;\\approx 0.91 \\end{aligned}\\] which is relatively close to 1 suggesting that Americans are fairly diverse in their political views.\n    2  For the female population where \\(N = 629\\), we have\n \\(Q_1 = 629\\cdot 0.25 = 157.25\\)\n  \\(Q_3 = 629 \\cdot 0.75 = 471.75\\)\n or about the 158th and 472nd cases which are representative of cases where the degree are in the High school graduate and Bachelor’s degree categories, respectively. This yields \\(IQR = 471.75-157.25=314.5\\) which includes those who have a highest attained degree of high school, junior college and bachelor’s .\nFor the male population where \\(N = 488\\), we have\n \\(Q_1 = 488 \\cdot 0.25 = 122\\)\n  \\(Q_3 = 488 \\cdot 0.75 = 366\\)\n or the 122nd and 366th cases which are representative of cases where the degree are in the High school graduate and Bachelor’s degree categories, respectively. This yields \\(IQR = 366-122=244\\) that includes those who have a highest attained degree of high school, junior college and bachelor’s .\nSo both IQRs provide the same groups of cases.\n Based on the IQR calculation, we know that 50% of all cases for males and females lies between high school and bachelor’s degree, but that does not inform us about the variability of the distribution. A better measure would be IQV and if interval-ratio measures were available, the variance and standard deviation would provide better information and estimates.\n    3  The range of convictions in   2010 is given by \\(397 – 108 =\\) \\(289\\). 2015 is given by \\(402 – 97 =\\) \\(305\\).  Therefore the range of conviction in 2015 is larger.\nThe mean number in 2010 across all levels of government was\n\\[\\begin{aligned} \\overline{Y} \u0026amp;= \\dfrac{397+108+280}{3}\\\\\\\\ \u0026amp;\\approx 261.67 \\end{aligned}\\] or about an average of 262 convictions.\nThe mean number in 2015 across all levels of government was\n\\[\\begin{aligned} \\overline{Y} \u0026amp;= \\dfrac{402+97+200}{3}\\\\\\\\ \u0026amp;= 233 \\end{aligned}\\] or an average of 233 convictions.\n The sum of squares can be found by\n   Government Level\n Number of Convictions\n \\(Y-\\overline{Y}\\)\n \\((Y-\\overline{Y})^2\\)\n     Federal\n 397\n \\(397 - 261.1 = 135.33\\)\n \\((135.33)^2 = 18314.21\\)\n   State\n 108\n \\(108 - 261.67 = -153.67\\)\n \\((-153.67)^2 = 23614.47\\)\n   Local\n 280\n \\(280 - 261.67 = 18.33\\)\n \\((18.33)^2 = 335.99\\)\n   Total\n 785\n $ $\n 42264.67\n     implying that\n\\[\\begin{aligned} S \u0026amp;= \\sqrt{\\dfrac{42264.67}{2}}\\\\\\\\ \u0026amp;\\approx 145.37 \\end{aligned}\\] So in 2010, there was a standard deviation of about 145.37.\nThe sum of squares can be found by\n   Government Level\n Number of Convictions\n \\(Y-\\overline{Y}\\)\n \\((Y-\\overline{Y})^2\\)\n     Federal\n 402\n \\(402 - 233 = 169\\)\n \\((169)^2 = 28561\\)\n   State\n 97\n \\(97 - 233 = -136\\)\n \\((-136)^2 = 18496\\)\n   Local\n 200\n \\(200 - 233 = -33\\)\n \\((-33)^2 = 1089\\)\n   Total\n 699\n $ $\n 48146\n     implying that\n\\[\\begin{aligned} S \u0026amp;= \\sqrt{\\dfrac{48146}{2}}\\\\\\\\ \u0026amp;\\approx 155.15 \\end{aligned}\\] So in 2015, there was a standard deviation of about 155.15.\n The standard deviation is larger in the latter time period so there is more variability in number of convictions in 2015 than in 2010. This supports the results of the range.\n    7  The range can be found by \\(4.50-2.20=\\) \\(2.30\\).\nMeanwhile for \\(n=10\\), we have\n 25th percentile: \\(10\\cdot 0.25 = 2.5\\)th case so \\(\\dfrac{2.40+2.50}{2}=2.45\\)\n  75th percentile: \\(10\\cdot 0.75 = 7.5\\)th case so \\(\\dfrac{3.60+3.60}{2}=3.60\\)\n S. the IQR is \\(3.60 – 2.45 =\\) \\(1.15\\). While both measures will work, the range is a bit more precise, in that it provides a better picture of the variability of divorce rates for all states in the sample.\n The sum of squares can be found by\n   State\n Divorce rate/thousand\n \\(Y-\\overline{Y}\\)\n \\((Y-\\overline{Y})^2\\)\n     Florida\n 3.6\n 3.5 – 3.14\n 0.210\n   Idaho\n 3.9\n 3.9 – 3.14\n 0.460\n   Maine\n 3.2\n 3.2 – 3.14\n 0.004\n   Maryland\n 2.5\n 2.5 – 3.14\n 0.410\n   Nevada\n 4.5\n 4.5 – 3.14\n 1.850\n   New Jersey\n 2.6\n 2.6 – 3.14\n 0.290\n   Texas\n 2.2\n 2.2 – 3.14\n 0.880\n   Vermont\n 2.9\n 2.9 – 3.14\n 0.058\n   Wisconsin\n 2.4\n 2.4 – 3.14\n 0.550\n   Total\n 31.4\n  4.922\n     implying that\n\\[\\begin{aligned} \\overline{Y} \u0026amp;= \\sqrt{\\dfrac{3.6 + 3.6 + 3.9 + 3.2 + 2.5 + 4.5 + 2.6 + 2.2 + 2.9 + 2.4}{10}}\\\\\\\\ \u0026amp;\\approx 3.14 \\end{aligned}\\] and\n\\[\\begin{aligned} S \u0026amp;= \\sqrt{\\dfrac{4.922}{9}}\\\\\\\\ \u0026amp;\\approx 0.74 \\end{aligned}\\] So between 1997-2017, the average divorce rate per 1000 people was about \\(3\\) with an approximated standard deviation of 0.74.\n Divorce rates may vary by state due to multiple factors which are not represented in the included data set such as variations in \u0026gt;span class=“boxed”\u0026gt;employment status, policies and laws by region (i.e. states with no-fault divorce laws), religious beliefs, etc.\n    8  DEGREE is an ordinal measure whereas AGEKDBRN is an interval measure.\n As DEGREE increases, AGEKDBRN increases as well implying the likelihood of a positive relationship. The youngest first-time parents are those with less than a high school degree, while the oldest first-time parents are those with graduate degrees with difference \\(28.59 – 21.33 = 7.26\\) years. The variability in age when first child was born is larger as educational attainment increases. The standard deviation for the high school degree group is largest at \\(5.498\\) years, the smallest is for the some college group with \\(4.581\\).\n   11  a. `Type of Work` is a nominal variable implying that the appropriate measure of variability as the index of qualitative variation (IQV). b. The pertage squared for both grades can be found by   Type of Work  Grade 8 Percentage $c$  Grade 8 Percentage Squared $c^2$  Grade 10 Percentage $c$  Grade 10 Percentage Squared $c^2$      Lawn work  28  $(28)^2 = 784$  20  $(20)^2 = 400$    Food service  3  $(3)^2 = 9$  10  $(10)^2 = 100$    Babysitting  37  $(37)^2 = 1369$  28  $(28)^2 = 784$    Other  32  $(32)^2 = 1024$  42  $(42)^2 = 1764$    Total  100  $3186$  100  $2048$     So the IQV for both grades can be found by  Grade 8: $\\dfrac{4\\cdot(100^2-3186)}{100^2\\cdot (4-1)} = \\dfrac{27256}{30000} = 0.91$   Grade 10: $\\dfrac{4\\cdot(100^2-3048)}{100^2\\cdot (4-1)} = \\dfrac{27808}{30000} = 0.93$  implying IQV(Grade 8) $=0.91$ and IQV(Grade 10) $=0.93$. c. Though both IQVs are greater than $0.90$, there is slightly more variation among 10th graders than 8th gradersin the type of jobs they hold. The difference may be attributed to more employment options for older students and limitations for younger studentsxxxx leading to more informal jobs such as lawn work and babysitting.  --  12  Using the summary statistics for life expectancy    Measure  Statistic for European Countries  Statistic for Non-european Countries      Median  81.500  82.70    Variance  7.985  11.65    Standard deviation  2.830  3.41    Minimum  75.300  76.30    Maximum  82.000  85.50    Range  6.700  9.20    Interquartile range  3.800  4.95      we find that the variable is interval ratio implying that variance (or standard deviation), range, or IQR are possible and valid. Among these three measures, variance and/or standard deviation is preferred for precision. For just the average life expectancy for these 10 countries, we should rely on the mean.\nOn average, non-European countries have a slightly higher life expectancy at birth (\\(82.00\\) vs. \\(80.30\\)). Both the mean and median are higher for non-European countries than for European countries. Also, the distribution of non-European countries exhibits more variability; the standard deviation for European countries is \\(3.41\\) years, while for non-European countries it is \\(2.83\\) years.\nThese differences might be explained by access and availability of health care and/or diet. However, the difference might simply be random due to the small number of countries presented in this example. We are likely to find different results if more countries were incorporated into the analyses.\n  13  Overall, Clinton voters were younger, more educated, and attended religious services less than Trump voters. The youngest voters were male Clinton voters at \\(50.49\\) years (\\(s = 18.84\\)), followed by female Clinton voters, \\(51.93\\) years (\\(s = 18.21\\)). For education, males who voted for Clinton had the highest mean of \\(14.69\\) (\\(s = 2.73\\)). Males who voted for Trump had \\(13.80\\) years of education (\\(s = 2.75\\)). Trump voters, both males and females, attended religious services more often than Clinton voters. Mean scores were \\(3.72\\) for males (\\(s = 2.93\\)) and \\(3.82\\) for females (\\(s = 2.75\\)), indicating church attendance about several times a year to once a month. The standard deviations indicate a consistency in the distributions of education, age, and religious service attendance across all four groups. The largest standard deviations are for age, ranging from \\(15.94\\) to \\(18.84\\) years. These wide standard deviations indicate more dispersion around the mean age scores.\n :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d78716373c973d0053a3723106f8b81","permalink":"/assignment/04-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/04-homeworks/","section":"assignment","summary":"Week 4 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 4 Problem Set Assignment Chapter 4 Exercises: 1, 2, 3, 7, 8, 12, 13.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 4 after class ends What to do for Week 5 before coming to class   What to do for Week 4 after class ends1   eCampus?  Description  Location         Complete a module in Data Camp focused on basic descriptive statistics and an introduction to probability.  Link       Complete problems 1, 2, 3, 7, 8, 12, and 13 in Chapter 4.  Link     \n What to do for Week 5 before coming to class2   Area  Description  Location         Read chapter 5 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch a video on The Normal Distribution.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4bca9f280d6851de4bfe166681764e5c","permalink":"/due/04-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/04-due/","section":"due","summary":"What to do for Week 4 after class ends What to do for Week 5 before coming to class   What to do for Week 4 after class ends1   eCampus?","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Spread. You may be on point if you think spread is referring to the dispersion of data.\n \nIf you are feeling up to it and want to know more about how data spread and Distributions relate to each other, take a look at this one too.\n \nRemember you can always access a Crash Course Statistics playlist using the following link.\n  --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"/example/04-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics. This time it is with a broad concept called Measures of Spread.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough - Doing the Class Notes   Data Camp The fourth Data Camp module is due the Wednesday following the completion of the Week 4 class. The module covers doing basic descriptive statistics in R out of the box1. In particular, the sections are:\n Variables Histograms and Distributions Scales of Measurement Measures of Central Tendency Measures of Variability  As promised, we start performing basic statistical analyses in R.\n Book Materials Download the textbook\n PowerPoint and Lecture Notes.   Class Notes Posted after class\n PDF and Outline.   R Walkthrough - Doing the Class Notes Posted after class\n You can download a [PDF](/slides/Week 4/Slides-Week-4.pdf){target=\"_blank\"} of the presentation above. --   Mostly without any additional packages or what is known as Base R↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a003ab3247913ac8f1034d05b4692ee","permalink":"/lesson/04-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/04-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough - Doing the Class Notes   Data Camp The fourth Data Camp module is due the Wednesday following the completion of the Week 4 class.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 4: Measures of Variability1 in Frankfort-Nachmias and Leon-Guerrero.\n  Remember to please take this sections slow and to pay particular attention to the ideas of standard deviation and variance. These are essentially “gateway” concepts which is to say your ability to understand the remaining concepts in the course hinges on really knowing what both of these are, what they measure, and how they are found. No pressure!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8dd4002cb4ac647f78e11eaaa49febed","permalink":"/readings/04-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/04-readings/","section":"readings","summary":"Book Stuff   Book Stuff Read Chapter 4: Measures of Variability1 in Frankfort-Nachmias and Leon-Guerrero.\n  Remember to please take this sections slow and to pay particular attention to the ideas of standard deviation and variance.","tags":null,"title":"Measures of Variability","type":"docs"},{"authors":null,"categories":null,"content":"    An Introduction to dplyr  Purpose  Objectives Packages  The Tidyverse Package Tidy Data Basics The Pipe %\u0026gt;% Operator  Gapminder mutate() adds new variables Try These Out group_by() operates on groups summarize() with group_by() arrange() orders columns  NFL Data Set Task Logical Operators Data Science Acknowledgements Don’t Get Bogged Down!    An Introduction to dplyr Purpose What are some common things you like to do with your data? Maybe remove rows or columns, do calculations and maybe add new columns? This is called data wrangling. It’s not data management or data manipulation: you keep the raw data raw and do these things programatically in R with the tidyverse.\nYou are going to be introduced to data wrangling in R without using the base package, or “Base R.” The tidyverse is a suite of packages that match a philosophy of data science developed by Hadley Wickham and the RStudio team. It is a more straight-forward way to learn R. I encourage you to take a look around the tidyverse web page just to see everything it can do. You can fine it here: https://www.tidyverse.org/\nNow that you have had two weeks of utter frustration with “Base R”, which means, in R without using any additional packages (though we have used a bit of tidyverse), I will show you by comparison what code will look like in “Base R”. For some things, base-R is more straightforward and where tat is apparent, I will note it. Whenever we use a function that is from the tidyverse, we will prefix it so you’ll know for sure.\nObjectives  discuss tidying data read data from a csv file into R explore gapminder data with base-R functions wrangle gapminder data with dplyr from the tidyverse family of functions   Packages Please load up the following packages\nlibrary(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.2 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::group_rows() masks kableExtra::group_rows() ## x dplyr::lag() masks stats::lag() library(gapminder) Remember to download them if you receive an error:\ninstall.packages(\u0026quot;tidyverse\u0026quot;) install.packages(\u0026quot;gapminder\u0026quot;)   The Tidyverse Package The tidyverse package is actually a family of packages that have been constructed to take all kinds of data sets in multiple formats and to tidy them. Before we get into that concept, the typical path that we take when working with real world data sets is relatively simple…\n…yet the process can be extremely complex and time consuming as described below:\n Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information (NYTimes, 2014).\n tidyverse provides packages - given in green - that address all of these notions, but we won’t even be able to scratch the surface of most of them.\nIn this walk through, we’ll be concentrating on the use of pipes using the dplyr.\n  Tidy Data Let’s start off discussing tidy data which has simple convention: put variables in the columns and observations in the rows - amazing right?\nThere are three interrelated rules which make a data set tidy:\nEach variable must have its own column. Each observation must have its own row. Each value must have its own cell.  We are going to wrangle - yup that is a real term for messing with the structure of data - a tidy-ish data set (the Mutate part of the cycle), and then come back to tidying messy data using tidyr once we’ve gotten it into proper form.\nConceptually, making data tidy first is really critical. Instead of building your analyses around whatever format your data are in, we’ll take deliberate steps to make your data tidy. When your data are in this format, you can use a growing assortment of powerful analytical and visualization tools instead of inventing home-grown ways to accommodate your data. This will save you time since you aren’t reinventing the wheel, and will make your work more clear and understandable to your collaborators. Additionally after struggling with Base R and its arduous process, pipes will probably be something you welcome!\n Basics There are six functions in dplyr that you will primarily use to wrangle data. Remember that variables are the column names while observations are the values within a column.\n the filter() command which let’s you pick observations by their values.    the select() command which let’s you pick variables by their names.\n   the mutate() command which let’s you create new variables with functions of existing variables.   the summarise() command which let’s you collapse multiple values to a single summary.    the group_by() command which let’s you perform operations with respect to a variable.   the arrange() command which let’s you reorder the rows of a data frame.  (nope there isn’t a picture for this) Should you memorize these? NO! NEVER! DON’T DO IT! That’s what the internet is for! As with any data set, the main objective is to logically think about what you can do to achieve a goal. The commands are simply paths you could take to get there.\n The Pipe %\u0026gt;% Operator Pipes are a logical operator from the magrittr package that allows you to pass logic down a chain. Find that confusing? Let’s try to explain it another way:\nI %\u0026gt;% woke up %\u0026gt;% showered %\u0026gt;% got dressed %\u0026gt;% ate breakfast %\u0026gt;% showed up for work Here every act is dependent on all of the previous acts. This essentially signifies what pipes do, in that you can fit multiple commands in a row without having to do them one by one as in Base R. Pipes are given by the %\u0026gt;% symbol. In RStudio, the keyboard shortcut for a pipe is\n Cmd + Shift + M (Mac or Linux)\n Ctrl + Shift + M (Windows)\n  Gapminder In this walk through, we’ll be using Gapminder data, which represents the health and wealth of nations. It was pioneered by Hans Rosling, who is famous for describing the prosperity of nations over time through famines, wars and other historic events with an interactive data visualization in his 2006 TED Talk: The best stats you’ve ever seen which you can access by selecting the image on the next page.\n\nLet’s take a look at the gapminder data set.\nhead(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. Now using pipes, we could have done\ngapminder %\u0026gt;% head() ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. Here are some others using the filter and select commands with multiple steps:\n No Pipes gapusa_filter \u0026lt;- filter(gapminder, country == \u0026quot;United States\u0026quot;) gapusa_filter ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 United States Americas 1952 68.4 157553000 13990. ## 2 United States Americas 1957 69.5 171984000 14847. ## 3 United States Americas 1962 70.2 186538000 16173. ## 4 United States Americas 1967 70.8 198712000 19530. ## 5 United States Americas 1972 71.3 209896000 21806. ## 6 United States Americas 1977 73.4 220239000 24073. ## 7 United States Americas 1982 74.6 232187835 25010. ## 8 United States Americas 1987 75.0 242803533 29884. ## 9 United States Americas 1992 76.1 256894189 32004. ## 10 United States Americas 1997 76.8 272911760 35767. ## 11 United States Americas 2002 77.3 287675526 39097. ## 12 United States Americas 2007 78.2 301139947 42952. gapusa_select \u0026lt;- select(gapusa_filter, -continent, -lifeExp) gapusa_select ## # A tibble: 12 x 4 ## country year pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 United States 1952 157553000 13990. ## 2 United States 1957 171984000 14847. ## 3 United States 1962 186538000 16173. ## 4 United States 1967 198712000 19530. ## 5 United States 1972 209896000 21806. ## 6 United States 1977 220239000 24073. ## 7 United States 1982 232187835 25010. ## 8 United States 1987 242803533 29884. ## 9 United States 1992 256894189 32004. ## 10 United States 1997 272911760 35767. ## 11 United States 2002 287675526 39097. ## 12 United States 2007 301139947 42952.  With Pipes gapusa \u0026lt;- gapminder %\u0026gt;% filter(country == \u0026quot;United States\u0026quot;) %\u0026gt;% select(-continent, -lifeExp) gapusa ## # A tibble: 12 x 4 ## country year pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 United States 1952 157553000 13990. ## 2 United States 1957 171984000 14847. ## 3 United States 1962 186538000 16173. ## 4 United States 1967 198712000 19530. ## 5 United States 1972 209896000 21806. ## 6 United States 1977 220239000 24073. ## 7 United States 1982 232187835 25010. ## 8 United States 1987 242803533 29884. ## 9 United States 1992 256894189 32004. ## 10 United States 1997 272911760 35767. ## 11 United States 2002 287675526 39097. ## 12 United States 2007 301139947 42952. By using multiple lines you can actually read this like a story and there aren’t temporary variables that get confusing. This reads like:\n “start with the gapminder data, and then filter for the United States, and lastly drop the variables continent and lifeExp.”\n   mutate() adds new variables Let’s say we needed to add an index column so we know which order these data came in. Let’s not make a new variable, let’s add a column to our gapminder data frame. How do we do that? With the mutate() function.\nImagine we want to know each country’s annual GDP. We can multiply pop by gdpPercap to create a new column named gdp.\ngapminder %\u0026gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdp ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows  Try These Out On Your Own 1 Try to figure this out by yourself using pipes and only the filter, mutate, and select commands. Compare your syntax with the syntax below. Remember! There are multiple ways to go about finding the outcome.\n Calculate the population in thousands for all European countries in the year 2007 and add it as a new column.   Possible solution\ngapminder %\u0026gt;% filter(continent == \u0026quot;Europe\u0026quot;, year == 2007) %\u0026gt;% mutate(pop_thousands = pop/1000) %\u0026gt;% select(country, year, pop_thousands) #this cleans up the dataframe but isn\u0026#39;t necessary ## # A tibble: 30 x 3 ## country year pop_thousands ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania 2007 3601. ## 2 Austria 2007 8200. ## 3 Belgium 2007 10392. ## 4 Bosnia and Herzegovina 2007 4552. ## 5 Bulgaria 2007 7323. ## 6 Croatia 2007 4493. ## 7 Czech Republic 2007 10229. ## 8 Denmark 2007 5468. ## 9 Finland 2007 5238. ## 10 France 2007 61084. ## # … with 20 more rows  \nIf you got it, that’s great! However, it’s absolutely fine if you did not. The best way to learn is to practice (and possibly yell at your computer a few times if that helps).\n  group_by() operates on groups What if we wanted to know the total population on each continent in 2002? Answering this question requires a grouping variable. By using group_by() we can set our grouping variable to continent and create a new column called cont_pop that will add up all country populations by their associated continents.\ngapminder %\u0026gt;% filter(year == 2002) %\u0026gt;% group_by(continent) %\u0026gt;% mutate(cont_pop = sum(pop)) ## # A tibble: 142 x 7 ## # Groups: continent [5] ## country continent year lifeExp pop gdpPercap cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 2002 42.1 25268405 727. 3601802203 ## 2 Albania Europe 2002 75.7 3508512 4604. 578223869 ## 3 Algeria Africa 2002 71.0 31287142 5288. 833723916 ## 4 Angola Africa 2002 41.0 10866106 2773. 833723916 ## 5 Argentina Americas 2002 74.3 38331121 8798. 849772762 ## 6 Australia Oceania 2002 80.4 19546792 30688. 23454829 ## 7 Austria Europe 2002 79.0 8148312 32418. 578223869 ## 8 Bahrain Asia 2002 74.8 656397 23404. 3601802203 ## 9 Bangladesh Asia 2002 62.0 135656790 1136. 3601802203 ## 10 Belgium Europe 2002 78.3 10311970 30486. 578223869 ## # … with 132 more rows Sure this great but what if we don’t care about the other columns and only want each continent and their population in 2002? That leads us to the next function:\n summarize() with group_by() We want to operate on a group, but actually collapse or distill the output from that group. The summarize() function will do that for us.\ngapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarize(cont_pop = sum(pop)) %\u0026gt;% ungroup() ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## continent cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 6187585961 ## 2 Americas 7351438499 ## 3 Asia 30507333901 ## 4 Europe 6181115304 ## 5 Oceania 212992136 summarize() will actually only keep the columns that are grouped_by or summarized. So if we wanted to keep other columns, we’d have to do have a few more steps. ungroup() removes the grouping and it’s good to get in the habit of using it after a group_by() because R remembers! We can use more than one grouping variable. Let’s get total populations by continent and year.\ngapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarize(cont_pop = sum(as.numeric(pop))) %\u0026gt;% ungroup() ## `summarise()` regrouping output by \u0026#39;continent\u0026#39; (override with `.groups` argument) ## # A tibble: 60 x 3 ## continent year cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 1952 237640501 ## 2 Africa 1957 264837738 ## 3 Africa 1962 296516865 ## 4 Africa 1967 335289489 ## 5 Africa 1972 379879541 ## 6 Africa 1977 433061021 ## 7 Africa 1982 499348587 ## 8 Africa 1987 574834110 ## 9 Africa 1992 659081517 ## 10 Africa 1997 743832984 ## # … with 50 more rows  arrange() orders columns This is ordered alphabetically, which is helpful in certain circumstances. But let’s say we wanted to order it in ascending order for year. The dplyr function to do that is arrange().\ngapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarize(cont_pop = sum(as.numeric(pop))) %\u0026gt;% arrange(year) %\u0026gt;% ungroup() ## `summarise()` regrouping output by \u0026#39;continent\u0026#39; (override with `.groups` argument) ## # A tibble: 60 x 3 ## continent year cont_pop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 1952 237640501 ## 2 Americas 1952 345152446 ## 3 Asia 1952 1395357351 ## 4 Europe 1952 418120846 ## 5 Oceania 1952 10686006 ## 6 Africa 1957 264837738 ## 7 Americas 1957 386953916 ## 8 Asia 1957 1562780599 ## 9 Europe 1957 437890351 ## 10 Oceania 1957 11941976 ## # … with 50 more rows On Your Own 2 Try to figure this out by yourself using pipes and some if not all of the commands introduced above. Compare your syntax with the script On Your Own (Pipes and the NFL) Syntax Set.R. Remember! There are multiple ways to go about finding the outcome.\n Now what is the maximum GDP per continent across all years?   Possible solution\ngapminder %\u0026gt;% mutate(gdp = pop * gdpPercap) %\u0026gt;% group_by(continent) %\u0026gt;% mutate(max_gdp = max(gdp)) %\u0026gt;% filter(gdp == max_gdp) %\u0026gt;% select(-country) ## # A tibble: 5 x 7 ## # Groups: continent [5] ## continent year lifeExp pop gdpPercap gdp max_gdp ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Oceania 2007 81.2 20434176 34435. 7.04e11 7.04e11 ## 2 Asia 2007 73.0 1318683096 4959. 6.54e12 6.54e12 ## 3 Africa 2007 71.3 80264543 5581. 4.48e11 4.48e11 ## 4 Europe 2007 79.4 82400996 32170. 2.65e12 2.65e12 ## 5 Americas 2007 78.2 301139947 42952. 1.29e13 1.29e13     NFL Data Set Grab the data set1: 2014-average-ticket-price\n Save it in the same place as your script.\n Load it up in R2:\n  nfl2014 \u0026lt;- read_csv(\u0026quot;2014-average-ticket-price.csv\u0026quot;) ## ## ── Column specification ───────────────────────────────────────────────────────────────────────────── ## cols( ## Event = col_character(), ## Division = col_character(), ## `Avg TP, $` = col_double() ## ) To make a good habit, let’s check the file’s characteristics. This is a good habit and can alleviate some pains down the line. It is recommended you run these simple checks on any data set.\nCheck that your set is in a data frame format and get information about your columns and their types (character, numeric, or factor) by running  str(nfl2014) ## tibble [108 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Event : chr [1:108] \u0026quot;Baltimore Ravens at Pittsburgh Steelers Tickets on 02-Nov-2014 (9037819)\u0026quot; \u0026quot;Pittsburgh Steelers at Baltimore Ravens Tickets on 11-Sep-2014 (9037835)\u0026quot; \u0026quot;Cleveland Browns at Pittsburgh Steelers Tickets on 07-Sep-2014 (9037806)\u0026quot; \u0026quot;Cincinnati Bengals at Pittsburgh Steelers Tickets on 28-Dec-2014 (9037828)\u0026quot; ... ## $ Division : chr [1:108] \u0026quot;AFC North\u0026quot; \u0026quot;AFC North\u0026quot; \u0026quot;AFC North\u0026quot; \u0026quot;AFC North\u0026quot; ... ## $ Avg TP, $: num [1:108] 202 199 196 164 148 137 135 102 89 83 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. Event = col_character(), ## .. Division = col_character(), ## .. `Avg TP, $` = col_double() ## .. ) or you can do\nglimpse(nfl2014) ## Rows: 108 ## Columns: 3 ## $ Event \u0026lt;chr\u0026gt; \u0026quot;Baltimore Ravens at Pittsburgh Steelers Tickets on 02-No… ## $ Division \u0026lt;chr\u0026gt; \u0026quot;AFC North\u0026quot;, \u0026quot;AFC North\u0026quot;, \u0026quot;AFC North\u0026quot;, \u0026quot;AFC North\u0026quot;, \u0026quot;AFC … ## $ `Avg TP, $` \u0026lt;dbl\u0026gt; 202, 199, 196, 164, 148, 137, 135, 102, 89, 83, 83, 81, 2… So we do have a data frame with 108 rows and three columns as well as two columns that are made up of characters (i.e. letters) and one that is numeric, or made of numbers. These columns are actually called vectors. Please keep this mind as we’ll be referring to the columns as vectors from now on.\nCheck that its a tibble, which is essentially a data frame but tweaked to make it easier to wrangle.  is_tibble(nfl2014) The differences between the two are worth a brief discussion in a data science course, but not here. Some older functions don’t work with tibbles. If you encounter one of these functions, use the command as.data.frame() to turn a tibble back to a data.frame like so:\nnfl2014_dfonly \u0026lt;- as.data.frame(nfl2014) Anyway, if you test for both of those and they pass, you will be able to use all of the commands listed above.\nWe’ll tweak the second question on the task list a bit to say: “What was the highest and lowest average ticket price for all 2014-2015 NFL games by division?” For this session, we’ll disregard the histogram and answer the question that was asked for on the original.\nTo tackle this, let’s think about it logically. Originally you had to\ninspect the data frame which I called nfl2014,\n remove the columns with NA,\n create eight different data frames, one for each division, and\n derive the mean for each.\n put the values in order.\n  That is a lot of steps! Let’s try using pipes to do this instead:\nnfl2014 %\u0026gt;% select(Division, `Avg TP, $`) %\u0026gt;% group_by(Division) %\u0026gt;% na.omit() %\u0026gt;% summarize(Mean_by_Division = mean(`Avg TP, $`, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(Mean_by_Division)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 2 ## Division Mean_by_Division ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 NFC North 179. ## 2 NFC East 173. ## 3 NFC West 165. ## 4 AFC North 135. ## 5 AFC East 127. ## 6 AFC West 125. ## 7 NFC South 95.4 ## 8 AFC South 83.3 Well that was MUCH EASIER! Now what happened? Let’s go through it line by line:\nI called the data frame nfl2014. This was the name given to the csv file when it was brought in.\n I selected the two columns I want to know more about. In this case, I can’t compute anything by Division if I don’t have the numerical data that explains ticket prices Avg TP, $.\n Since I need the mean by each Division, grouping by that vector will allow me to find perform any and all operations by each unique value. In this case we have eight divisions, so well have outcomes for each.\n NAs are a special type of value in R. New users tend to disregard them as blanks or sometimes zeros. However, the name itself NA means not available aka a missing value. When you see this, it means that R has recognized that a value should be there but its not and depending on the command, it may bring up an error or worst case scenario, an analysis runs and your output is skewed. The command na.omit() will remove any row where an NA is found. Please note this differs from removing only rows with NAs. The distinction, while minute, can have lasting ramifications.\n summarize tells R that I’m going to reduce the data set using some summary technique. In this case the mean is needed so I constructed a new column named Mean_by_Division which took the mean of `Avg TP, $. Note that since we are using pipes, we do not have to tell R about the name of the data frame. It has been understood since step 1.\n As a good rule of thumb, it is always good to ungroup() your data because the grouping is information that R retains. R remembers!\n Arrange the Mean_by_Division vector in decreasing order. The default is increasing.\n  On Your Own 3  Now try doing the same as the above except only for AFC teams.  \nThere are a lot of ways to accomplish this, but I would suggest filtering with a string. Take a look at the link and see if you can figure out the command to use.\nFilter with Text data\nOf course, a solution is below. Again you are learning how to do this so take some time and for many of you, it may be oddly satisfying to find the output you want after being frustrated.  Possible solution\nnfl2014 %\u0026gt;% select(Division, `Avg TP, $`) %\u0026gt;% filter(str_detect(Division, \u0026quot;AFC\u0026quot;)) %\u0026gt;% na.omit() %\u0026gt;% summarize(AFC_mean = mean(`Avg TP, $`, na.rm = TRUE)) ## # A tibble: 1 x 1 ## AFC_mean ## \u0026lt;dbl\u0026gt; ## 1 117.    Task For this task first download the drinks2010andBeyond data set3. This is an amended version of the average serving sizes per person by country as reported by the World Health Organization (WHO), Global Information System on Alcohol and Health (GISAH) since 2010. If you are interested, please visit the WHO GISAH site.\nIn a typical submission4, you must include\nYour name and task in the top of the script.\n Separated solutions in proper numerical order.\n Your code.\n An answer to the question.\n  Remember you can leave text that R will ignore by putting a hashtag # in front of it.\nAs an example, if I were to submit the altered item 2 from above, my script would read:\n# Abhik Roy # EDP 613 # Drinks Task # 1 blah blah blah # 2 nfl2014 %\u0026gt;% select(Division, `Avg TP, $`) %\u0026gt;% group_by(Division) %\u0026gt;% na.omit() %\u0026gt;% summarize(Mean_by_Division = mean(`Avg TP, $`, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% arrange(desc(Mean_by_Division)) # According to the output, the highest average price paid was $170.00 per ticket paid by # fans the NFC North whereas the lowest average price ticket was paid by attendees who # went to games in the AFC South at $83.30. # 3 blah blah blah Now please answer the following questions. Please note that all consumption measurements are in liters:\n First step  We have to bring in the data set:\ndrinks \u0026lt;- read_csv(\u0026quot;drinks2010andBeyond.csv\u0026quot;) ## ## ── Column specification ───────────────────────────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## `Beverage Types` = col_character(), ## `2016` = col_double(), ## `2015` = col_double(), ## `2014` = col_double(), ## `2013` = col_double(), ## `2012` = col_double(), ## `2011` = col_double(), ## `2010` = col_double() ## )  \nOK now we can move on with the questions! Which country or countries had the highest average consumption of wine and beer in each year?   Possible solution  drinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Beer\u0026quot; | `Beverage Types` == \u0026quot;Wine\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% group_by(year, Country) %\u0026gt;% summarize(avg = mean(measure)) %\u0026gt;% top_n(n=1, avg) ## `summarise()` regrouping output by \u0026#39;year\u0026#39; (override with `.groups` argument) ## # A tibble: 7 x 3 ## # Groups: year [7] ## year Country avg ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Portugal 5.42 ## 2 2011 Croatia 5.40 ## 3 2012 Equatorial Guinea 5.82 ## 4 2013 Equatorial Guinea 5.08 ## 5 2014 Austria 5.25 ## 6 2015 Slovenia 5.32 ## 7 2016 Czechia 4.84 Breakdown line-by-line:\nWe call the data frame drinks.\n We filter the column Beverage Types by “Beer” OR “Wine”.5\n The gather() command is used to go from a wide data set to a long one while ignoring the application of the command on the columns Country, and Beverage Types. To see what occurs, first run the first two lines by highlighting them and ending before the %\u0026gt;% (pipe) on the second line:\n  That simply runs through steps 1 and 2 above. Now run the first three lines in the same way ending prior to the %\u0026gt;% (pipe) in the third line\nYou can find a multitude of resources for gather() and its complement spread() but my advice is simply to play around with the key, value, and columns you don’t want to make long. However if you like some reference material while you explore these commands, try Data Science for R. If the link is not work, the address is https://garrettgman.github.io/tidying/.\nIn our case all of the data is now in relation to the columns Country and Beverage Types which gives us the remaining columns (the key) as data points that have values (the value) which I have named year (via key = \"year\") and measure (via value = \"measure\"). Those that do not have associated values become NAs (for example the country of Afghanistan did not have any values for Beer or Wine in 2016). Please take some time to investigate the gather() command as R works will with long data sets and not so well with wide ones!\nna.omit() is a command you have seen before and it simply sees if there is an NA in a row and then deleted said row. It performs this recursively throughout your entire data frame.\n The group_by() command notifies R that all operations must be performed with respect to the columns indicated which in this case implies that  the summarize() command used here to find the mean of the column measure is performed by year and Country. We assign the results to a new column avg.\n Finally the top_n() which gives us the top value of the average avg column.\n  Again to see how this layering works, I suggest that you run this line by line to observe the results. This is the benefit of the tidyverse approach.\n \nWhich country or countries had the highest average consumption of spirits in each year?   Possible solution  The process here is exactly the same as the one described in item 1 except the Beverage Types are \"Spirits\" rather than \"Beer\" or \"Wine\".\ndrinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Spirits\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% group_by(year, Country) %\u0026gt;% summarize(avg = mean(measure)) %\u0026gt;% top_n(n=1, avg) ## `summarise()` regrouping output by \u0026#39;year\u0026#39; (override with `.groups` argument) ## # A tibble: 7 x 3 ## # Groups: year [7] ## year Country avg ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Estonia 7.53 ## 2 2011 Estonia 8.18 ## 3 2012 Estonia 8.53 ## 4 2013 Estonia 8.93 ## 5 2014 Estonia 8.7 ## 6 2015 Estonia 8.37 ## 7 2016 Estonia 7.72  \nWhich country had the highest average consumption of all alcohol across time?   Possible solution  Again, the process here is exactly the same as the one described in items 1 and 2 except the Beverage Types are \"All types\" rather than any individual type.\ndrinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;All types\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% group_by(year, Country) %\u0026gt;% summarize(avg = mean(measure)) %\u0026gt;% top_n(n=1, avg) ## `summarise()` regrouping output by \u0026#39;year\u0026#39; (override with `.groups` argument) ## # A tibble: 7 x 3 ## # Groups: year [7] ## year Country avg ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Estonia 15.0 ## 2 2011 Estonia 16.3 ## 3 2012 Estonia 17.0 ## 4 2013 Estonia 17.8 ## 5 2014 Estonia 17.3 ## 6 2015 Estonia 16.6 ## 7 2016 Estonia 15.4  \nBased on each years average consumption of beer across all counties, how far does the United States deviate from the mean, if at all? Is the mean an appropriate measure of the average here? Why or why not?\nThis one was fairly difficult. A deviation from the mean essentially indicates how a single measure differs from an established value of a mean. In our case, how the mean of the United States differs from that of the mean derived from the rest of the countries. Your first thought may be to find the difference between both the means by subtracting them. This would be fine if both had the same standard deviation, but they do not.   Possible solution  # USA only drinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Beer\u0026quot; \u0026amp; Country == \u0026quot;United States of America\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% summarize(avg = mean(measure), stanDev = sd(measure))  ## # A tibble: 1 x 2 ## avg stanDev ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4.22 0.0699 # Remaining countries drinks %\u0026gt;% filter(`Beverage Types` == \u0026quot;Beer\u0026quot; \u0026amp; Country != \u0026quot;United States of America\u0026quot;) %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% summarize(avg = mean(measure), stanDev = sd(measure))  ## # A tibble: 1 x 2 ## avg stanDev ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2.06 1.80 Note that our filter() command again uses an AND command rather than an OR and rather than filtering using one column, we use two (Beverage Types AND Country) but in the latter we we want to exclude the United States of America by using Country != \"United States of America\". So interpreting this, we can say that we’re looking for figures for all Beer consumption around the world except for the United States of America.\nSo what can we do? At this point not much! In the future we should be able to address this issue by using a statistical test known as a t-test but we need to satisfy some initial prerequisites first. We will explore this in R5 but for now, we’ll leave it here.\nOn a side note, it is absolutely fine if you subtracted the means! That would be the most logical choice and you also did not have much of a choice at the current time. Consider that the issue of subtracting means with differing standard deviations is that the spread differs between each and while a mean is a mean, they’re not measures of the mean on the same footing - ergo all things are not equal.  \nSince we measure beer in the United States in ounces (for some reason) rather than in liters, convert the 2016 measures into ounces using the conversion 1 Liter = 33.814022 oz.   Possible solution  One of the nicer aspects of using %\u0026gt;% is in the fact you can selectively perform operations and general augmentations on the columns of your choice without ever having to worry about how any other columns are affected. This is actually accomplished using the command mutate() which in a nutshell allows for column wise operations.\nIn this task, we are informed that the column 2016 is given in liters. However since we and a handful of other countries have chosen to use a measurement that are nowadays based off the metric system (yes that’s true..for example an inch is officially defined as 2.54 centimeters) and not the metric system itself (base 10 seems pretty easy but what do I know), we have to convert on order to report. In this case we are given the easy to remember 1 Liter = 33.814022 (fluid) ounces. To convert this, we use the mutate() command by drinks %\u0026gt;% mutate(`2016` = 33.814022 * `2016`) %\u0026gt;% rename(`2016 (using garbage measurements)` = `2016`) ## # A tibble: 954 x 9 ## Country `Beverage Types` `2016 (using ga… `2015` `2014` `2013` `2012` `2011` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghan… All types NA 0.02 0.03 0.03 0.04 0.04 ## 2 Afghan… Beer NA 0.01 0.01 0.01 0.01 0.01 ## 3 Afghan… Wine NA 0 0 0 0 0 ## 4 Afghan… Spirits NA 0.02 0.02 0.02 0.03 0.03 ## 5 Afghan… Other alcoholic… NA 0 0 0 0 0 ## 6 Albania All types 171. 4.77 4.81 5.06 5.43 5.65 ## 7 Albania Beer 58.8 1.57 1.58 1.82 1.81 1.88 ## 8 Albania Wine 45.3 1.17 1.12 1.06 1.3 1.27 ## 9 Albania Spirits 64.2 1.94 2.02 2.09 2.22 2.4 ## 10 Albania Other alcoholic… 3.04 0.08 0.09 0.09 0.1 0.1 ## # … with 944 more rows, and 1 more variable: `2010` \u0026lt;dbl\u0026gt; with the last line being dedicated to renaming the column that was just converted from 2016 to 2016 (using garbage measurements). Notice that the rename() command requires that the new name of a column precede the current name separated by and =. I realize this appears to be counterintuative and there is a reason for it, but unless you love discussing how R compiles code, I’ll leave it be.\nOn a side note, we only converted the column 2016. What if we wanted to convert every column? Well we’d be using a lot of %\u0026gt;% and wasted energy. This is one of many instances where gather() is a great command to know. Switching from a wide data frame to a long one allows you to convert all years at once. We can do this by\ndrinks %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% mutate(`measure` = 33.814022 * `measure`) %\u0026gt;% rename(`Measures (in ounces because that\u0026#39;s apparently easy)` = `measure`) ## # A tibble: 6,016 x 4 ## Country `Beverage Types` year `Measures (in ounces because that\u0026#39;s app… ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania All types 2016 171. ## 2 Albania Beer 2016 58.8 ## 3 Albania Wine 2016 45.3 ## 4 Albania Spirits 2016 64.2 ## 5 Albania Other alcoholic bever… 2016 3.04 ## 6 Algeria All types 2016 18.9 ## 7 Algeria Beer 2016 10.8 ## 8 Algeria Wine 2016 4.73 ## 9 Algeria Spirits 2016 3.38 ## 10 Algeria Other alcoholic bever… 2016 0 ## # … with 6,006 more rows Unlike the piping we used earlier, we aren’t filtering for anything (though we could if we wanted by putting the filters back in). The rest is pretty much a combination of previous column-wise wrangling. Try running the first three lines. You will once again notice that the measure column is the associated liter count by year. With that, we mainly need to use mutate() on the measure column to convert it.\nLearning both gather() and to some degree spread() which is not covered in this walk-through will save you a great deal of time. It is worth taking the time now to explore what you can do with it.\n Explore the data set a bit and play around with pipes. Provide one outcome you derived not already asked about that you (hopefully) found interesting.   Possible solution  Of course answers will vary here but we can see the values for each country by year\ncoalesce_by_column \u0026lt;- function(df) { return(dplyr::coalesce(!!! as.list(df))) } drinks %\u0026gt;% gather(key = \u0026quot;year\u0026quot;, value = \u0026quot;measure\u0026quot;, -Country, -`Beverage Types`) %\u0026gt;% na.omit() %\u0026gt;% rowid_to_column(\u0026quot;Country ID\u0026quot;) %\u0026gt;% spread(`Beverage Types`, value = `measure`) %\u0026gt;% select(-`Country ID`) %\u0026gt;% group_by(Country, year) %\u0026gt;% summarise_all(coalesce_by_column) %\u0026gt;% na_if(0) ## # A tibble: 1,202 x 7 ## # Groups: Country [190] ## Country year `All types` Beer `Other alcoholic beverages` Spirits Wine ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan 2010 0.03 0.01 NA 0.02 NA ## 2 Afghanistan 2011 0.04 0.01 NA 0.03 NA ## 3 Afghanistan 2012 0.04 0.01 NA 0.03 NA ## 4 Afghanistan 2013 0.03 0.01 NA 0.02 NA ## 5 Afghanistan 2014 0.03 0.01 NA 0.02 NA ## 6 Afghanistan 2015 0.02 0.01 NA 0.02 NA ## 7 Albania 2010 5.53 1.72 0.12 2.39 1.3 ## 8 Albania 2011 5.65 1.88 0.1 2.4 1.27 ## 9 Albania 2012 5.43 1.81 0.1 2.22 1.3 ## 10 Albania 2013 5.06 1.82 0.09 2.09 1.06 ## # … with 1,192 more rows where NA represents values not reported.\nOn a side note, there is an easier way to run spread() and it is called pivot_wider().\n  Logical Operators    R syntax  Idea  Logic      !  NOT  !a    \u0026amp;  vector-based AND (value-wise)  a \u0026amp; b    \u0026amp;\u0026amp;  value-based AND (single value)  a \u0026amp;\u0026amp; b    |  vector-based inclusive OR (value-wise)  a | b    ||  value-based inclusive OR (single value)  a || b    xor  vector-based exclusive OR (value-wise)  xor(a,b)    \u0026lt;  LESS than  a \u0026lt; b    \u0026gt;  GREATER than  a \u0026gt; b    ==  exactly EQUALS  a == b    \u0026lt;=  LESS than or EQUAL to  a \u0026lt;= b    \u0026gt;=  GREATER than or EQUAL to  a \u0026gt;= b    !=  NOT EQUAL to  a != b    %in%  included in?  a %in% b    isTRUE  test if something is TRUE  isTRUE(a)      The use of %\u0026gt;% will get easier. Some people think of what steps they use when using a spreadsheet software (e.g. Microsoft Excel ©) when learning the logic of R. In fact, many of the formulas and syntax used in spreadsheet softwares parallel those in R. If you are used to Excel for example, try checking out R for Excel Users. If the link is broken for some reason, head over to https://www.rforexcelusers.com/. Please note that I do not receive any compensation in any form for recommending this site nor do I take responsibility for its content. For assistance regarding the site, please contact the authors\n Data Science Data science uses the statistics you will learn in this course and to some degree, in the follow-up course to tackle real life data. According to Glassdoor, the average salary for a data scientist is 113,436 USD (range: 95,000 USD - 250,000 USD; Burtch-Works, 2018). If you are interested in learning more about Data Science, a course is tentatively scheduled for the Fall 2021 term in addition to Data Visualization in Spring 2021. Both of these courses run very differently from the statistics courses and are focused on exploring data sets of interest to students. If interested, please let me know or stay tuned.\n Acknowledgements A great deal of material presented here was adapted from two online texts: Introduction to Open Data Science and R for Data Science. If interested in how R is used in data science, try taking a look.\n Don’t Get Bogged Down! Just get a feel for the items above. There is a whole bunch of commands and ridiculous syntax above but that’s not the point. Concentrate on knowing what to do (e.g. “Is this best represented by a bar plot?”) rather than how to do it (“The ggplot command is…”). You can ALWAYS search for the syntax associated with whatever in R but if you don’t how to conceptually solve a problem, software won’t help. Remember computers are stupid!\n   From the FiveThirtyEight NFL Ticket Prices Github Page↩︎\n If you are getting an error, remember to point R to the right location by Session \u0026gt; Working Directory \u0026gt; To Source File Location in the menu.↩︎\n From the FiveThirtyEight Alcohol Consumption Github Page↩︎\n This is just for practice so you do not have to turn anything in!↩︎\n Note that OR here means what is known as an inclusive or…aka it this or that or both. Using this idea, the second line search reads like Beverage Types for “Beer” OR “Wine” OR both. The other type of OR is called an exclusive or ..aka it this or that but NOT both. If you ever have to use it, it is given by the command xor(). Included on the last page of this document are some standard logical operators for your reference.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2368b9776cb091e0e9ab93cd309154c4","permalink":"/walkthroughs/nfl/nflpipes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/walkthroughs/nfl/nflpipes/","section":"walkthroughs","summary":"An Introduction to dplyr  Purpose  Objectives Packages  The Tidyverse Package Tidy Data Basics The Pipe %\u0026gt;% Operator  Gapminder mutate() adds new variables Try These Out group_by() operates on groups summarize() with group_by() arrange() orders columns  NFL Data Set Task Logical Operators Data Science Acknowledgements Don’t Get Bogged Down!","tags":null,"title":"Pipes and the NFL","type":"docs"},{"authors":null,"categories":null,"content":"   Week 5 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 5 Problem Set Assignment Chapter 5 Exercises: 1, 2, 5, 6, 7, 8, 11, 12. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions  1  We are looking the area under the curve greater than \\(8\\). The \\(z\\)-score for a person who watches more than 8 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{8-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx 1.68 \\end{aligned}\\] So \\(z \\approx 1.68\\).\n We are looking the area under the curve less than \\(5\\). The \\(z\\)-score for a person who watches less than 5 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{5-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx 0.68 \\end{aligned}\\] So \\(z \\approx 0.68\\). Since the area between \\(z=0.68\\) and \\(z=0\\) is \\(0.2517\\), we have \\(0.5000+0.2517=0.7517\\). This implies that \\(0.7517\\cdot 1014 \\approx\\) \\(762\\) or \\(763\\) people watch television less than 5 hours/day.\n We have\n\\[\\begin{aligned} Y\u0026amp;=2.97+1\\cdot 3\\\\ \u0026amp;= 5.97 \\end{aligned}\\] implying that 5.97 television hr/day corresponds to a \\(z\\)-score of \\(+1\\).\n We are looking at the area between \\(1\\) and \\(6\\). The \\(z\\)-score for a person who watches more than 1 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{1-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx -0.66 \\end{aligned}\\] with the area between the mean and \\(z=0.66\\) found to be \\(0.2454\\). The \\(z\\)-score for a person who watches less than 6 hr/day is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{6-2.97}{3.00}\\\\\\\\ \u0026amp;\\approx 1.01 \\end{aligned}\\] with the area between the mean and \\(z=1.01\\) found to be \\(0.3438\\).\nSo the percentage of people who watch between 1 hr/day and 6 hr/day of television can be found by \\(0.2454 + 0.3438 = 0.5892\\), or about \\(59\\%\\).\n    2  The 95th percentile, or \\(0.9500\\) is approximately equivalent to \\(z=1.65\\). So the number of women needing shelter is\n\\[\\begin{aligned} Y\u0026amp;= 250+1.65\\cdot 75\\\\ \u0026amp;= 373.75 \\end{aligned}\\] or about \\(374\\) women. Since this figure exceeds the total capacity of 350, there will not be enough space for all abused women on 95% of all nights. at minimum, the city needs \\(374\\) beds.\n We are looking the area under the curve greater than \\(220\\). The \\(z\\)-score for exceeding the capacity is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{220-250}{75}\\\\\\\\ \u0026amp;\\approx -0.40 \\end{aligned}\\] The area below this value is \\(0.3446\\), so the area exceeding this is \\(1 – 0.3446 = 0.6554\\), or about \\(66\\%\\) of all nights the number of women seeking shelter will exceed the capacity of 220.\n    5  We are looking at the area between \\(12\\) and \\(16\\) for the Working class. The \\(z\\)-score for a person with more than 12 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{12-13.05}{2.77}\\\\\\\\ \u0026amp;\\approx -0.38 \\end{aligned}\\] with the area between the mean and \\(z=-0.38\\) found to be \\(0.1480\\).\nThe \\(z\\)-score for a person who watches less than 16 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-13.05}{2.77}\\\\\\\\ \u0026amp;\\approx 1.06 \\end{aligned}\\] with the area between the mean and \\(z=1.06\\) found to be \\(0.3554\\).\nSo the proportion of working-class respondents with 12–16 years of education can be found by \\(0.1480 + 0.3554 =\\) \\(0.5034\\).\nWe are still looking at the area between \\(12\\) and \\(16\\) for the Upper class. The \\(z\\)-score for a person with more than 12 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{12-15.48}{2.76}\\\\\\\\ \u0026amp;\\approx -1.26 \\end{aligned}\\] with the area between the mean and \\(z=-1.26\\) found to be \\(0.3962\\).\nThe \\(z\\)-score for a person who watches less than 16 years of education is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-15.48}{2.67}\\\\\\\\ \u0026amp;\\approx 0.19 \\end{aligned}\\] with the area between the mean and \\(z=0.19\\) found to be \\(0.0753\\).\nSo the proportion of upper-class respondents with 12–16 years of education can be found by \\(0.3962 + 0.0753 =\\) \\(0.4715\\).\n We are looking the area under the curve greater than \\(16\\). For working-class respondents, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-12.05}{2.77}\\\\\\\\ \u0026amp;\\approx 1.06 \\end{aligned}\\] The area between \\(z=1.06\\) and the tail is \\(0.1446\\) thus implying the probability of a working-class respondent having more than 16 years of education is \\(14.46\\%\\).\nFor middle-class respondents, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-14.56}{2.95}\\\\\\\\ \u0026amp;\\approx 0.49 \\end{aligned}\\] The area between \\(z=0.49\\) and the tail is \\(0.3121\\) thus implying the probability of a middle-class respondent having more than 16 years of education is \\(31.21\\%\\).\n We are looking the area under the curve less than \\(10\\). For middle-class respondents, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{16-14.56}{2.95}\\\\\\\\ \u0026amp;\\approx 0.49 \\end{aligned}\\] The area beyond \\(z=0.63\\) and the tail is \\(0.2643\\) thus implying the probability of a lower-class respondent having more less 10 years of education is \\(26.43\\%\\).\n    6  We are looking the area under the curve greater than \\(625\\). For these high school graduates, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{625-536}{102}\\\\\\\\ \u0026amp;\\approx 0.87 \\end{aligned}\\] The area beyond \\(z=0.87\\) and the tail is \\(0.1992\\) thus implying the probability that a high school graduate having earning a score of more than 625 is \\(19.92\\%\\).\n We are looking the area under the curve between than \\(400\\) and \\(625\\). For these high school graduates, the \\(z\\)-score for 625 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{625-536}{102}\\\\\\\\ \u0026amp;\\approx 0.87 \\end{aligned}\\] The area between the mean and \\(z=0.87\\) is \\(0.3087\\).\nFor these high school graduates, the \\(z\\)-score for 400 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{400-536}{102}\\\\\\\\ \u0026amp;\\approx -1.33 \\end{aligned}\\] The area between\\(z=-1.33\\) and the mean is \\(0.4082\\).\nSumming these proportions yields \\(0.3087+0.4082=0.7168\\), or \\(71.68\\%\\) of the high school graduates earned a score between 400 and 625.\n The 20th percentile, or \\(0.2000\\) is equivalent to \\(z=-0.84\\). Thus the SAT ERW equivalency can be found by \\(536-0.84\\cdot 102=450.32\\), or a score of \\(450\\).\n    7  We have\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{150-100}{15} \u0026amp;\\approx 3.33 \\end{aligned}\\] or \\(z\\approx 3.33\\).\n We are looking the area under the curve greater than \\(150\\). The area beyond \\(z=3.33\\) is \\(0.0004\\) implying that the percentage of scores above 150 is \\(0.04\\%\\)1.\n We are looking at the area between \\(85\\) and \\(150\\). The \\(z\\)-score for a score of 85 is\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{85-100}{15}\\\\\\\\ \u0026amp;\\approx -1 \\end{aligned}\\] with the area between \\(z=-1\\) and the mean found to be \\(0.3413\\). The \\(z\\)-score for a score of 150 is\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{150-100}{15} \u0026amp;\\approx 3.33 \\end{aligned}\\] with the area between the mean and \\(z=3.33\\) found to be \\(0.4996\\).\nSo the \\(0.3413 + 0.4996 = 0.8409\\), or about \\(84\\%\\) of scores fall between \\(85\\) and \\(150\\).\n Scoring in the 95th percentile means that 95% of the sample scored below this level. This outcome can be calculated by \\(100 + 1.65\\cdot 15 = 124.75\\). So the IQ score that is associated with the 95th percentile is \\(124.75\\).\n    8  We are looking the area under the curve less than \\(400\\). For these high school graduates, the \\(z\\)-score is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{400-531}{114}\\\\\\\\ \u0026amp;\\approx -1.15 \\end{aligned}\\] The area beyond \\(z=-1.15\\) and the tail is \\(0.1251\\) thus implying that about \\(12.51\\%\\)of high school graduates hearned a score less than 400.\n We are looking the area under the curve between than \\(600\\) and \\(700\\). For these high school graduates, the \\(z\\)-score for 600 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{600-531}{114}\\\\\\\\ \u0026amp;\\approx 0.61 \\end{aligned}\\] The area between the mean and \\(z=0.61\\) is \\(0.2291\\).\nFor these high school graduates, the \\(z\\)-score for 700 is given by\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{700-531}{114}\\\\\\\\ \u0026amp;\\approx 1.48 \\end{aligned}\\] The area between \\(z=1.48\\) and the mean is \\(0.4306\\).\nSubtracting these proportions yields \\(0.4306-0.2291=0.2015\\), or \\(20.15\\%\\) of the high school graduates earned a score between 600 and 700\n For an earned score of \\(725\\) which is greater than the mean, we have From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{725-531}{114}\\\\\\\\ \u0026amp;\\approx 1.70 \\end{aligned}\\] The area beyond \\(z=1.70\\) is \\(0.0466\\). So the percentile rank may be found by \\(1.000-0.0466 = 0.9535\\), or \\(95.35\\%\\) which implies the 96th percentile.\n    11  For an earned score of \\(990\\) which is greater than the mean, we have From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{990-981}{27.3}\\\\\\\\ \u0026amp;\\approx 0.33 \\end{aligned}\\] The area beyond \\(z=0.33\\) is \\(0.3707\\). So the percentile rank may be found by \\(1.000-0.3707 = 0.6293\\), or \\(62.93\\%\\) which implies the 62nd percentile. So the team is not in the upper quartile.\n For the top 25%, or \\(0.0025\\), the area beyond \\(z\\) is approximated by \\(z = 0.67\\). The cutoff score is then \\(981+0.67\\cdot 27.3\\approx\\) \\(999.29\\).\n As noted above, \\(z = 0.67\\).\n    12  For Team A an eligibility criterion score of \\(971\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{971-983}{33}\\\\\\\\ \u0026amp;\\approx -0.36 \\end{aligned}\\] For Team A a retention criterion score of \\(958\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{958-976}{34.9}\\\\\\\\ \u0026amp;\\approx -0.52 \\end{aligned}\\] For Team B an eligibility criterion score of \\(987\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{987-983}{33}\\\\\\\\ \u0026amp;\\approx 0.12 \\end{aligned}\\] For Team B a retention criterion score of \\(970\\) is given by From this we can calculate the \\(z\\)-score as\n\\[\\begin{aligned} z \u0026amp;= \\dfrac{970-976}{34.9}\\\\\\\\ \u0026amp;\\approx -0.17 \\end{aligned}\\] So Team B is better on both eligibility and retention than Team A.\n For Team B a retention criterion score of \\(z=-0.17\\) is below the mean and the corresponding proportion can be viewed by   which is the area between \\(z\\) and the tail, or \\(0.4325\\).\nFor Team A an eligibility criterion score of \\(z=-0.36\\) is below the mean and the corresponding proportion can be viewed by which is the area between \\(z\\) and the tail, or \\(0.3594\\) implying about the 35th percentile.   :::\n   Which is the reason you don’t notice any shading in the plot↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"530a08282c6261f8515db43910630058","permalink":"/assignment/05-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/05-homeworks/","section":"assignment","summary":"Week 5 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 5 Problem Set Assignment Chapter 5 Exercises: 1, 2, 5, 6, 7, 8, 11, 12.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 5 after class ends What to do for Week 6 before coming to class   What to do for Week 5 after class ends1   eCampus?  Description  Location         Complete a module in Data Camp focused on basic descriptive statistics and an introduction to probability.  Link       Go through the NFL and Pipes R walkthrough  Link       Complete problems 1, 2, 5, 6, 7, 8, 11 and 12 in Chapter 5.  Link     \n What to do for Week 6 before coming to class2   Area  Description  Location         Read chapter 6 in the text.  Link       Look over the text PowerPoint and corresponding notes.  Link       Watch videos on Experimental Designs and Sampling Methods.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a231b77af4723575ec2f6a55e71ed37d","permalink":"/due/05-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/05-due/","section":"due","summary":"What to do for Week 5 after class ends What to do for Week 6 before coming to class   What to do for Week 5 after class ends1   eCampus?","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics Pipes and the NFL   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics on the Bell Curve, or what is formally known as The Normal Distribution.\n \nRemember you can always access a Crash Course Statistics playlist using the following link.\n Pipes and the NFL Take a look at a walkthrough about using NFL data and pipes from tidyverse.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"/example/05-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Crash Course Statistics Pipes and the NFL   Crash Course Statistics This week we have another brief introduction from Crash Course Statistics on the Bell Curve, or what is formally known as The Normal Distribution.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class. The module covers types of data sampling, and experiments. In particular, the sections are:\n Language of data Study types and cautionary tales Sampling strategies and experimental design Case study   Book Materials Download the textbook\n PowerPoint and Lecture Notes.   Class Notes Download the class\n PDF and Outline.   R Walkthrough Take a look at a walkthrough about using NFL data and pipes from tidyverse.\n You can download a [PDF](/slides/Week 4/Slides-Week-4.pdf){target=\"_blank\"} of the presentation above. --  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aa54929a56a6d237dc8692bd2c9cd024","permalink":"/lesson/05-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/05-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 5: The Normal Distribution\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"face5aa80a48f6b904e9856a6d9c7340","permalink":"/readings/05-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/05-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 5: The Normal Distribution\n ","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"   Crash Course Statistics   Crash Course Statistics This week we have two videos from Crash Course Statistics. The first is one on Experimental Designs which may benefit many of you after the conclusion of this course.\n \nThe second is on Sampling which focuses more on the chapter elements. Sampling is one of the most important aspects of any quantitative study. If you underestimate a sample (aka undersample), then a whole study can become useless. If you overestimate a sample (aka oversample), then you’ll be doing a bunch of extra work for no reason.\n \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"/example/06-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Crash Course Statistics   Crash Course Statistics This week we have two videos from Crash Course Statistics. The first is one on Experimental Designs which may benefit many of you after the conclusion of this course.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Book Materials Class Notes R Walkthrough: NFL Ticket Prices   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class. The module covers types of data sampling, and experiments. In particular, the sections are:\n Language of data Study types and cautionary tales Sampling strategies and experimental design Case study   Book Materials Download the textbook\n PowerPoint and Lecture Notes.   Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download a PDF of the presentation above.\n Outline.   R Walkthrough: NFL Ticket Prices Download the data sets1 and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.   Week 6 Walkthrough Data Set  Click on the presentation itself and then you may   Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.   You can download a PDF of the presentation above.\n  You will have to unzip this file. If you are unfamilair with this process, please check the Unzipping files section under Resources for assistance.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4cd52ded401299a83d62039ff42e8b4","permalink":"/lesson/06-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/06-lesson/","section":"lesson","summary":"Data Camp Book Materials Class Notes R Walkthrough: NFL Ticket Prices   Data Camp The fifth Data Camp module is due two Wednesdays following the completion of the Week 5 class.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"48122284ea46294df4577acb15f0803c","permalink":"/readings/06-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/06-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Week 7 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 7 Problem Set Assignment Chapter 6 Exercises: 1, 2, 3, 4, 5, 6, 7, 8, 9. Please turn it in to the Submission Portal on ecampus by 11:59 PM next Wednesday.\n Solutions :::\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"59cf66c3ec57afe00b0487aabf6e9b2e","permalink":"/assignment/07-homeworks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/07-homeworks/","section":"assignment","summary":"Week 7 Problem Set    span.boxed { border: 0px solid #FFFFFF; padding: 5px; color: #FFFFFF; background-color: #005b96; display: inline; } table { margin-left: auto; margin-right: auto; } table thead th { border-bottom: 1px solid #ddd; } th, td { padding: 5px; } tr:nth-child(even) { background: #ffffff; }  Week 7 Problem Set Assignment Chapter 6 Exercises: 1, 2, 3, 4, 5, 6, 7, 8, 9.","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"    What to do for Week 7 after class ends What to do for Week 6 before coming to class   What to do for Week 7 after class ends1   eCampus?  Description  Location         Take a look at the coding chunks page that may help you on the upcoming exam.  Link       Complete problems 1, 2, 3, 4, 5, 6, 7, 8, and 9 in Chapter 6.  Link       Is reality probabilistic or deterministic? Figure out what kind of statistical thinking you have.  Link     \n What to do for Week 6 before coming to class2   Area  Description  Location         Being reviewing Chapters 1-6 and material on Data Camp/Walkthroughs for the exam.  Everywhere       Watch videos on the differences between Bayesian and Frequentist Statistics.3  Link       For grading.↩︎\n For preparation.↩︎\n Optional but recommended.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fa05ee7a5ec9b3ad5001fe63b7e34625","permalink":"/due/07-due/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/due/07-due/","section":"due","summary":"What to do for Week 7 after class ends What to do for Week 6 before coming to class   What to do for Week 7 after class ends1   eCampus?","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"    Crash Course Statistics Warning A Line is Drawn Full Disclosure on My Unintended Bias A Mostly Non Stats Example A Table of Differences So Who is Right?   Crash Course Statistics This week we’re doing something a little different. We have two videos from Crash Course Statistics and another from a channel called Veritasium which all deal with the idea of probability. But you’ll soon see that there are two types of statisticians who fundamentally disagree on how probability explains the world around them.\n Warning There’s going to be some philosophical ideas discussed here. Get through it and you may figure out what kind of statistics you would practice. Why does it matter? Oh only that it may define how you not only practice statistics, but also explain how you view and deal with daily problems. No big deal.\n   A Line is Drawn1 Statisticians and those who practice statistics are at odds on how the world works. One group - the frequentists - think deductively and see probability as a means for finding a single defined outcome while another - the Bayesians - think inductively and use probability to describe the chance of many possible outcomes.\n Full Disclosure on My Unintended Bias I am a Bayesian statistician through and through so there is an inherent bias in this writing even though I did my best to stay neutral. Hopefully its small and not detectable. But now you have an inherent bias having read this. What if I had not written this disclosure?\nFrequentist Statisticians Statisticians who view the world as deterministic, do not include subjectivity, and see probabilities as a way to explain how random events would look after a bunch of trials are known as frequentist statisticians. To show this through a statistical lens, let’s look at the traditional coin flipping example: As a frequentist statistician, we would\nfirst suppress any prior ideas of how the outcome should look; then flip a coin over and over and record the results; and find that after enough flips that while we will likely never get 50-50 odds, the data shows that we’re heading that way so with the idea if we flipped that coin an infinite number of times, that’s the true outcome.  This is the idea of something being deterministic where probabilities are used to describe a fixed value which in this case is always going to be 50-50. For any social scientists, the epistemological perspective is that frequentist for the most part believe in a single truth.\n Bayesian Statisticians Statisticians who view the world as probabilistic, allow for prior beliefs about a phenomena, and update the probability of those beliefs with new evidence are known as Bayesian statisticians. To again show this through a statistical lens, let’s look at the traditional coin flipping example: As a Bayesian statistician, we would\nfirst have a prior belief of what the probability of getting a heads or tails is say 50-50. then flip a coin over and over and record the results; and find that after enough flips that we will never get to the 50-50 odds implying that there are multiple possible outcomes, each with its own associated probability.  This is the idea of something being deterministic where probabilities are used to describe multiple values which in this case may be 50-50, but could also be 40-60, 60-40, 20 - 80 and so on, each associated with some chance of being true. For any social scientists, the epistemological perspective is that Bayesian for the most part believe in multiple truths with some more likely than others.\n  A Mostly Non Stats Example Rather than provide an example that uses a bunch of statistics, let’s look at it practically.\nSituation: You have misplaced your iPhone somewhere in your apartment or home. You can use the Find My app ® to find it and hear beeping.\nProblem: Which room in your apartment or home should you search?\nApproaches\n Frequentist: You hear the phone beeping and you have some approach using a mental model to help figure out what room the beeping is coming from. So you used inferences from the beeps to locate the room in your home you must search to find the phone.\n Bayesian: You hear the phone beeping and along with some approach using a mental model, you also know all of the rooms you have misplaced the phone before which combined help to figure out what room the beeping is coming from. So you used inferences from the beeps and prior knowledge  to locate the room in your home you must search to find the phone.\n   A Table of Differences   Type  Information Used  What is Random?  Type of Reasoning  Terminology  Observed Data      Frequentist statistician  Outcomes derived strictly from experiments  Observed data2  Deductive logic.  Common terms like p-value, significant, null hypothesis, and confidence interval.  Unknown and comes only from experiments.    Bayesian statistician  Prior beliefs about what the truth might be which are updated as experiments progress.  Population parameters3  Inductive logic.  Uncommon terms like prior probability, noninformative priors, and credible intervals.  Known since we already know what we know.      So Who is Right? We honestly just don’t know right now. Similar to physics where there are two incompatible laws governing the universe - relativity for large and quantum for small objects, respectively - statistics has the same difficulty regarding inferences. The frequentist thinking is by far the dominant approach because it involves fairly “simple” and concrete calculations that can be tested and verified when explaining the phenomena around us. In fact, the approach aligns with the certainty that humans like and need to make sense of the world. However, humans also live with and through uncertainty everyday - every time we do anything, there are a range of outcomes that could occur all with their own associated likelihoods. However until recently testing these ideas were difficult. However with the rise of machine learning and advancements in computing4, complex calculations allow us to consider the possibilities of multiple outcomes for a given problem which necessitates the need for a Bayesian point of view. The fact you are able to read this right now is .\nLooking ahead, the actual approach is likely a combination of both. With that said, both could also could be incorrect and there may be a third approach we have not considered yet or is beyond our current understanding. Just because both types of thinking can be used to explain a lot of phenomena doesn’t necessarily mean either are correct!\nFrequentist Statistics  \n Bayesian Statistics  \n Bayes Theorem  \n   Note that what is described is an oversimplification and there are multiple differences between the two viewpoints.↩︎\n Any data that has been collected from an experiment.↩︎\n Any summary number, like an average or percentage, that describes the entire population.↩︎\n Fun fact: The fact you are able to view this exact page right now is based on based on probability↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"/example/07-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Crash Course Statistics Warning A Line is Drawn Full Disclosure on My Unintended Bias A Mostly Non Stats Example A Table of Differences So Who is Right?","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Data Camp Class Notes   Data Camp The fifth Data Camp module is due. The module covers types of data sampling, and experiments. In particular, the sections are:\n Language of data Study types and cautionary tales Sampling strategies and experimental design Case study   Class Notes  Download the class slides: Remember to click on the presentation itself and then you may\n Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard. Press the letter O at any point to see see a tile view of the slideshow. Use this link to view a larger version of slideshow in a new window.    You can download a PDF of the presentation above.  You can download a [PDF](/slides/Week 6/Slides-Week-6R.pdf){target=\"_blank\"} of the presentation above. --\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"36f453f698c7a3972914c894cffd917b","permalink":"/lesson/07-lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/07-lesson/","section":"lesson","summary":"Data Camp Class Notes   Data Camp The fifth Data Camp module is due. The module covers types of data sampling, and experiments. In particular, the sections are:","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"52401e1c7f6b8dc680826870d85b28f1","permalink":"/readings/07-readings/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readings/07-readings/","section":"readings","summary":"   Book Stuff   Book Stuff Read Chapter 6: Sampling and Sampling Distributions\n ","tags":null,"title":"Sampling and Sampling Distributions","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to whatever you want. The ones below are some of the more larger repositories:\n Data World: A plethora of open data sets to peruse. If you are a data fiend, consider collaborating to solve problems.\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to whatever you want. The ones below are some of the more larger repositories:\n Data World: A plethora of open data sets to peruse.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"  You can find information pertaining to Data Camp by heading over to lessons page. To simply get to Data Camp site, please click on their logo below to go to their sign in page (or directly to the modules if you are already signed in):\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"25d4817ae10e3ebf2a1eb5dead763b0e","permalink":"/assignment/datacamp-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/datacamp-assignment/","section":"assignment","summary":"You can find information pertaining to Data Camp by heading over to lessons page. To simply get to Data Camp site, please click on their logo below to go to their sign in page (or directly to the modules if you are already signed in):","tags":null,"title":"What is a Data Camp?","type":"docs"},{"authors":null,"categories":null,"content":"   Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness) Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account. Colorgorical: Create color palettes based on fancy mathematical rules for perceptual distance. Color Hex Color Codes: A collection of colors and palettes given only by hex identifiers. Colorpicker for data: More fancy mathematical rules for color palettes (explanation). ColourLovers: Like Facebook for color palettes. Comprehensive list of color palettes in r: A wided ranging collection of various palettes from the entire R community. These can now all be downloaded within a the package paleteer by running install.packages(\"paletteer\"). More information can be found here. iWantHue: Yet another perceptual distance-based color palette builder. Photochrome: Word-based color palettes. PolicyViz Design Color Tools: Large collection of useful color resources Scientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico. viridis: Perceptually uniform color scales. Wes Anderson Palettes: Palettes based off of Wes Anderson films and also one of my personal favorite sites to choose colors.   Fonts  Google Fonts: Huge collection of free, well-made fonts. The Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).   Graphic assets Images  Burst freephotos.cc Pexels Pixabay StockSnap.io Unsplash Use the Creative Commons filters on Google Images or Flickr   Vectors  aiconica: 1,000+ vector icons Noun Project: Thousands of free simple vector images Vecteezy: Thousands of free vector images   Vectors, photos, videos, and other assets  Stockio    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness) Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"   RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (as of this writing, it’s R-4.0.0.pkg; as of right now but download the most current version.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n   It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"/resource/install/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse    You will do all of your work in this class with the open source (and free!","tags":null,"title":"Installing R, RStudio, and tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids){target=\"_blank\"} APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"/resource/markdown/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R R in the Wild Sample of R Visualizations   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with the tidyverse family of packages, it’s often easier to just search for that instead of the letter “R” (e.g. “tidy pivoting”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio. LHS 610: Exploratory Data Analysis for Health: A comprehensive way to learn R using health based data sets by Dr. Karandeep Singh out of the University of Michigan. R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. RStudio Conference cheat sheets: A comprehensive collection of all recognized RStudio cheat sheets from rstudio::conf 2019. In fact if you are interested in R, check out the conference workshop materials. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. Stats Illustrations: Dr. Allison Horst created and continues to build beautiful illustrations of many R commands you will probably use. Teacup Giraffes: A fantastic and visually stunning way to learn R using animations accessible to children but for adults by Dr. Desirée De Leon at RStudio.   R in the Wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\n Sample of R Visualizations  Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test A decade(ish) of listening to Sigur Rós Disproving Approval R color Palettes by Emil Hvitfeldt is a very comprehensive list of pre packaged color palettes you can get. He has even compiled all of them into an R package called paletteer General (Attys) Distributions Health care indicators in Utah counties Mapping Fall Foliage Personal Blog from Isabella Benabaye with some fun and colorful R and Python data visualizations Sexism on the Silver Screen: Exploring film’s gender divide Song lyrics across the United States Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Who came to vote in Utah’s caucuses?    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the original source code @ andrewheiss). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R R in the Wild Sample of R Visualizations   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;August 19, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"/resource/rmarkdown/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"   Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer The Stories Behind a Line   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n The Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R. Emery’s Essentials: Descriptions and examples of 26 different chart types. From Data to Viz: A decision tree for dozens of chart types with links to R and Python code. The Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations. The Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.). R Graph Catalog:{target=\"_blank\"} R code for 124 ggplot graphs.   General resources  Ann K. Emery’s blog: Blog and tutorials by Ann Emery. Data Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway. The Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic. Evergreen Data: Helpful resources by Stephanie Evergreen. FlowingData: Blog by Nathan Yau. Info We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field. Information is Beautiful: Blog by David McCandless. PolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch. Visualising Data:{target=\"_blank\"} Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk. Storytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic. Junk Charts: Blog by Kaiser Fung. WTF Visualizations: Visualizations that make you ask “wtf?” Seeing Data: A series of research projects about perceptions and visualizations.   Visualization in Excel  How to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel. Ann Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.   Visualization in Tableau Because it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  Australia as 100 people: You can make something like this with d3 and the potato project.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"  Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"   This is me This is what I am This is where I come from This is what I research This is how you can contact me This is Sparta   This is me     This is what I am Professor of Educational Psychology and head of the Program Evaluation Certificate at WVU.\n This is where I come from  PhD in Program Evaluation from Western Michigan University. MS in Mathematical Sciences from Michigan Technological University. BS in Mathematics from West Virginia Wesleyan College.   This is what I research  Transference of Foundational Evaluation Concepts Modeling using Machine Learning Applications of Predictive Hierarchical Social Networks  which are just fancy ways of saying\n How people learn basic concepts in evaluation Programming a type of artificial intelligence and then training it to do some of my work for me1 Figuring out how things are related to each other and then making judgments2  You can check out more about what I do here:    ORCID   GitHub   ResearchGate  This is how you can contact me    Abhik.Roy   @DrTheoreticalPE  This is Sparta     No its not Skynet. I said it in the syllabus and I’ll say it here: computers are stupid.↩︎\n Which is a very evaluation like thing to do!↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"779259905d42fe327a85b980891c96bc","permalink":"/professor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/professor/","section":"","summary":"This is me This is what I am This is where I come from This is what I research This is how you can contact me This is Sparta   This is me     This is what I am Professor of Educational Psychology and head of the Program Evaluation Certificate at WVU.","tags":null,"title":"Professor","type":"page"},{"authors":null,"categories":null,"content":"  Every class session may be comprised of four important sections with tasks denoted in a fifth area. You should read about the details for each using the main menu at the top of this webpage.\n Readings (): This page contains the readings for the topic. Read these first each time you visit. Lesson (): This page contains interactive lessons, slides, and/or recorded walkthroughs that teaches you the principles and code you need to know. Go through these after doing the content. Example (): This page contains extra items that are picked or created to help you understand the current material. You do not have anything to complete in this section. From time to time I may post videos of performing something in R that may be beneficial. Assignment (): This page contains the instructions for all student related tasks including information about the tasks and the final. Due (): This page contains the instructions what to submit by week. Assignments are due by Wednesday at 11:59 PM or as noted.  ::: note  tl;dr: I recommend that you follow these lessons in the following order:\n        \n or in terms that aren’t as ambiguous as emojis, follow this process: read the material, complete data camp with other possible items, and then use the example (if given) along with the assignment to help you with what is due. :::\n   Concentration Readings Lesson Example Assignment Due   September 2, 2020 The What and Why of Statistics        September 9, 2020 The Organization and Graphic Presentation of Data        September 16, 2020 Measures of Central Tendency        September 23, 2020 Measures of Variability        September 30, 2020 The Normal Distribution        October 7, 2020 Sampling and Sampling Distributions         October 14, 2020 Sampling and Sampling Distributions           ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Every class session may be comprised of four important sections with tasks denoted in a fifth area. You should read about the details for each using the main menu at the top of this webpage.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"    Course objectives FAQ  Is the course content difficult? Is R difficult? What if I find a mistake?  Course materials  Texts Software Note: Online help  Assignments and Grades  R, Rmarkdown, Extra Credit and Showing Work Your hours Class conduct and expectations Learning and knowledge during a pandemic  Course policies COVID-19 Statement  Psychological and Psychiatric Services CARE Team Lauren’s Promise Academic Integrity Inclusivity Statement Incomplete Grades Sale of Course Materials Student Evaluation of Instruction (SEI) University Attendance Policy Course Netiquette Response Time Technical Requirements Technical Support    Instructor  Dr. Abhik Roy  Allen Hall 504O  Abhik.Roy@mail.wvu.edu  @DrTheoreticalPE  Schedule an appointment   Course details  Forever (for the next 13 weeks)  August 26 - December 11  Thursdays from 4:00-6:50 PM  Agricultural Sciences building Rm. 2004  Slack   Contacting me Slack is the best way to get in contact with me. Response times are typically much quicker than the standard 24-48 hours given to emails. Please remember that life is somewhat chaotic at this time!\n  Course objectives Students will become proficient and be able to describe in detail the following aspects:\nAssessing and describing counterfactuals and scientific argumentation. Defining the underlying constructs for appropriate statistical techniques. Describing and graphing data. Explaining and drawing inferences from data. Gaining real world experience using R to analyze data sets. Obtaining an understanding of ethical, social, political, and cultural issues confronted by those who perform statistical analyses. Understanding and critically assessing differing analytical methods.   FAQ Is the course content difficult? You’ve probably heard an answer like this before: At times some of the material can be dense. Well that isn’t much of a response. Difficulty is not the issue here since as humans with differing educational background, we will have strengths in some areas more than others. Instead the question is can you identify areas that need strengthened and communicate them? If you can, then you have a good shot at succeeding in this class.\n Is R difficult? Learning R can be especially challenging at first—it’s akin to learning a new language like Spanish or even mathematics. Even experienced R users get frustrated…and so much so that some of us have swear jars. However as silly as it sounds one of the best feelings is to overcome a roadblock. With that said, if you find yourself getting irritated, try the following: take a break, go let some frustration out, sleep, discuss with a peer, etc. If you are at your limit, take a few breaths and contact me!\n What if I find a mistake? Tell me! I strive to be error free but like everyone else, make silly mistakes. This includes grammar and spelling errors as well!\n  Course materials There are two texts and three software packages necessary for this course. With that said, you will receive some supplementary materials in the course as well.\nTexts We’ll rely on the texts below:\n Frankfort-Nachmias, C. and Leon-Guerrero, A. (2020). Social Statistical for a Diverse Society (9th ed.). Sage.\n American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\n  There will occasionally be additional articles and videos to read and watch. When this happens, links to these other resources will be included on the content page for that session.\n Software R and RStudio You will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard — R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code. Please note that\nyou do not need to have any programming experience to use R\n R is free, but it can sometimes be a task to install and configure. To make life easier, you can opt to use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This is recommended for those of you who do not want to install programs right now but please note that you will most likely have to in the near future.\nThe service is convenient, but please keep in mind that it can be slow and is not designed to handle large data sets or more complicated analysis and graphics. You also cannot customize much with RStudio.cloud. Over the course of the term, you’ll want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here.\n  Note: Online help Data science and statistical programming can be difficult. Computers are stupid and its always the tiny errors in your coding can result in tons of headache. People working in any syntax based software package at any level experience this!\nBut there are multiple resources online to help.\n First you will have access to Data Camp which allows you to relieve professional training that otherwise costs a lot of money. Is it perfect and flashy? Nope but it is interactive and if you pay attention, then R’s initial steep learning curve won’t feel like an uphill battle.\n Second two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)). I freely admit that StackOverflow has saved me multiple hours of frustration. Just note if you have a syntax or process issue, both sites require you to produce a minimally reproducible example.\n Third using Twitter you can post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\n Fourth searching for help with R on Google (or another indexed search site) can sometimes be tricky because the program name is,for a lack of a better explanation, a single letter.\n Fifth we have a class chatroom which may be accessed via Slack where you can poise a question. I will monitor Slack regularly and will respond quickly. (This is a rare instances where I keep notifications on so please utilize it!) Ask questions about the readings, exercises, and mini projects. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too.\n Sixth and lastly you can send items by Slack or email me with questions about content, R or whatever (after giving a graduate student effort of course). Please include the subject header EDP 613 “the title of your email” where the title of your email is given without the quotes. Know that I will not just give you the answer, but am happy to push you in the right direction. If you do email me, please include the following:\n A brief description of the problem and what you have done thus far including the line number of the issue (if applicable). A copy of your data set (if applicable). A copy of your script or Rmarkdown document with comments throughout so I know what you’ve tried historically leading up to the issue.   I have a mail filter that will automatically bring your email to a top priority status.1\nYou should absolutely expect to struggle at times, but there is no better and more satisfying feeling than figuring things out for yourself! In the long run, you’re more likely to remember things you’ve figured out rather than those you’ve been shown or told.\n  Assignments and Grades You can find descriptions for all the assignments on the assignments page.\n   Percent  Assignment      10  Data Camp    10  Data Explorations    10  Homeworks    20  Quizzes    25  Exams    25  Final Exam      \n   Grade  Range      A  90–100%    B  80%-89%    C  70–79%    D  60–69%    F  \u0026lt; 60%      R, Rmarkdown, Extra Credit and Showing Work If you submit your work in Rmarkdown, you may earn up to an extra five percent on any task (this includes quizzes and exams!). Additionally in lieu of showing step by step work, you may submit R code instead. In these instances, both the code used and output must be presented but you still must provide a written or typed interpretation in context. Do note that certain items will explicitly ask for work to be shown by hand. In those cases, work in R will not be accepted.\n Your hours Please watch this video:\n Your hours (formerly known as office hours) are set times dedicated to you the student!2). This means that I will be in my office at home waiting for you to come by talk to me remotely with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns.\nBecause of the pandemic, we cannot meet in person. I can meet you online via Zoom. You can request a meeting through either e-mail or Slack.\n Class conduct and expectations Here are the rules, expectations, and policies that I came up with or stole from other professors:\n Late work: Barring the exams and quizzes, past due deliverables will only be accepted up to 48 hours after the initial time and due date. For each full day an assignment is late, 10% of the final grade will be deducted. All submissions must be made via eCampus. There are no exceptions to this policy. Please note that I will not accept coursework by email or any other means.\n Showing work: On any submission, you must show all relevant work where applicable. I don’t know what you know so its your job to provide all of the necessary evidence to convince me that you do know what you say you know. While you’ve probably heard this multiple times over your life, think of it this way: if someone tells you that not only is the Earth flat, but the core is actually made out of styrofoam and Moscato, the first question you should probably ask is what’s your evidence? and possibly the second may be is the core delicious?.\n Participation: Please ensure that you are engaged and participate in class. Engagement is mostly defined by you—if that means commenting and answering questions, neat; if it means sitting quietly and being focused, also neat; but if it means being being disrespectful or flaking off, not so neat.\n Rubrics: While there are valid reasons for the utilization of a rubric in undergraduate classes, at the graduate level, I do not (often) provide nor use a such an item to guide or evaluate your submission due to four primary concerns:\nWhen writing anything in academia that is pivotal (a thesis, dissertation, journal article, report, etc.), there is (typically) no such document as a rubric. If you write within the limitations as defined in a rubric, then creativity may be stifled (i.e. writing to the rubric rather than constructing a product from the ground up). Feedback can only be given along the criteria listed within a rubric which limits your learning as a student and constrains me as the instructor. Unless you are in a very specific area, the real world does not use rubrics!   Technology use: Use phones, computers, etc. responsibly. You’re all adults…well I am most of the time.\n My philosophy:\nJust assume that all submissions are formal and must be submitted with the appropriate use of language, grammar, syntax, etc. and follow standard APA 7th edition formatting guidelines where applicable. People who are easily offended by content, believe their work to be flawless or are generally unable to handle criticism should consider looking at another course. If you want rainbows and ponies, consider scrapbooking. If you care about data and learning highly marketable skills, you’ve come to the right place. There is a great deal of content in this course and you will likely struggle with some at times. Given that, there is also something to be said about the satisfaction a person gets when figuring something out, but nowhere is it written that has to be on your own. You may find that a nudge here or there elicits the same feeling so please reach out for help.    Learning and knowledge during a pandemic When course objectives are written explicitly and clearly, they provide the information you need to figure out what a student should be able to do by the end of a given term. In fact, professors often test your proficiency in an area through multiple assessments such as exams, papers, presentations, etc where you are essentially asked to show us what you have learned. However learning is not the same as knowledge .\nTo save you from a long philosophical narrative on epistemology, in a nutshell we humans aren’t that good at evaluating a person’s knowledge mainly because its not a well-defined concept. With that said, we believe one indicator of knowledge is in a person’s ability to successfully explain a high level concept in such a way that the lay person can understand it. Every so often, consider asking yourself this:\nCan I describe whatever using conversational language so that a child could understand it?\n On top of what’s noted above and I’m not sure how to articulate this any better - life sucks right now! It is likely by now you know people who have been hospitalized or passed away, lost their jobs, and/or tested positive for COVID-19. Additionally stresses in life are up. Given this, here is my promise to you:\nIf you keep an open line, show initiative, and let me know ahead of time if something is going not according to plan3, I will do everything I can to help you learn everything you were hoping to learn from this class!\n   Course policies Its pretty simple: Be nice. Be honest. Don’t cheat. Stay in touch. Be a good human.\n We will also follow WVU’s Code of Conduct.\nThis syllabus reflects a plan for the term but things change and plans change. so deviations may become necessary as we move along during the term. Note that I reserve the right to alter or amend this syllabus and will send notifications if course tasks are affected.\n COVID-19 Statement WVU is committed to maintaining a safe learning environment for all students, faculty, and staff. Should campus operations change because of health concerns related to the COVID-19 pandemic, it is possible that this course will move to a fully online delivery format. If that occurs, students will be advised of technical and/or equipment requirements, including remote proctoring software.\nIn a face-to-face environment, our commitment to safety requires students, staff, and instructors to observe the social distancing and personal protective equipment (PPE) guidelines set by the University at all times. While in class, students will sit in assigned seats when applicable and wear the required PPE. Should a student forget to bring the required PPE, PPE will be available in the building for students to acquire. Students who fail to comply will be dismissed from the classroom for the class period and may be referred to the Office of Student Conduct for further sanctions.\nIf a student becomes sick or is required to quarantine during the semester, they should notify the instructor. The student should work with the instructor to develop a plan to receive the necessary course content, activities, and assessments to complete the course learning outcomes.\nPsychological and Psychiatric Services Life at WVU can be complicated and challenging, especially during a pandemic! You might feel overwhelmed, experience anxiety or depression, or struggle with relationships or family responsibilities. Psychological and Psychiatric Services provides free, confidential support for students who are struggling with mental health and emotional challenges. The office is staffed by professional counselors and psychiatrists who are attuned to the needs of all types of college and professional students. Please do not hesitate to contact them for assistance—getting help is a smart and courageous thing to do.\n CARE Team If you or anyone you know may be at-risk such as those listed here, please make a CARE referral. You may do so directly at the main WVU CARE TEAM site.\n Lauren’s Promise I will listen and believe you if someone is threatening you.\n Lauren McCluskey, a 21-year-old honors student athlete, was murdered on October 22, 2018 by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or the Campus Police at 304-293-3136.\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or call/text a private Title IX On-Call Line 304-906-9930.\nAny form of sexual harassment or violence will not be excused or tolerated at West Virginia University. WVU has instituted procedures to respond to violations of these laws and standards, programs aimed at the prevention of such conduct, and intervention on behalf of the victims.\n Academic Integrity The integrity of the classes offered by any academic institution solidifies the foundation of its mission and cannot be sacrificed to expediency, ignorance, or blatant fraud. Therefore, I will enforce rigorous standards of academic integrity in all aspects and assignments of this course. For the detailed policy of West Virginia University regarding the definitions of acts considered to fall under academic dishonesty and possible ensuing sanctions, please see the West Virginia University Academic Catalog at http://catalog.wvu.edu/undergraduate/coursecreditstermsclassification/#academicintegritytext. Should you have any questions about possibly improper research citations or references, or any other activity that may be interpreted as an attempt at academic dishonesty, please see me before the assignment is due to discuss the matter.\n Inclusivity Statement The West Virginia University community is committed to creating and fostering a positive learning and working environment based on open communication, mutual respect, and inclusion.\nIf you are a person with a disability and anticipate needing any type of accommodation in order to participate in this class, please advise me and make appropriate arrangements with the Office of Accessibility Services (304-293-6700).\nFor more information on West Virginia University’s Diversity, Equity, and Inclusion initiatives, please see http://diversity.wvu.edu.\n Incomplete Grades Students who want to be considered for an Incomplete must apply to their instructor prior to the end of the term. If the instructor agrees, the instructor and the student must negotiate the conditions under which the grade of I will be changed to a letter grade and sign a contract. The date to submit the incomplete work should not be set beyond the last day of class of the following semester. If the student does not complete the terms of contract, then the instructor should submit a grade of F. All incomplete contracts must be filed with the department and Dean’s Office. See the policy at [Students who want to be considered for an Incomplete must apply to their instructor prior to the end of the term. If the instructor agrees, the instructor and the student must negotiate the conditions under which the grade of I will be changed to a letter grade and sign a contract. The date to submit the incomplete work should not be set beyond the last day of class of the following semester. If the student does not complete the terms of contract, then the instructor should submit a grade of F. All incomplete contracts must be filed with the department and Dean’s Office. See the policy at http://catalog.wvu.edu/undergraduate/enrollmentandregistration/#gradestext.\n Sale of Course Materials All course materials, including lectures, class notes, quizzes, exams, handouts, presentations, and other materials provided to students for this course are protected under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Please review the sharing and editing restrictions prior to distributing or amending any material on this site. As such, the unauthorized purchase or sale of these materials may result in disciplinary sanctions under the Campus Student Code. Basically you can share what you like but don’t try to make a buck.\n Student Evaluation of Instruction (SEI) Effective teaching is a primary mission of West Virginia University. Student evaluation of instruction provides the university and the instructor with feedback about your experiences in the course for review and course improvement. Your participation in the evaluation of course instruction is both strongly encouraged and highly valued. Results are strictly confidential, anonymous, and not available to the instructor until after final grades are released by Admissions and Records. Information about how you can complete this evaluation will be provided later.\n University Attendance Policy At West Virginia University, class attendance contributes significantly to academic success. Students who attend classes regularly tend to earn higher grades and have higher passing rates in courses. Excessive absences may jeopardize students’ grades or even their ability to continue in their courses. There is a strong correlation between regular class attendance and academic success.\n Course Netiquette The basic premise is that the etiquette expected of students in the online environment is the same as that expected in a classroom. Common courtesy is the guiding rule of Internet communications. Be prepared to communicate effectively when taking an online course. Following these simple netiquette rules in your online class or education environment will ensure your success:\n Include a professional salutation. In this case, “Hello Dr. Roy” or “Dear Dr. Roy” is appropriate. Include a proper ending such as “Thank you” or “With regards.” Then type in your full name. Never type in ALL CAPS, because it reads as if you ARE SHOUTING AT PEOPLE. Act as professionally, via your writing, as you would in a face to face classroom. Refrain from inappropriate language and derogatory or personal attacks. Do not dominate any discussion. Give other students the opportunity to join in the discussion. Disagree with ideas but avoid challenges that may be interpreted as a personal attack. Check that you are replying to the specific person you intend, and not to the entire class. Never give your password to another person. Respect the virtual classroom. Never forward in-class communications or posts by others outside of this virtual space. Never spam your classmates. If you quote someone’s previous post, only quote enough to make your point.  Be aware of the University’s Academic Integrity and Dishonesty Policy http://catalog.wvu.edu/undergraduate/coursecreditstermsclassification/#academicintegritytext. You can review the rules, regulations, and procedures concerning student conduct and discipline for the main campus of West Virginia University, at http://campuslife.wvu.edu/r/download/1802350.\n Response Time I generally respond to Slack queries in the same day while responses to emails and discussion posts are within 48 hours, except during holidays. Often, I will reply much more quickly but you should not count on a immediate. Please plan accordingly so that you don’t miss deadlines! I generally return assignments within one to two weeks after a final submission date.\n Technical Requirements Students need to have access to a computer for word processing, e-mail and access to eCampus. Access to the Internet is necessary for completion of this course. Run the Browser Check. This tool will check that you are using a supported Internet browser and have a valid Java version installed. The required technical skills to participate in this course are:\nNavigate the web Use email with attachments Create and submit files in commonly used word processing program formats Copy and paste Download and install software Consult software tutorials and other online sources as a method of learning software features Use syntax when necessary  Notice that programming is not on here!\n Technical Support Technical support regarding your use of eCampus is available by contacting 304-293-4444 (telephone), 1-877-327-9260 (toll free number), itshelp@mail.wvu.edu (email), and/or http://it.wvu.edu (website).\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n   In this new normal, I receive an overwhelming number of emails per day and may miss yours if the header is not formatted properly.↩︎\n There is some misunderstanding about what office hours actually are! For some reason that is not clear, particular graduate students have noted in my course evaluations that they believe these to be the times I should not be disturbed. This is not just a local issue!, which is the exact opposite of what they are for!↩︎\n I realize sometimes this just isn’t possible so contact me as soon as you can.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course objectives FAQ  Is the course content difficult? Is R difficult? What if I find a mistake?  Course materials  Texts Software Note: Online help  Assignments and Grades  R, Rmarkdown, Extra Credit and Showing Work Your hours Class conduct and expectations Learning and knowledge during a pandemic  Course policies COVID-19 Statement  Psychological and Psychiatric Services CARE Team Lauren’s Promise Academic Integrity Inclusivity Statement Incomplete Grades Sale of Course Materials Student Evaluation of Instruction (SEI) University Attendance Policy Course Netiquette Response Time Technical Requirements Technical Support    Instructor  Dr.","tags":null,"title":"Syllabus","type":"page"}]