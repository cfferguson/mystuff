<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>About the Lessons | Statistical Methods I</title>
    <link>/lesson/</link>
      <atom:link href="/lesson/index.xml" rel="self" type="application/rss+xml" />
    <description>About the Lessons</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>/img/course_info.png</url>
      <title>About the Lessons</title>
      <link>/lesson/</link>
    </image>
    
    <item>
      <title>Testing Hypotheses</title>
      <link>/lesson/10-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/10-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-notes&#34;&gt;In Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#textbook-class-notes&#34;&gt;Textbook Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough-testing&#34;&gt;R Walkthrough: Testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;in-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%2010/Slides-Week-10.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 10/Slides-Week-10.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%2010/Slides-Week-10.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;textbook-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Textbook Class Notes&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%209/SSDS_Ch7.PPTX&#34; target=&#34;_blank&#34;&gt;Ch 7 PowerPoint&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week9/SSDS_Ch7.docx&#34; target=&#34;_blank&#34;&gt;Ch 7 Lecture Notes&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%2010/SSDS_Ch8.PPTX&#34; target=&#34;_blank&#34;&gt;Ch 8 PowerPoint&lt;/a&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week10/SSDS_Ch8.docx&#34; target=&#34;_blank&#34;&gt;Ch 8 Lecture Notes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough-testing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough: Testing&lt;/h3&gt;
&lt;p&gt;Take a look at a &lt;a href=&#34;/lesson/hypotheses/&#34;&gt;walkthrough&lt;/a&gt; about &lt;em&gt;t&lt;/em&gt;-tests.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The t-tests</title>
      <link>/lesson/hypotheses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/hypotheses/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preparation&#34;&gt;Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#purpose&#34;&gt;Purpose&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objectives&#34;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-t.test-command&#34;&gt;The &lt;code&gt;t.test()&lt;/code&gt; Command&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#its-all-about-the-assumptions&#34;&gt;It’s All About the Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#midwest-data-set-from-ggplot2&#34;&gt;Midwest Data Set from ggplot2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-sample-t-test&#34;&gt;One-sample &lt;em&gt;t&lt;/em&gt;-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-sample-t-test&#34;&gt;Two-sample &lt;em&gt;t&lt;/em&gt;-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-paired-t-test&#34;&gt;The Paired &lt;em&gt;t&lt;/em&gt;-test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;p&gt;Download a &lt;a href=&#34;/scripts/Hypotheses.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in this walkthrough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;One of the most common tests in statistics, the &lt;em&gt;t&lt;/em&gt;-test, is used to determine whether the means of two groups are equal to each other. The assumption for the test is that both groups are sampled from normal distributions with equal variances. The null hypothesis is that the two means are equal, and the alternative is that they are not. It is known that under the null hypothesis, we can calculate a &lt;em&gt;t&lt;/em&gt;-statistic that will follow a &lt;em&gt;t&lt;/em&gt;-distribution with &lt;span class=&#34;math inline&#34;&gt;\(n_1+n_2 - 2\)&lt;/span&gt; degrees of freedom. There is also a widely used modification of the &lt;em&gt;t&lt;/em&gt;-test, known as Welch’s &lt;em&gt;t&lt;/em&gt;-test that adjusts the number of degrees of freedom when the variances are thought not to be equal to each other. This tutorial covers the basics of performing &lt;em&gt;t&lt;/em&gt;-tests in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;objectives&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Objectives&lt;/h3&gt;
&lt;p&gt;This guide serves as an introduction to performing &lt;em&gt;t&lt;/em&gt;-tests to compare two groups. Here’s what we’ll&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;introduce the &lt;code&gt;midwest&lt;/code&gt; example data set&lt;/li&gt;
&lt;li&gt;use the &lt;code&gt;patchwork&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;associate the &lt;em&gt;t&lt;/em&gt;-test in R by using the overarching command &lt;code&gt;t.test()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;provide some familiarity with the variants of the &lt;em&gt;t&lt;/em&gt;-test:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;One-sample &lt;em&gt;t&lt;/em&gt;-tests&lt;/strong&gt;: Compare the sample mean with a known value, when the variance of the population is unknown&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-sample &lt;em&gt;t&lt;/em&gt;-tests&lt;/strong&gt;: Compare the means of two groups under the assumption that both samples are random, independent, and normally distributed with unknown but equal variances&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paired &lt;em&gt;t&lt;/em&gt;-tests&lt;/strong&gt;: Compare the means of two sets of paired samples, taken from two populations with unknown variance&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages&lt;/h3&gt;
&lt;p&gt;There is one package we’ll be using that you probably do not have: the &lt;code&gt;patchwork&lt;/code&gt; package which is a tidy update to the base R &lt;code&gt;grid&lt;/code&gt; and &lt;code&gt;gridExtra&lt;/code&gt; packages. If you do not have it yet, make sure to install it first using&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;#39;patchwork&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now please load up the following packages&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.4     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter()     masks stats::filter()
## x dplyr::group_rows() masks kableExtra::group_rows()
## x dplyr::lag()        masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-t.test-command&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;t.test()&lt;/code&gt; Command&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;t.test()&lt;/code&gt; function can be used to perform both one and two sample &lt;em&gt;t&lt;/em&gt;-tests on vectors of data. The function contains a variety of arguments and is called as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, y = NULL, alternative = c(&amp;quot;two.sided&amp;quot;, &amp;quot;less&amp;quot;, &amp;quot;greater&amp;quot;), mu = 0, 
       paired = FALSE, var.equal = FALSE, conf.level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the letters - aka arguments - inside of the command are specifically defined:&lt;/p&gt;
&lt;center&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Options that you can use in t.test()
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
What it is…
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;x&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a &lt;em&gt;numeric vector&lt;/em&gt; from a data set
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;y&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
an optional &lt;em&gt;numeric vector&lt;/em&gt; from a data set
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;mu&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a number indicating the true value of the mean
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;alternative&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
preference on type of test you wish to run
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;paired&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
preference on whether you wish to perform a paired &lt;em&gt;t&lt;/em&gt;-test
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;var.equa&lt;/code&gt;l
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
indicates whether or not to assume equal variances when performing a two-sample &lt;em&gt;t&lt;/em&gt;-test
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 20em; &#34;&gt;
&lt;code&gt;conf.level&lt;/code&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the confidence level of the reported confidence interval
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;Below are a few things to note about the &lt;code&gt;t.test()&lt;/code&gt; command before we move on that can drastically affect your outcomes if you are not aware of them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the variable &lt;code&gt;y&lt;/code&gt; is
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;excluded&lt;/em&gt;, &lt;code&gt;t.test()&lt;/code&gt; will run as a &lt;strong&gt;one-sample &lt;em&gt;t&lt;/em&gt;-test&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;included&lt;/em&gt;, &lt;code&gt;t.test()&lt;/code&gt; will run as a &lt;strong&gt;two-sample &lt;em&gt;t&lt;/em&gt;-test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The default &lt;code&gt;t.test()&lt;/code&gt; command will run as a two-sided &lt;em&gt;t&lt;/em&gt;-test. However you can perform an alternative hypothesis by changing the &lt;code&gt;alternative&lt;/code&gt; argument to:
&lt;ul&gt;
&lt;li&gt;“&lt;code&gt;greater&lt;/code&gt;”, or&lt;/li&gt;
&lt;li&gt;“&lt;code&gt;less&lt;/code&gt;”.
For example&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; t.test(x, alternative = &amp;quot;greater&amp;quot;, mu = 47)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;performs a one-sample &lt;em&gt;t&lt;/em&gt;-test on the data contained in x where the
- null hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(\mu = 47\)&lt;/span&gt;, and the
- alternative hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(\mu &amp;gt; 47\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;paired&lt;/code&gt; argument has a default setting of &lt;code&gt;FALSE&lt;/code&gt; indicating that a paired &lt;em&gt;t&lt;/em&gt;-test should not be run. If you wish to run a paired &lt;em&gt;t&lt;/em&gt;-test, this can be done by changing the setting to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;var.equals&lt;/code&gt; argument has a default setting of &lt;code&gt;FALSE&lt;/code&gt; indicating unequal variances ans applies the Welsch approximation to the degrees of freedom. If you wish to have equal variances, this can be done by changing the setting to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;conf.level&lt;/code&gt; argument is set to 95%, or where &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;. The confidence interval is determined by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; for the one-sample &lt;em&gt;t&lt;/em&gt;-test, and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_1-\mu_2\)&lt;/span&gt; for the one-sample &lt;em&gt;t&lt;/em&gt;-test.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On a final note, the &lt;code&gt;wilcox.test()&lt;/code&gt; option provides the same basic functionality and arguments, but with the idea that we &lt;strong&gt;do not want to assume the data to follow a normal distribution&lt;/strong&gt;. This is often the case for samples but at this time, we will assume that the data that is normally distributed. Understanding the &lt;code&gt;wilcox.test()&lt;/code&gt; function is beyond the scope of this course.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;its-all-about-the-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;It’s All About the Assumptions&lt;/h2&gt;
&lt;p&gt;Before we can move forward in the exploration of the &lt;em&gt;t&lt;/em&gt;-test, we must first make sure that any data that we use adheres to the test assumptions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assumption #1: Random sampling&lt;/strong&gt;: Data is used from random sampling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumption #2: Independent observations&lt;/strong&gt;: Observations are independent from one another.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumption #3: Normality&lt;/strong&gt;: Observations are from a normally distributed population.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumption #4: Homogeneity&lt;/strong&gt;: If more than one population is sampled from, then the populations have equal variances (also known as &lt;strong&gt;&lt;em&gt;homogeneity of variances&lt;/em&gt;&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;div id=&#34;midwest-data-set-from-ggplot2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Midwest Data Set from ggplot2&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;ggplot2&lt;/code&gt; package has some test data built in, though you may not find them that exciting which is the reason we tend to use real-world data that is more connected to the social sciences and education. However, in this walkthrough, we will be using one of the &lt;code&gt;ggplot2&lt;/code&gt; data sets named &lt;code&gt;midwest&lt;/code&gt;. This data set contains contains county-level data for 5 states: IL, IN, MI, OH, &amp;amp; WI derived from the 2010 U.S. Census. I’ll leave the level of excitement up to you. When you lead up &lt;code&gt;tidyverse&lt;/code&gt;, or more specifically &lt;code&gt;ggplot2&lt;/code&gt;, the &lt;code&gt;midwest&lt;/code&gt; data sets is also loaded automatically. Let’s take a look at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(midwest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 28
##     PID county state  area poptotal popdensity popwhite popblack popamerindian
##   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt;         &amp;lt;int&amp;gt;
## 1   561 ADAMS  IL    0.052    66090      1271.    63917     1702            98
## 2   562 ALEXA… IL    0.014    10626       759      7054     3496            19
## 3   563 BOND   IL    0.022    14991       681.    14477      429            35
## 4   564 BOONE  IL    0.017    30806      1812.    29344      127            46
## 5   565 BROWN  IL    0.018     5836       324.     5264      547            14
## 6   566 BUREAU IL    0.05     35688       714.    35157       50            65
## # … with 19 more variables: popasian &amp;lt;int&amp;gt;, popother &amp;lt;int&amp;gt;, percwhite &amp;lt;dbl&amp;gt;,
## #   percblack &amp;lt;dbl&amp;gt;, percamerindan &amp;lt;dbl&amp;gt;, percasian &amp;lt;dbl&amp;gt;, percother &amp;lt;dbl&amp;gt;,
## #   popadults &amp;lt;int&amp;gt;, perchsd &amp;lt;dbl&amp;gt;, percollege &amp;lt;dbl&amp;gt;, percprof &amp;lt;dbl&amp;gt;,
## #   poppovertyknown &amp;lt;int&amp;gt;, percpovertyknown &amp;lt;dbl&amp;gt;, percbelowpoverty &amp;lt;dbl&amp;gt;,
## #   percchildbelowpovert &amp;lt;dbl&amp;gt;, percadultpoverty &amp;lt;dbl&amp;gt;,
## #   percelderlypoverty &amp;lt;dbl&amp;gt;, inmetro &amp;lt;int&amp;gt;, category &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sample-t-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One-sample &lt;em&gt;t&lt;/em&gt;-test&lt;/h3&gt;
&lt;p&gt;The one-sample &lt;em&gt;t&lt;/em&gt;-test compares a sample’s mean with a known value, when the variance of the population is unknown. Consider we want to assess the percent of college educated adults in the midwest and compare it to a certain value. For example, let’s assume the nation-wide average of college educated adults is approximately &lt;a href=&#34;%22https://www.census.gov/data/tables/2018/demo/education-attainment/cps-detailed-tables.html%22&#34; target=&#34;_blank&#34;&gt;37% (Bachelor’s degree or higher)&lt;/a&gt; and we want to see if the midwest mean is significantly different than the national average; in particular we want to test if the midwest average is less than the national average.&lt;/p&gt;
&lt;p&gt;Let’s first take a look at how the data looks (important!) and some descriptive statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(midwest$percollege, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 19.63139 11.24331 17.03382 17.27895 14.47600 18.90462 11.91739 16.19712
##  [9] 14.10765 41.29581&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(midwest$percollege)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   7.336  14.114  16.798  18.273  20.550  48.079&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then plot it to see the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(midwest, aes(x = percollege)) + 
        geom_histogram(aes(fill = ..count..), color = &amp;quot;#ccccaa&amp;quot;) +
        scale_fill_gradient(&amp;quot;Frequency&amp;quot;, low = &amp;quot;#5cb85c&amp;quot;, high = &amp;quot;#428bca&amp;quot;) +
        theme_minimal() +
        theme(legend.position = &amp;quot;bottom&amp;quot;,
              legend.direction = &amp;quot;horizontal&amp;quot;)



p2 &amp;lt;- ggplot(midwest, aes(x = percollege)) + 
        geom_histogram(aes(fill = ..count..), color = &amp;quot;#ccccaa&amp;quot;) +
        scale_fill_gradient(&amp;quot;Frequency&amp;quot;, low = &amp;quot;#5cb85c&amp;quot;, high = &amp;quot;#428bca&amp;quot;) +
        scale_x_log10() +
        theme_minimal() +
        theme(legend.position = &amp;quot;bottom&amp;quot;,
              legend.direction = &amp;quot;horizontal&amp;quot;)


p1 + p2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/Hypotheses_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To test if the midwest average is less than the national average, we’ll perform three tests. First we test with a normal &lt;em&gt;t&lt;/em&gt;-test without any distribution transformations. The results below show a &lt;em&gt;p&lt;/em&gt;-value &amp;lt; .001 supporting the alternative hypothesis that “the true mean is less than 37%.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(midwest$percollege, mu = 37, alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  midwest$percollege
## t = -62.518, df = 436, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is less than 37
## 95 percent confidence interval:
##     -Inf 18.7665
## sample estimates:
## mean of x 
##  18.27274&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, due to the non-normality concerns we can perform this test in another way to ensure our results are not being biased due to assumption violations. We can perform the &lt;em&gt;t&lt;/em&gt;-test and transform our data. The results support our initial conclusion that the percent of college educated adults in the midwest is statistically less than the nationwide average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(log(midwest$percollege), mu = log(37), alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  log(midwest$percollege)
## t = -51.37, df = 436, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true mean is less than 3.610918
## 95 percent confidence interval:
##      -Inf 2.879812
## sample estimates:
## mean of x 
##  2.855574&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;two-sample-t-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two-sample &lt;em&gt;t&lt;/em&gt;-test&lt;/h3&gt;
&lt;p&gt;Now let’s say we want to compare the differences between the average percent of college educated adults in Ohio versus Michigan. Here, we want to perform a two-sample &lt;em&gt;t&lt;/em&gt;-test. So let’s first do some data wrangling…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ohio_mi &amp;lt;- midwest %&amp;gt;%
        filter(state == &amp;quot;OH&amp;quot; | state == &amp;quot;MI&amp;quot;) %&amp;gt;%
        select(state, percollege)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…and find some descriptive statistics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First for Ohio:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ohio summary stats
summary(ohio_mi %&amp;gt;% 
          filter(state == &amp;quot;OH&amp;quot;) %&amp;gt;% 
          .$percollege)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   7.913  13.089  15.462  16.890  18.995  32.205&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;and then for Michigan:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ohio summary stats
summary(ohio_mi %&amp;gt;% 
          filter(state == &amp;quot;MI&amp;quot;) %&amp;gt;% 
          .$percollege)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   11.31   14.61   17.43   19.42   21.31   48.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see Ohio appears to have slightly less college educated adults than Michigan but the graphic doesn’t tell us if it is statistically significant or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(ohio_mi, aes(x = state, y = percollege, fill = state)) +
        geom_boxplot(alpha = 0.5) +
        scale_fill_manual(values = c(&amp;quot;#00274C&amp;quot;, &amp;quot;#BB0000&amp;quot;)) +
        theme_minimal() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/Hypotheses_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also once again see similar skewness within the sample distributions by running the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;- ggplot(ohio_mi, aes(x = percollege)) +
        geom_histogram(aes(fill = ..count..), color = &amp;quot;#ccccaa&amp;quot;) +
        scale_fill_gradient(&amp;quot;Frequency&amp;quot;, low = &amp;quot;#5cb85c&amp;quot;, high = &amp;quot;#428bca&amp;quot;) +
        facet_wrap(~ state) +
        theme_minimal() +
        theme(legend.position = &amp;quot;bottom&amp;quot;,
              legend.direction = &amp;quot;horizontal&amp;quot;)
    

p4 &amp;lt;- ggplot(ohio_mi, aes(x = percollege)) +
        geom_histogram(aes(fill = ..count..), color = &amp;quot;#ccccaa&amp;quot;) +
        scale_fill_gradient(&amp;quot;Frequency&amp;quot;, low = &amp;quot;#5cb85c&amp;quot;, high = &amp;quot;#428bca&amp;quot;) +
        facet_wrap(~ state) + 
        scale_x_log10() +
        theme_minimal() +
        theme(legend.position = &amp;quot;bottom&amp;quot;,
              legend.direction = &amp;quot;horizontal&amp;quot;)

p3 + p4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/Hypotheses_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to the plots in the one-sample example, to test if the Ohio and Michigan averages differ we’ll perform two tests. Also, note that we am searching for any differences between the means rather than if one is specifically less than or greater than the other. First we test with a normal &lt;em&gt;t&lt;/em&gt;-test without any distribution transformations. The results below show a &lt;em&gt;p&lt;/em&gt;-value &amp;lt; 0.01 supporting the alternative hypothesis that “true difference in means is not equal to 0”; essentially it states there is a statistical difference between the two means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(percollege ~ state, data = ohio_mi)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  percollege by state
## t = 2.5953, df = 161.27, p-value = 0.01032
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.6051571 4.4568579
## sample estimates:
## mean in group MI mean in group OH 
##         19.42146         16.89045&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again as an alternative, due to the non-normality concerns we can perform this test in another way to ensure our results are not being biased due to assumption violations. We can perform the &lt;em&gt;t&lt;/em&gt;-test and transform our data. The results support our initial conclusion that the percent of college educated adults in Ohio is statistically different than the percent in Michigan.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(log(percollege) ~ state, data = ohio_mi)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  log(percollege) by state
## t = 2.9556, df = 168.98, p-value = 0.003567
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.04724892 0.23732151
## sample estimates:
## mean in group MI mean in group OH 
##         2.915873         2.773587&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-paired-t-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Paired &lt;em&gt;t&lt;/em&gt;-test&lt;/h3&gt;
&lt;p&gt;To illustrate the paired &lt;em&gt;t&lt;/em&gt;-test we’ll use the built-in &lt;code&gt;sleep&lt;/code&gt; data set.&lt;/p&gt;
&lt;p&gt;Please load up the following packages&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    extra group ID
## 1    0.7     1  1
## 2   -1.6     1  2
## 3   -0.2     1  3
## 4   -1.2     1  4
## 5   -0.1     1  5
## 6    3.4     1  6
## 7    3.7     1  7
## 8    0.8     1  8
## 9    0.0     1  9
## 10   2.0     1 10
## 11   1.9     2  1
## 12   0.8     2  2
## 13   1.1     2  3
## 14   0.1     2  4
## 15  -0.1     2  5
## 16   4.4     2  6
## 17   5.5     2  7
## 18   1.6     2  8
## 19   4.6     2  9
## 20   3.4     2 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sleep, aes(group, extra, fill = group)) +
        geom_boxplot(alpha = 0.5) +
        scale_fill_manual(values = c(&amp;quot;#00274C&amp;quot;, &amp;quot;#BB0000&amp;quot;)) +
        theme_minimal() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/Hypotheses_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case we are assessing if there is a statistically significant effect of a particular drug on sleep (increase in hours of sleep compared to control) for 10 patients. Please see &lt;code&gt;?sleep&lt;/code&gt; for more details on the variables.&lt;/p&gt;
&lt;p&gt;We want to see if the mean values for the extra variable differs between group 1 and group 2. Here, we perform the &lt;code&gt;t.test&lt;/code&gt; as in the previous sections but just add the &lt;code&gt;paired = TRUE&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(extra ~ group, data = sleep, paired = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  extra by group
## t = -4.0621, df = 9, p-value = 0.002833
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.4598858 -0.7001142
## sample estimates:
## mean of the differences 
##                   -1.58&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example it appears that the drug does have an effect as the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value = 0.0028 suggesting that the drug increases sleep on average by 1.58 hours.&lt;/p&gt;
&lt;p&gt;The above looks at exact matching. If you are getting an error, try running which will only run on values that can be matched. To use the command below, make sure both comparison variables are numeric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep &amp;lt;- sleep %&amp;gt;%
  mutate(group = as.numeric(group))

t.test(sleep$extra, sleep$group, paired = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  sleep$extra and sleep$group
## t = 0.095569, df = 19, p-value = 0.9249
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.8360223  0.9160223
## sample estimates:
## mean of the differences 
##                    0.04&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bivariate Tables</title>
      <link>/lesson/11-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/11-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-notes&#34;&gt;In Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#textbook-class-notes&#34;&gt;Textbook Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough-comparing-groups&#34;&gt;R Walkthrough: Comparing Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;in-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%2011/Slides-Week-11.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 11/Slides-Week-11.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%2011/Slides-Week-11.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;textbook-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Textbook Class Notes&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%2011/SSDS_Ch9.PPTX&#34; target=&#34;_blank&#34;&gt;Ch 9 PowerPoint&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week11/SSDS_Ch9.docx&#34; target=&#34;_blank&#34;&gt;Ch 9 Lecture Notes&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough-comparing-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough: Comparing Groups&lt;/h3&gt;
&lt;p&gt;Click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%209/Slides-Week-9R.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 11/Slides-Week-11R.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;/slides/Week%2011/predimed_codebook.csv&#34; target=&#34;_blank&#34;&gt;PREDIMED Codebook&lt;/a&gt; used in the slideshow.&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%2011/Slides-Week-11R.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%2011/Slides-Week-11R.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Chi-Square Test and Measures of Association</title>
      <link>/lesson/12-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/12-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-notes&#34;&gt;In Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#textbook-class-notes&#34;&gt;Textbook Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough-chi-square&#34;&gt;R Walkthrough: Chi Square&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;in-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%2012/Slides-Week-12.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 12/Slides-Week-12.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%2012/Slides-Week-12.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;textbook-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Textbook Class Notes&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%2012/SSDS_Ch10.PPTX&#34; target=&#34;_blank&#34;&gt;PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week12/SSDS_Ch10.docx&#34; target=&#34;_blank&#34;&gt;Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough-chi-square&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough: Chi Square&lt;/h3&gt;
&lt;p&gt;Take a look at a &lt;a href=&#34;/lesson/chisquare/&#34;&gt;walkthrough&lt;/a&gt; about the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistic.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 12: Tidying Up the Chi Square Statistics</title>
      <link>/lesson/chisquare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/chisquare/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#purpose&#34;&gt;Purpose&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objectives&#34;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#its-all-about-the-assumptions&#34;&gt;It’s All About the Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-general-social-survey-gss&#34;&gt;The General Social Survey (GSS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-some-of-the-gss-data-set&#34;&gt;Getting (Some of) the GSS Data Set:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#purpose-1&#34;&gt;Purpose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-wrangling&#34;&gt;Data Wrangling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up&#34;&gt;Setting Up&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis&#34;&gt;Hypothesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions&#34;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis&#34;&gt;Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#analytically&#34;&gt;Analytically&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#programatically&#34;&gt;Programatically&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-and-conclusion&#34;&gt;Interpretation and Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#your-turn&#34;&gt;Your turn!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;While we can certainly perform statistical analysis in Base R, it is far more dynamic when we use the &lt;code&gt;tidyverse&lt;/code&gt; package. In this walk through, we’ll explore chi-square statistics using the &lt;code&gt;infer&lt;/code&gt; package which conducts statistical inference and plays well with &lt;code&gt;tidyverse&lt;/code&gt; design framework.&lt;/p&gt;
&lt;div id=&#34;objectives&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Objectives&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;introduce the &lt;code&gt;infer&lt;/code&gt; and &lt;code&gt;patchwork&lt;/code&gt; packages&lt;/li&gt;
&lt;li&gt;apply the Chi square statistic&lt;/li&gt;
&lt;li&gt;provide some familiarity with the General Social Survey&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages&lt;/h3&gt;
&lt;p&gt;Please load up the following packages&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.4     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter()     masks stats::filter()
## x dplyr::group_rows() masks kableExtra::group_rows()
## x dplyr::lag()        masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(infer)
library(patchwork)
library(viridis)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to download them if you receive an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;infer&amp;quot;)
install.packages(&amp;quot;patchwork&amp;quot;)
install.packages(&amp;quot;viridis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;its-all-about-the-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;It’s All About the Assumptions&lt;/h2&gt;
&lt;p&gt;Before we can move forward in the exploration of the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; statistic, we must first make sure that any data that we use adheres to the following checklist, which are formally known as , or just .&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;: The variables are categorical and can be categorized in to one of the three standard types:
&lt;ul&gt;
&lt;li&gt;Dichotomous: Two (2) groups (e.g. Male and Female).&lt;/li&gt;
&lt;li&gt;Nominal: Three (3) or more categorical groups (e.g. undergraduate, graduate student, postdoctoral scholar, professor).&lt;/li&gt;
&lt;li&gt;Ordinal: ordered groups (e.g. Pain Level 1, Pain Level 2, Pain Level 3, …).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;: Observations are independent of one another (e.g. no relationship between any of the cases).&lt;/li&gt;
&lt;li&gt;: Categorical variable groupings must be mutually exclusive (e.g. a participant cannot be both a Democrat and Republican).&lt;/li&gt;
&lt;li&gt;: There must be at minimum five (5) expected frequencies in each group of your categorical variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NOTE: If you do not meet ALL of the assumptions for any statistical test, this violation changes the conclusion of the research and interpretation of the results of any analysis which is be misleading or complete nonsense, or analytical garbage. So please please please make sure you account for all assumptions! Generally, most tests that are used the same four assumptions about the data set being analyzed (see ). The four may not be the only ones, but they are typically observed in many commonly used approaches.&lt;/p&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;p&gt;OK you have that in mind? Now let’s go exploring!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;div id=&#34;the-general-social-survey-gss&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The General Social Survey (GSS)&lt;/h3&gt;
&lt;p&gt;Funded by the &lt;a href=&#34;https://www.nsf.gov/pubs/2008/nsf08506/nsf08506.htm&#34; target=&#34;_blank&#34;&gt;National Science Foundation (NSF)&lt;/a&gt;, the General Social Survey (GSS) is a social science oriented opinion based survey that has been regularly administered since 1972 by the &lt;a href=&#34;https://www.norc.org&#34; target=&#34;_blank&#34;&gt;National Opinion Research Center (NORC)&lt;/a&gt; at the University of Chicago.&lt;/p&gt;
&lt;p&gt;The GSS provides two pieces of key information about the American society at a given point in time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It gathers data on our contemporary society to assess and describe trends and constants in key areas related to people’s attitudes, behaviors, and attributes.&lt;/li&gt;
&lt;li&gt;It contains a core of demographic, behavioral, attitudinal questions, and those deemed important at the time. Topics that describe the latter include civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.
&lt;a href=&#34;https://gss.norc.org/About-The-GSS&#34; target=&#34;_blank&#34;&gt;(NORC at the University of Chicago, 2016)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The de-identified variant, or the public data set is free to use by anyone. As far as “big data” that is also longitudinal, the GSS is a fantastic source of information about how the views of those in the United States has or has not shifted since 1972.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-some-of-the-gss-data-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting (Some of) the GSS Data Set:&lt;/h3&gt;
&lt;p&gt;We are only using a select portion of the GSS because the data set is humongous! &lt;code&gt;R&lt;/code&gt; is really good at getting analyses performed in a logical and comprehensive way but what it has in processing power, it lacks in handling “big data” sets. For the processing of very large data sets, you really need a software like &lt;code&gt;Python&lt;/code&gt;. With that said, &lt;code&gt;R&lt;/code&gt; can still handle a lot. It is rare to come across a data set that &lt;code&gt;R&lt;/code&gt; cannot handle well, but the entire GSS is one of the few. Of course other criteria such as a computer’s memory, disk space, internet speed, etc. also limit &lt;code&gt;R&lt;/code&gt; or any other software package from loading and analyzing any data set.&lt;/p&gt;
&lt;p&gt;Rather than having you load an external file into R Studio, we’ll do it via the web. Run the following command that uses the &lt;code&gt;read_csv()&lt;/code&gt; command to grab and load external data sets from an external website. In this case it is a site called &lt;a href=&#34;%22https://github.com/%22&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;, namely from a repository from my public site (which doesn’t actually have much of anything publicly available).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_nasa &amp;lt;- &amp;quot;https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/gss_nasa.csv&amp;quot; %&amp;gt;%
  read_csv()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   .default = col_character(),
##   id = col_double(),
##   year = col_double(),
##   age = col_double(),
##   consci = col_double(),
##   oversamp = col_double()
## )
## ℹ Use `spec()` for the full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;purpose-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Purpose&lt;/h3&gt;
&lt;p&gt;In this walk through, we’ll be assessing if funding for space exploration is a partisan oriented issue. Many data sets include more information than you need to answer your questions, some of which have nothing to do with your goals. Let’s see what this data set has:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(gss_nasa)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;id&amp;quot;       &amp;quot;year&amp;quot;     &amp;quot;age&amp;quot;      &amp;quot;class&amp;quot;    &amp;quot;degree&amp;quot;   &amp;quot;sex&amp;quot;     
##  [7] &amp;quot;marital&amp;quot;  &amp;quot;race&amp;quot;     &amp;quot;region&amp;quot;   &amp;quot;partyid&amp;quot;  &amp;quot;happy&amp;quot;    &amp;quot;relig&amp;quot;   
## [13] &amp;quot;cappun&amp;quot;   &amp;quot;finalter&amp;quot; &amp;quot;natspac&amp;quot;  &amp;quot;natarms&amp;quot;  &amp;quot;conclerg&amp;quot; &amp;quot;confed&amp;quot;  
## [19] &amp;quot;conpress&amp;quot; &amp;quot;conjudge&amp;quot; &amp;quot;consci&amp;quot;   &amp;quot;conlegis&amp;quot; &amp;quot;zodiac&amp;quot;   &amp;quot;oversamp&amp;quot;
## [25] &amp;quot;postlife&amp;quot; &amp;quot;party&amp;quot;    &amp;quot;space&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Wrangling&lt;/h3&gt;
&lt;p&gt;You should never assess a data set without a codebook. The one for the GSS is pretty large and can be found &lt;a href=&#34;%22https://gss.norc.org/documents/codebook/gss_codebook.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;, but for the purposes of this walk through, we’ll be looking at the fields &lt;code&gt;party&lt;/code&gt; and &lt;code&gt;space&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_select &amp;lt;- gss_nasa %&amp;gt;%
  select(party, space)

gss_select %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   party space      
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;      
## 1 Ind   TOO LITTLE 
## 2 Ind   ABOUT RIGHT
## 3 Dem   ABOUT RIGHT
## 4 Ind   TOO LITTLE 
## 5 Ind   TOO MUCH   
## 6 Ind   TOO LITTLE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK great but what about the choices? How would we know what factors make up each vector? Well we can do this using the command &lt;code&gt;unique()&lt;/code&gt; by&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_select %&amp;gt;%
  select(party) %&amp;gt;%
  unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 1
##   party
##   &amp;lt;chr&amp;gt;
## 1 Ind  
## 2 Dem  
## 3 Rep&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_select %&amp;gt;%
  select(space) %&amp;gt;%
  unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 1
##   space      
##   &amp;lt;chr&amp;gt;      
## 1 TOO LITTLE 
## 2 ABOUT RIGHT
## 3 TOO MUCH&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that people identified themselves as either Democrat (&lt;code&gt;Dem&lt;/code&gt;), Independent (&lt;code&gt;Ind&lt;/code&gt;), or Republican (&lt;code&gt;Rep&lt;/code&gt;) and they could decide if the funding for space exploration was &lt;code&gt;TOO LITTLE&lt;/code&gt;, &lt;code&gt;ABOUT RIGHT&lt;/code&gt;, or &lt;code&gt;TOO MUCH&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now is probably a good time to visualize the data so that we can simply get an idea of any disparity between the views of those within each political organization, if it exists.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_select %&amp;gt;% 
  ggplot(aes(x=party, fill = space)) + 
  geom_bar() +
  scale_fill_viridis(discrete = TRUE) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figurename1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/lesson/chiSquare_files/figure-html/figurename1-1.png&#34; alt=&#34;Bar plot by frequencies.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Bar plot by frequencies.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Well that is nice but the legend is out of order. Let’s fix that!&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let’s first get information about the format of each vector.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(gss_select)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [149 × 2] (S3: tbl_df/tbl/data.frame)
##  $ party: chr [1:149] &amp;quot;Ind&amp;quot; &amp;quot;Ind&amp;quot; &amp;quot;Dem&amp;quot; &amp;quot;Ind&amp;quot; ...
##  $ space: chr [1:149] &amp;quot;TOO LITTLE&amp;quot; &amp;quot;ABOUT RIGHT&amp;quot; &amp;quot;ABOUT RIGHT&amp;quot; &amp;quot;TOO LITTLE&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well it looks like we have two character vectors. We can’t do much with the order of those types of variables. This is where the variable type &lt;strong&gt;factor&lt;/strong&gt; comes in handy. So what are they? It is how we store truly categorical information in R. In general, the levels are friendly human-readable character strings, like “male/female/transgender/other”, “control/treatment”, etc. The values a factor can take on are called the &lt;strong&gt;levels&lt;/strong&gt;. For example, in the Gapminder data set, the levels of the factor &lt;code&gt;continent&lt;/code&gt; were &lt;code&gt;Africa&lt;/code&gt;, &lt;code&gt;Americas&lt;/code&gt;, &lt;code&gt;Asia&lt;/code&gt;, &lt;code&gt;Europe&lt;/code&gt;, and &lt;code&gt;Oceania&lt;/code&gt;. To verify, simply load the &lt;code&gt;gapminder&lt;/code&gt; package and run the command &lt;code&gt;levels()&lt;/code&gt; on the &lt;code&gt;continent&lt;/code&gt; vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gapminder)

levels(gapminder$continent)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Africa&amp;quot;   &amp;quot;Americas&amp;quot; &amp;quot;Asia&amp;quot;     &amp;quot;Europe&amp;quot;   &amp;quot;Oceania&amp;quot;&lt;/code&gt;&lt;/pre&gt;

&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Converts any vector we want to reorder into factors:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_factors &amp;lt;- gss_select %&amp;gt;%
  mutate(party = as.factor(party)) %&amp;gt;%
  mutate(space = as.factor(space))&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check the current factor orders.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_factors %&amp;gt;%
  pull(party) %&amp;gt;% 
  levels()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Dem&amp;quot; &amp;quot;Ind&amp;quot; &amp;quot;Rep&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_factors %&amp;gt;%
  pull(space) %&amp;gt;% 
  levels()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ABOUT RIGHT&amp;quot; &amp;quot;TOO LITTLE&amp;quot;  &amp;quot;TOO MUCH&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note in the above two commands that the &lt;code&gt;pull()&lt;/code&gt; command is used because &lt;code&gt;levels()&lt;/code&gt; expects a vector. Using &lt;code&gt;select()&lt;/code&gt; would have kept the &lt;code&gt;space&lt;/code&gt; column as a data frame. An alternative approach would be to use Base R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(gss_factors$party)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Dem&amp;quot; &amp;quot;Ind&amp;quot; &amp;quot;Rep&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(gss_factors$space)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ABOUT RIGHT&amp;quot; &amp;quot;TOO LITTLE&amp;quot;  &amp;quot;TOO MUCH&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have noticed by now that the default factor order is in its natural positioning, which in this case is in alphabetical order. While this appears fine for some items, its probably better to report items in the order they normally are. In the case of party affiliation, we should have Democrats (&lt;code&gt;Dem&lt;/code&gt;), Republicans (&lt;code&gt;Rep&lt;/code&gt;), and then Independents(&lt;code&gt;Ind&lt;/code&gt;) and their possible choices of &lt;code&gt;TOO MUCH&lt;/code&gt;,&lt;code&gt;ABOUT RIGHT&lt;/code&gt;, and &lt;code&gt;TOO LITTLE&lt;/code&gt;. Of course how you order factors is absolutely dependent on each situation. Sometimes its for reporting but other times its simply for aesthetic reasons.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Reorder the factor levels&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels &amp;lt;- gss_factors %&amp;gt;%
  mutate(party = factor(party, levels = c(&amp;quot;Dem&amp;quot;, &amp;quot;Rep&amp;quot;, &amp;quot;Ind&amp;quot;))) %&amp;gt;%
  mutate(space = factor(space, levels = c(&amp;quot;TOO MUCH&amp;quot;, &amp;quot;ABOUT RIGHT&amp;quot;, &amp;quot;TOO LITTLE&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use &lt;code&gt;factor()&lt;/code&gt; on the columns &lt;code&gt;party&lt;/code&gt; and &lt;code&gt;space&lt;/code&gt; and then define the order we want by setting &lt;code&gt;levels =&lt;/code&gt;. Let’s check them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;%
  pull(space) %&amp;gt;%
  levels()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;TOO MUCH&amp;quot;    &amp;quot;ABOUT RIGHT&amp;quot; &amp;quot;TOO LITTLE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;%
  pull(party) %&amp;gt;%
  levels()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Dem&amp;quot; &amp;quot;Rep&amp;quot; &amp;quot;Ind&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That looks right!&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Plot with the new factor levels&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;% 
  ggplot(aes(x=party, fill = space)) + 
  geom_bar() +
  scale_fill_viridis(discrete = TRUE) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figurename2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/lesson/chiSquare_files/figure-html/figurename2-1.png&#34; alt=&#34;Ordered bar plot by frequencies.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Ordered bar plot by frequencies.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;OK now it plots in the correctly in the order we want. However, the problem now lies in the fact that its hard to compare the three bar plots since they aren’t on equal footing. This is the problem with frequency data, in that it can be misleading when we’re looking for trends between categorical variables within groups. To counteract this, let’s normalize them by looking at each chunk of each bar as a percent using the command &lt;code&gt;position = fill&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;% 
  ggplot(aes(x=party, fill = space)) + 
  geom_bar(position = &amp;quot;fill&amp;quot;) +
  scale_fill_viridis(discrete = TRUE) +
  theme_minimal() + 
  ylab(&amp;quot;within group percentage&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figurename3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/lesson/chiSquare_files/figure-html/figurename3-1.png&#34; alt=&#34;Ordered bar plot by percent total.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Ordered bar plot by percent total.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;That is so much nicer! However, it doesn’t really look like there is much of a difference in how Democrats, Republicans, and Independents support space exploration/ Remember that the bar plot represents descriptive statistics but to infer any differences, we must use the aptly named inferential statistics.&lt;/p&gt;
&lt;p&gt;Let’s drill down into this with some hypothesis testing, comparing Base R and the &lt;code&gt;infer&lt;/code&gt; package. What we essentially have is a contingency table of party affiliation and attitude towards space exploration, and we want to see if there’s a relationship between these variables. The Chi Squared Test of independence is used to determine if a significant relationship exists between two categorical variables, so we will use this test.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting Up&lt;/h2&gt;
&lt;div id=&#34;hypothesis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hypothesis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Null hypothesis: There is no relationship between party (Democrat, Republican, Independent) and attitude towards space exploration (too little, about right, too much).&lt;/li&gt;
&lt;li&gt;Alternative hypothesis: There is a relationship between party (Democrat, Republican, Independent) and attitude towards space exploration (too little, about right, too much).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OK first, do we meet the assumptions?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assumptions&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Clearly both &lt;code&gt;party&lt;/code&gt; and &lt;code&gt;space&lt;/code&gt; are categorical.&lt;/li&gt;
&lt;li&gt;Observations are clearly independent in that &lt;code&gt;party&lt;/code&gt; and &lt;code&gt;space&lt;/code&gt; are measure two very different concepts.&lt;/li&gt;
&lt;li&gt;Respondents had to choose between being Democrats, Republicans, or Independents and their corresponding viewpoints were delineated between too little, about right, or too much. These are mutually exclusive “events” or values.&lt;/li&gt;
&lt;li&gt;We have more than five cases for each. In fact, we have&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;%
   count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       n
##   &amp;lt;int&amp;gt;
## 1   149&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or if you want to get counts of each value, run&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  party            space   
##  Dem:43   TOO MUCH   :43  
##  Rep:34   ABOUT RIGHT:76  
##  Ind:72   TOO LITTLE :30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In any case, we more than meet the minimum threshold.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;There are two main ways to solve this problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analytically&lt;/li&gt;
&lt;li&gt;Programatically&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;analytically&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Analytically&lt;/h3&gt;
&lt;p&gt;We assume the expected values follow a Chi-squared distribution, with a probability density function that depends on the degrees of freedom. Looking at the plot below, observe how the distribution varies with the degrees of freedom (&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;) on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis is the Chi-squared statistic, which we can calculate in R.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:figurename4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/lesson/chiSquare_files/figure-html/figurename4-1.png&#34; alt=&#34;Chi-squre distribution constructed in R (code availible upon request).&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Chi-squre distribution constructed in R (code availible upon request).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We could then see where it falls in the distribution, and observe the probability of arriving at that combination of variables, or a more extreme example. As our Chi-squared test statistic increases, we move further along the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis to the right. There is less area under the curve to the right, and our &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value (the area under the curve to the right of the observed statistic) decreases.&lt;/p&gt;
&lt;p&gt;Generally speaking, a larger Chi-squared statistic suggests stronger evidence for rejecting our null hypothesis. If we observe a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value &lt;span class=&#34;math inline&#34;&gt;\(\leq 0.05\)&lt;/span&gt;, we would reject our null hypothesis.&lt;/p&gt;
&lt;p&gt;What would it mean to accept our alternative hypothesis?&lt;/p&gt;
&lt;p&gt;In the case of our example, if we we lived in a completely random universe, less than 5% of the time we would arrive at the particular combination of &lt;code&gt;party&lt;/code&gt; and attitude towards &lt;code&gt;space&lt;/code&gt; exploration we observe in our data. In other words, the relationship between party and attitude towards space exploration we see in our data is .&lt;/p&gt;
&lt;p&gt;But we don’t live in a completely random universe - think about getting randomly married to someone else - so the real question still remains:  Well we can use Base R’s Chi-squared test to find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chisq.test(gss_newlevels$party, gss_newlevels$space)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s Chi-squared test
## 
## data:  gss_newlevels$party and gss_newlevels$space
## X-squared = 1.3261, df = 4, p-value = 0.8569&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s save this observed Chi statistic for later use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;observed_stat &amp;lt;- chisq.test(gss_newlevels$party, gss_newlevels$space)$stat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might be tempted to look at this and say, there’s a high &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value. No significant relationship exists. So we’re done! This is what we expected looking at the bar plots earlier! Well not so fast? Let’s look at it programatically.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;programatically&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Programatically&lt;/h3&gt;
&lt;p&gt;Another way to test if there is a significant relationship in our data is to take a programmatic approach. Basically the idea here is if we live in world where variables are totally unrelated, they might as well have been randomly put together. Basically in the real world, yes of course things (variables) are related! Someone’s party affiliation is predicated on their history and maturation and that then most likely informs their views on funding, especially for an area of the sciences like space exploration. So does the data we have look more like random or normal world?&lt;/p&gt;
&lt;p&gt;Let’s explore this by taking one of the columns of our data frame and scrambling it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;%
  mutate(random_1 = sample(space),
         random_2 = sample(space))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 149 x 4
##    party space       random_1    random_2   
##    &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;      
##  1 Ind   TOO LITTLE  ABOUT RIGHT TOO MUCH   
##  2 Ind   ABOUT RIGHT TOO MUCH    ABOUT RIGHT
##  3 Dem   ABOUT RIGHT TOO LITTLE  TOO LITTLE 
##  4 Ind   TOO LITTLE  TOO LITTLE  TOO MUCH   
##  5 Ind   TOO MUCH    TOO LITTLE  ABOUT RIGHT
##  6 Ind   TOO LITTLE  ABOUT RIGHT TOO MUCH   
##  7 Ind   ABOUT RIGHT ABOUT RIGHT ABOUT RIGHT
##  8 Dem   ABOUT RIGHT ABOUT RIGHT ABOUT RIGHT
##  9 Dem   TOO LITTLE  TOO MUCH    ABOUT RIGHT
## 10 Ind   TOO LITTLE  ABOUT RIGHT ABOUT RIGHT
## # … with 139 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last two columns are distributions of random samples from the available choices within &lt;code&gt;space&lt;/code&gt; and they represent what we would expect to see if the relationship between variables was completely random. We could generate many, many permutations, calculate an Chi-squared statistic for each, and we would expect their distribution to approach the density functions shown above. Then we could plot our data on that distribution and see where it fell. If the area under the curve to the right of the point was less than 5%, we could reject the null hypothesis.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:figurename5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;graphics/infer.png&#34; alt=&#34;Inferential testing laid out in the infer package.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Inferential testing laid out in the infer package.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-definitions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some definitions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;specify()&lt;/code&gt; is like &lt;code&gt;dplyr::select()&lt;/code&gt;: choose the variables from your data frame to test&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hypothesize()&lt;/code&gt; is where we select the null hypothesis&lt;/li&gt;
&lt;li&gt;&lt;code&gt;generate()&lt;/code&gt; creates randomized values form a predefined set of values&lt;/li&gt;
&lt;li&gt;&lt;code&gt;calculate()&lt;/code&gt; lets you choose what test statistic to calculate&lt;/li&gt;
&lt;li&gt;&lt;code&gt;visualize()&lt;/code&gt; automatically plots permuted with ggplot, making it easy to edit as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;benefits&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Benefits&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;inputs and outputs are both data frames&lt;/li&gt;
&lt;li&gt;composing tests with pipes&lt;/li&gt;
&lt;li&gt;reading an inferential chain describes an inferential procedure&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;% 
  specify(space ~ party)  %&amp;gt;% 
  hypothesize(null = &amp;quot;independence&amp;quot;) %&amp;gt;% 
  generate(reps = 1000, type = &amp;quot;permute&amp;quot;) %&amp;gt;% 
  calculate(stat = &amp;quot;Chisq&amp;quot;) %&amp;gt;% 
  visualize() +
  geom_vline(aes(xintercept = observed_stat), color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figurename6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/lesson/chiSquare_files/figure-html/figurename6-1.png&#34; alt=&#34;Visualization of the gss_newlevels data set using infer.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Visualization of the gss_newlevels data set using infer.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If we wanted to get a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value from this programmatic approach, we can calculate the area under the curve to the right of the observed statistic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_newlevels %&amp;gt;% 
  specify(space ~ party)  %&amp;gt;% 
  hypothesize(null = &amp;quot;independence&amp;quot;) %&amp;gt;% 
  generate(reps = 1000, type = &amp;quot;permute&amp;quot;) %&amp;gt;% 
  calculate(stat = &amp;quot;Chisq&amp;quot;) %&amp;gt;% 
  summarise(p_val = mean(stat &amp;gt; observed_stat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   p_val
##   &amp;lt;dbl&amp;gt;
## 1 0.859&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation-and-conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation and Conclusion&lt;/h3&gt;
&lt;p&gt;So when we compare the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value of this simulated data set with that of our real-world one, it sure looks like they are nearly identical. So It looks like a significant relationship does not exist so there isn’t a measurable difference between one’s party affiliation and their attitude towards space exploration funding.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;your-turn&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Your turn!&lt;/h2&gt;
&lt;p&gt;You may do this independently or in groups of two.&lt;/p&gt;
&lt;p&gt;Download both of the following items:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;GSS Extract Data Set.csv&lt;/code&gt; data set and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GSS Extract Codebook.csv&lt;/code&gt; code book&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;from my Github site using the following site addresses&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/GSS%20Extract%20Data%20Set.csv&#34; class=&#34;uri&#34;&gt;https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/GSS%20Extract%20Data%20Set.csv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/GSS%20Extract%20Codebook.csv&#34; class=&#34;uri&#34;&gt;https://raw.githubusercontent.com/piechartssuck/RWDataSets/master/GSS%20Extract%20Codebook.csv&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You are tasked to find if there is a relationship between a respondent and their views on science. This is exploratory, so it is incumbent on you to&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;define the input and output variables.&lt;/li&gt;
&lt;li&gt;make a clear justification of why both are reasonable measures in the real world.&lt;/li&gt;
&lt;li&gt;create a null and alternative hypothesis&lt;/li&gt;
&lt;li&gt;perform a chi-squared test&lt;/li&gt;
&lt;li&gt;interpret the results&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is how real world data explorations work! There is no expectation that you will find anything at all or that you are expected to discover some grand connection. The points of this exploration are: Can you&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;identify important variables in a real-world data set?&lt;/li&gt;
&lt;li&gt;formulate a question about them off based of a vague request?&lt;/li&gt;
&lt;li&gt;analyze the data in an appropriate manner?&lt;/li&gt;
&lt;li&gt;draw conclusions and report them?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chi-Square and ANOVAs</title>
      <link>/lesson/13-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/13-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-notes&#34;&gt;In Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#textbook-class-notes&#34;&gt;Textbook Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#jamovi&#34;&gt;Jamovi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;in-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%2013/Slides-Week-13.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 13/Slides-Week-13.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%2013/Slides-Week-13.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;textbook-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Textbook Class Notes&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%2013/SSDS_Ch12.pdf&#34; target=&#34;_blank&#34;&gt;Ch 12 PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week13/SSDS_Ch12.pdf&#34; target=&#34;_blank&#34;&gt;Ch 12 Outline&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;jamovi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Jamovi&lt;/h3&gt;
&lt;p&gt;Download and explore the &lt;a href=&#34;https://www.jamovi.org/&#34; target=&#34;_blank&#34;&gt;jamovi&lt;/a&gt; software that uses the power and open source abilities of R in an environment that resembles SPSS.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis</title>
      <link>/lesson/15-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/15-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough&#34;&gt;R Walkthrough&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;r-walkthrough&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough&lt;/h3&gt;
&lt;p&gt;Take a look at a fun &lt;a href=&#34;/lesson/SentimentAnalysisIntro/&#34;&gt;walkthrough&lt;/a&gt; about applying statistics and machine learning to perform a sentiment analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The What and Why of Statistics</title>
      <link>/lesson/01-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/01-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The first &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due the Wednesday following the completion of the Week 1 class. It provides an introduction to the structure in and elementary use of R with an end goal of getting you familiar with its functionality. In particular the sections address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intro to basics&lt;/li&gt;
&lt;li&gt;Vectors&lt;/li&gt;
&lt;li&gt;Matrices&lt;/li&gt;
&lt;li&gt;Factors&lt;/li&gt;
&lt;li&gt;Data frames&lt;/li&gt;
&lt;li&gt;Lists&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As noted last week, there is a lot on there but please refer to the &lt;a href=&#34;/syllabus/&#34;&gt;syllabus&lt;/a&gt; for the grading policy regarding those tasks. Just remember, this is not a programming class!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%201/SSDS_Ch1.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 1 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%201/SSDS_Ch1.docx&#34; target=&#34;_blank&#34;&gt;Chapter 1 Lecture Notes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;p&gt;Download the in-class slides via&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%201/ICS_Ch1.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%201/ICN_Ch1.pdf&#34; target=&#34;_blank&#34;&gt;Outline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Organization and Graphic Presentation of Data</title>
      <link>/lesson/02-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/02-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough&#34;&gt;R Walkthrough&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The second &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due the Wednesday following the completion of the Week 2 class. It covers the tidyverse family of packages in R which we will be using throughout the remainder of the course. In particular the sections address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data wrangling&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data visualization&lt;/li&gt;
&lt;li&gt;Grouping and summarizing&lt;/li&gt;
&lt;li&gt;Types of visualizations&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%202/SSDS_Ch2.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 2 PowerPoint&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%202/SSDS_Ch2.docx&#34; target=&#34;_blank&#34;&gt;Chapter 2 Lecture Notes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;p&gt;Download the in-class slides via&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%202/ICS_Ch2.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%202/ICN_Ch2.pdf&#34; target=&#34;_blank&#34;&gt;Outline&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough&lt;/h3&gt;
&lt;p&gt;First click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%202/Slides-Week-2.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 2/Slides-Week-2.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download a &lt;a href=&#34;/slides/Week%202/Slides-Week-2.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Please get used to this term!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Measures of Central Tendency</title>
      <link>/lesson/03-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/03-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough&#34;&gt;R Walkthrough&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The third &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due the Wednesday following the completion of the Week 3 class. It covers a particular tidyverse package called &lt;code&gt;dplyr&lt;/code&gt; which is by far the one of the most useful tools you can use. In particular the sections address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transforming Data with dplyr&lt;/li&gt;
&lt;li&gt;Aggregating Data&lt;/li&gt;
&lt;li&gt;Selecting and Transforming Data&lt;/li&gt;
&lt;li&gt;Case Study: The babynames Dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This now sets us up to do meaningful statistics in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%203/SSDS_Ch3.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 3 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%203/SSDS_Ch3.docx&#34; target=&#34;_blank&#34;&gt;Chapter 3 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;p&gt;Download the class&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%203/ICS_Ch3.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%203/ICN_Ch3.pdf&#34; target=&#34;_blank&#34;&gt;Outline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Download the data sets&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%203/Week3data.zip&#34; target=&#34;_blank&#34;&gt;Week 3 Walkthrough Materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Click on the presentation itself and then you may&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%203/Slides-Week-3.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 3/Slides-Week-3.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download a &lt;a href=&#34;/slides/Week%203/Slides-Week-3.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You will have to unzip this file. If you are unfamilair with this process, please check the &lt;a href=&#34;/resource/unzipping/&#34;&gt;Unzipping files&lt;/a&gt; section under Resources for assistance.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Measures of Variability</title>
      <link>/lesson/04-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/04-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough---doing-the-class-notes&#34;&gt;R Walkthrough - Doing the Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The fourth &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due the Wednesday following the completion of the Week 4 class. The module covers doing basic descriptive statistics in R out of the box&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In particular, the sections are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variables&lt;/li&gt;
&lt;li&gt;Histograms and Distributions&lt;/li&gt;
&lt;li&gt;Scales of Measurement&lt;/li&gt;
&lt;li&gt;Measures of Central Tendency&lt;/li&gt;
&lt;li&gt;Measures of Variability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As promised, we start performing basic statistical analyses in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%204/SSDS_Ch4.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 4 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%204/SSDS_Ch4.docx&#34; target=&#34;_blank&#34;&gt;Chapter 4 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Posted after class&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%204/ICS_Ch4.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%204/ICN_Ch4.pdf&#34; target=&#34;_blank&#34;&gt;Outline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough---doing-the-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough - Doing the Class Notes&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Posted after class&lt;/em&gt;&lt;/p&gt;
&lt;!--
1. Download the data sets^[You will have to unzip this file. If you are unfamilair with this process, please check the [Unzipping files](/resource/unzipping/) section under Resources for assistance.] and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.

- [Week 3 Walkthrough Materials](/slides/Week 4/Week4data.zip){target=&#34;_blank&#34;}

2. Click on the presentation itself and then you may

- Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.
- Press the letter **O** at any point to see see a tile view of the slideshow.
- Use this [link](/slides/Week 4/Slides-Week-4.html){target=&#34;_blank&#34;} to view a larger version of slideshow in a new window.

&lt;iframe src=&#34;/slides/Week 4/Slides-Week-4.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;&lt;/iframe&gt;

You can download a [PDF](/slides/Week 4/Slides-Week-4.pdf){target=&#34;_blank&#34;} of the presentation above. 
--&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Mostly without any additional packages or what is known as Base R&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Normal Distribution</title>
      <link>/lesson/05-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/05-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough&#34;&gt;R Walkthrough&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The fifth &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due &lt;strong&gt;two Wednesdays&lt;/strong&gt; following the completion of the Week 5 class. The module covers types of data sampling, and experiments. In particular, the sections are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language of data&lt;/li&gt;
&lt;li&gt;Study types and cautionary tales&lt;/li&gt;
&lt;li&gt;Sampling strategies and experimental design&lt;/li&gt;
&lt;li&gt;Case study&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%205/SSDS_Ch5.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 5 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%205/SSDS_Ch5.docx&#34; target=&#34;_blank&#34;&gt;Chapter 5 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;p&gt;Download the class&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%205/ICS_Ch5.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week5/ICN_Ch5.pdf&#34; target=&#34;_blank&#34;&gt;Outline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough&lt;/h3&gt;
&lt;p&gt;Take a look at a &lt;a href=&#34;/lesson/NFLpipes/&#34;&gt;walkthrough&lt;/a&gt; about using NFL data and pipes from &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sampling and Sampling Distributions</title>
      <link>/lesson/06-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/06-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough-nfl-ticket-prices&#34;&gt;R Walkthrough: NFL Ticket Prices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The fifth &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due &lt;strong&gt;two Wednesdays&lt;/strong&gt; following the completion of the Week 5 class. The module covers types of data sampling, and experiments. In particular, the sections are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language of data&lt;/li&gt;
&lt;li&gt;Study types and cautionary tales&lt;/li&gt;
&lt;li&gt;Sampling strategies and experimental design&lt;/li&gt;
&lt;li&gt;Case study&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%206/SSDS_Ch6.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 6 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%205/SSDS_Ch6.docx&#34; target=&#34;_blank&#34;&gt;Chapter 6 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%206/Slides-Week-6.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 6/Slides-Week-6.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%206/Slides-Week-6R.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/li&gt;
&lt;li&gt;an &lt;a href=&#34;/lecture_notes/Week6/ICN_Ch6.pdf&#34; target=&#34;_blank&#34;&gt;Outline&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough-nfl-ticket-prices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough: NFL Ticket Prices&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Download the data sets&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and codebooks needed for this walkthrough. Put them wherever you want BUT please remember where they are.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%206/2014-average-ticket-price.csv&#34; target=&#34;_blank&#34;&gt;Week 6 Walkthrough Data Set&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Click on the presentation itself and then you may&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%206/Slides-Week-6.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 6/Slides-Week-6R.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%206/Slides-Week-6R.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%206/Slides-Week-6R.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You will have to unzip this file. If you are unfamilair with this process, please check the &lt;a href=&#34;/resource/unzipping/&#34;&gt;Unzipping files&lt;/a&gt; section under Resources for assistance.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sampling and Sampling Distributions</title>
      <link>/lesson/07-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/07-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-camp&#34;&gt;Data Camp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#book-materials&#34;&gt;Book Materials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#class-notes&#34;&gt;Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough-an-incomplete-review-of-descriptive-statistics-commands-in-r&#34;&gt;R Walkthrough: An Incomplete Review of Descriptive Statistics Commands in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kaggle&#34;&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-sites&#34;&gt;Other Sites&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-camp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Camp&lt;/h3&gt;
&lt;p&gt;The fifth &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;Data Camp&lt;/a&gt; module is due. The module covers types of data sampling, and experiments. In particular, the sections are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language of data&lt;/li&gt;
&lt;li&gt;Study types and cautionary tales&lt;/li&gt;
&lt;li&gt;Sampling strategies and experimental design&lt;/li&gt;
&lt;li&gt;Case study&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;book-materials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Book Materials&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%206/SSDS_Ch6.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 6 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week%205/SSDS_Ch6.docx&#34; target=&#34;_blank&#34;&gt;Chapter 6 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%207/Slides-Week-7.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 7/Slides-Week-7.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%207/Slides-Week-7.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation.&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%207/Slides-Week-7.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough-an-incomplete-review-of-descriptive-statistics-commands-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough: An Incomplete Review of Descriptive Statistics Commands in R&lt;/h3&gt;
&lt;p&gt;Click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%207/Slides-Week-7.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 7/Slides-Week-7R.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%207/Slides-Week-7R.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%207/Slides-Week-7R.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;kaggle&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kaggle&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is an online learning community for data scientists and machine learning practitioners&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; That’s great but its greatest benefit are the numerous tutorials/walkthroughs provided and also to locate data sets&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. In an upcoming data task, you’ll be asked to locate a data set of interest to perform some analysis and Kaggle is a great place to start so if you get a chance, sign up for free and look around. However for now, there is an excellent &lt;a href=&#34;https://www.kaggle.com/jessemostipak/dive-into-dplyr-tutorial-1&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt; walkthrough&lt;/a&gt; that was posted recently and may be helpful to you for the exam.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-sites&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Sites&lt;/h3&gt;
&lt;p&gt;Head over to the &lt;a href=&#34;/resource/data/&#34;&gt;Data&lt;/a&gt; page under Resources to find more outlets!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Subsidiary of Google.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Stolen from Wikipedia.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;All data sets are submitted by people so while its fun for &lt;em&gt;window shopping&lt;/em&gt;, if possible find the original source. Typically users will list where their posted data sets can be found publicly&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimations</title>
      <link>/lesson/09-lesson/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/09-lesson/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-notes&#34;&gt;In Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#textbook-class-notes&#34;&gt;Textbook Class Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-walkthrough-confidence-intervals&#34;&gt;R Walkthrough: Confidence Intervals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;in-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In Class Notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Download the class slides: Remember to click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%209/Slides-Week-9.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 9/Slides-Week-9.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%209/Slides-Week-9.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;textbook-class-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Textbook Class Notes&lt;/h3&gt;
&lt;p&gt;Download the textbook&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%209/SSDS_Ch7.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 7 PowerPoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week9/SSDS_Ch7.docx&#34; target=&#34;_blank&#34;&gt;Chapter 7 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/slides/Week%2010/SSDS_Ch8.PPTX&#34; target=&#34;_blank&#34;&gt;Chapter 8 PowerPoint&lt;/a&gt; and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/lecture_notes/Week10/SSDS_Ch8.docx&#34; target=&#34;_blank&#34;&gt;Chapter 8 Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;r-walkthrough-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Walkthrough: Confidence Intervals&lt;/h3&gt;
&lt;p&gt;Click on the presentation itself and then you may&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move back and forth through the slideshow using the ⬅ and ⮕ buttons on your keyboard.&lt;/li&gt;
&lt;li&gt;Press the letter &lt;strong&gt;O&lt;/strong&gt; at any point to see see a tile view of the slideshow.&lt;/li&gt;
&lt;li&gt;Use this &lt;a href=&#34;/slides/Week%209/Slides-Week-9R.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view a larger version of slideshow in a new window.&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;/slides/Week 9/Slides-Week-9R.html&#34; width=&#34;672&#34; height=&#34;400px&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;You can download&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%209/Slides-Week-9R.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; of the presentation above.&lt;/li&gt;
&lt;li&gt;a &lt;a href=&#34;/slides/Week%209/Slides-Week-9R.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in the presentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 15: Something Fun - An Introduction to Sentiment Analysis Using (My Favorite Band): The Grateful Dead</title>
      <link>/lesson/sentimentanalysisintro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/sentimentanalysisintro/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preparation&#34;&gt;Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learning-by-doing&#34;&gt;Learning by Doing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extra-information&#34;&gt;Extra Information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-data&#34;&gt;Getting Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-wrangling&#34;&gt;Data Wrangling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-data&#34;&gt;Exploring Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lemmatize-words&#34;&gt;Lemmatize Words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-stop-words&#34;&gt;Using Stop Words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptive-statistics&#34;&gt;Descriptive Statistics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#wordclouds&#34;&gt;Wordclouds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bar-plots&#34;&gt;Bar Plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sentiment-analysis-1&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-text-analysis&#34;&gt;Basic Text Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#most-common-positive-and-negative-words-in-a-bar-graph&#34;&gt;Most Common Positive and Negative Words in a Bar Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-common-positive-and-negative-words-in-a-wordcloud&#34;&gt;Most Common Positive and Negative Words in a WordCloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#positive-and-negative-words-in-a-line-graph&#34;&gt;Positive and Negative Words in a Line Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#positive-and-negative-words-in-a-boxplot&#34;&gt;Positive and Negative Words in a Boxplot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#viewing-across-all-lexicons&#34;&gt;Viewing Across all Lexicons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;p&gt;Download a &lt;a href=&#34;/scripts/SentimentAnalysisIntro.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in this walkthrough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;Without getting into the specifics of machine learning, a &lt;strong&gt;sentiment analysis&lt;/strong&gt; is a quantitative process of determining whether a piece of writing is positive, negative or neutral. It entails using a mix of statistics (you know…that old chestnut), &lt;a href=&#34;https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html&#34; target=&#34;_blank&#34;&gt;natural language processing (NLP)&lt;/a&gt;, and &lt;a href=&#34;https://www.ibm.com/cloud/learn/machine-learning&#34; target=&#34;_blank&#34;&gt;machine learning&lt;/a&gt; to identify and extract subjective information from text files, for instance, a reviewer’s feelings, thoughts, judgments, or assessments within open text. If you want to know more about this method, Qualtrics provides a nice readable &lt;a href=&#34;https://www.qualtrics.com/experience-management/research/sentiment-analysis/&#34; target=&#34;_blank&#34;&gt;writeup&lt;/a&gt; about it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-by-doing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning by Doing&lt;/h2&gt;
&lt;p&gt;We are essentially going to learn about a sentiment analysis using music…well music that I like anyway. In this session we’ll be looking at some Grateful Dead lyrics from two albums.&lt;/p&gt;
&lt;p&gt;With that said, let’s load up some libraries. If you don’t have some of these (and you most likely don’t), remember to download them first using &lt;code&gt;Tools &amp;gt; Install Packages&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We are using the tidyverse family which is essentially the gold standard for 
# data wrangling:
library(tidyverse) 
library(scales)

# text mining and annoying wordclouds based on the tidyverse family
library(tidytext)
library(ggwordcloud)
library(textstem)
library(textdata)
library(wordcloud)

# getting lyrics
library(genius) # Allows you to download lyrics for an entire album in a tidy format from
# genius.com

# the aesthetics
library(viridis)
library(RColorBrewer)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extra-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extra Information&lt;/h2&gt;
&lt;p&gt;More information about
* the &lt;code&gt;tidyverse&lt;/code&gt; family of packages are introduced by selecting &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34;&gt;https://www.tidyverse.org/&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;genius.com can be found here &lt;a href=&#34;https://genius.com/&#34; target=&#34;_blank&#34;&gt;https://genius.com/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the genius package can be found via this link
&lt;a href=&#34;https://github.com/josiahparry/genius&#34; target=&#34;_blank&#34;&gt;https://github.com/josiahparry/genius&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Data&lt;/h2&gt;
&lt;p&gt;Grab the lyrics for &lt;strong&gt;&lt;em&gt;American Beauty&lt;/em&gt;&lt;/strong&gt; (1971) which was more about rock and roll and &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt; (1978) which was influenced by disco (ugh yes…disco). Were going to see if the sentiments associated within each album changed over time. On a side note, feel free to grab albums and lyrics that you like instead but the example will be based on the two albums so I would suggest going through it with these first and then doing your own. We’ll compare the two using a sentiment analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;american_beauty &amp;lt;- genius_album(artist = &amp;quot;The Grateful Dead&amp;quot;, album = &amp;quot;***American Beauty***&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;album_name&amp;quot;, &amp;quot;track_n&amp;quot;, &amp;quot;track_url&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see the results, just type in the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;american_beauty&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 359 x 4
##    track_n  line lyric                                               track_title
##      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                               &amp;lt;chr&amp;gt;      
##  1       1     1 Look out of any window                              Box of Rain
##  2       1     2 Any morning, any evening, any day                   Box of Rain
##  3       1     3 Maybe the sun is shining                            Box of Rain
##  4       1     4 Birds are winging or rain is falling from a heavy … Box of Rain
##  5       1     5 What do you want me to do                           Box of Rain
##  6       1     6 To do for you to see you through?                   Box of Rain
##  7       1     7 For this is all a dream we dreamed                  Box of Rain
##  8       1     8 One afternoon, long ago                             Box of Rain
##  9       1     9 Walk out of any doorway                             Box of Rain
## 10       1    10 Feel your way, feel your way like the day before    Box of Rain
## # … with 349 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s do the same for &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shakedown_street &amp;lt;- genius_album(artist = &amp;quot;The Grateful Dead&amp;quot;, album = &amp;quot;***Shakedown Street***&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;album_name&amp;quot;, &amp;quot;track_n&amp;quot;, &amp;quot;track_url&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the results look like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shakedown_street&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 304 x 4
##    track_n  line lyric                                               track_title
##      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                               &amp;lt;chr&amp;gt;      
##  1       1     1 &amp;quot;Well, I was feeling so bad, asked my family docto… Good Lovin&amp;#39;
##  2       1     2 &amp;quot;I said, \&amp;quot;Doctor, Doctor, Mister M.D., (doctor) c… Good Lovin&amp;#39;
##  3       1     3 &amp;quot;He said, \&amp;quot;Yeah, yeah, yeah, yeah, yeah, yeah, ye… Good Lovin&amp;#39;
##  4       1     4 &amp;quot;All you need, all you really need: good loving&amp;quot;    Good Lovin&amp;#39;
##  5       1     5 &amp;quot;Because you got to have loving (good loving)&amp;quot;      Good Lovin&amp;#39;
##  6       1     6 &amp;quot;Everybody got to have loving (good loving)&amp;quot;        Good Lovin&amp;#39;
##  7       1     7 &amp;quot;A little good loving now baby, good loving&amp;quot;        Good Lovin&amp;#39;
##  8       1     8 &amp;quot;So come on baby, squeeze me tight&amp;quot;                 Good Lovin&amp;#39;
##  9       1     9 &amp;quot;Don&amp;#39;t you want your daddy to be alright?&amp;quot;          Good Lovin&amp;#39;
## 10       1    10 &amp;quot;I said baby, now it&amp;#39;s for sure&amp;quot;                    Good Lovin&amp;#39;
## # … with 294 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To find out the internal structure of one of your data sets, run this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(shakedown_street)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [304 × 4] (S3: tbl_df/tbl/data.frame)
##  $ track_n    : int [1:304] 1 1 1 1 1 1 1 1 1 1 ...
##  $ line       : int [1:304] 1 2 3 4 5 6 7 8 9 10 ...
##  $ lyric      : chr [1:304] &amp;quot;Well, I was feeling so bad, asked my family doctor about what I had&amp;quot; &amp;quot;I said, \&amp;quot;Doctor, Doctor, Mister M.D., (doctor) can you tell me (doctor), what&amp;#39;s ailing me? (doctor)\&amp;quot;&amp;quot; &amp;quot;He said, \&amp;quot;Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah\&amp;quot;&amp;quot; &amp;quot;All you need, all you really need: good loving&amp;quot; ...
##  $ track_title: chr [1:304] &amp;quot;Good Lovin&amp;#39;&amp;quot; &amp;quot;Good Lovin&amp;#39;&amp;quot; &amp;quot;Good Lovin&amp;#39;&amp;quot; &amp;quot;Good Lovin&amp;#39;&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives you a good bit of information about the structure of your data set.
* The class of the data are in multiple formats: &lt;code&gt;tbl_df&lt;/code&gt; (tabular data frame), &lt;code&gt;tbl&lt;/code&gt; (table), and &lt;code&gt;data.frame&lt;/code&gt; (normal data frame). This implies that you can use common commands to wrangle the data within this variable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You have four (4) internal variables and 304 lines in this data frame (think an excel document which has 4 columns and 304 rows).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Notice that the track_title (track title) and &lt;code&gt;lyric&lt;/code&gt; (lyrics) columns are character vectors while &lt;code&gt;track_n&lt;/code&gt; (track number) and &lt;code&gt;line&lt;/code&gt; (line number) are integer vectors. These are important because what you can do with any vector is mostly dependent on its type.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Wrangling&lt;/h1&gt;
&lt;p&gt;If we’re going to compare the two albums, we should probably put them in the same data frame. But to make sure we can distinguish each, we’re going to have to tag them by album name. To do this, we need to add a column. The package &lt;code&gt;dplyr&lt;/code&gt; let’s you mess around with the data without destroying the data frame format. It also lets you deal with pipes (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) which is a fantastic thing. During the dark days of R before pipes, you had to go back and rerun any R code that you made changes to which meant you had to keep track of everything. In any case, when using pipes, to do (nearly) anything to the data frame, you must use the command &lt;code&gt;mutate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Remember we are adding a column that denoted the album name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;american_beauty_tagged &amp;lt;- american_beauty %&amp;gt;% 
  mutate(album = &amp;quot;***American Beauty***&amp;quot;) %&amp;gt;%
  select(album, track_title, track_n, line, lyric)&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok so let’s take a moment to get our bearings straight. Currently we are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;using the &lt;code&gt;american_beauty&lt;/code&gt; data set&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;adding a column named album and then populating it with the text &lt;strong&gt;&lt;em&gt;American Beauty&lt;/em&gt;&lt;/strong&gt; (in quotes); and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;rearranging the columns. By default, any new columns are put at the end of a data frame. Just to satisfy my obsessive nature, we’ll rearrange them in order of scope by putting the album name in front.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Good? Ok then let’s take a look at the data set now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;american_beauty&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 359 x 4
##    track_n  line lyric                                               track_title
##      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                               &amp;lt;chr&amp;gt;      
##  1       1     1 Look out of any window                              Box of Rain
##  2       1     2 Any morning, any evening, any day                   Box of Rain
##  3       1     3 Maybe the sun is shining                            Box of Rain
##  4       1     4 Birds are winging or rain is falling from a heavy … Box of Rain
##  5       1     5 What do you want me to do                           Box of Rain
##  6       1     6 To do for you to see you through?                   Box of Rain
##  7       1     7 For this is all a dream we dreamed                  Box of Rain
##  8       1     8 One afternoon, long ago                             Box of Rain
##  9       1     9 Walk out of any doorway                             Box of Rain
## 10       1    10 Feel your way, feel your way like the day before    Box of Rain
## # … with 349 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sure enough, there’s the column with the album name. So now let’s do the same for the other album&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shakedown_street_tagged &amp;lt;- shakedown_street %&amp;gt;% 
  mutate(album = &amp;quot;***Shakedown Street***&amp;quot;) %&amp;gt;%
  select(album, track_title, track_n, line, lyric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and verify:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shakedown_street&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 304 x 4
##    track_n  line lyric                                               track_title
##      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                               &amp;lt;chr&amp;gt;      
##  1       1     1 &amp;quot;Well, I was feeling so bad, asked my family docto… Good Lovin&amp;#39;
##  2       1     2 &amp;quot;I said, \&amp;quot;Doctor, Doctor, Mister M.D., (doctor) c… Good Lovin&amp;#39;
##  3       1     3 &amp;quot;He said, \&amp;quot;Yeah, yeah, yeah, yeah, yeah, yeah, ye… Good Lovin&amp;#39;
##  4       1     4 &amp;quot;All you need, all you really need: good loving&amp;quot;    Good Lovin&amp;#39;
##  5       1     5 &amp;quot;Because you got to have loving (good loving)&amp;quot;      Good Lovin&amp;#39;
##  6       1     6 &amp;quot;Everybody got to have loving (good loving)&amp;quot;        Good Lovin&amp;#39;
##  7       1     7 &amp;quot;A little good loving now baby, good loving&amp;quot;        Good Lovin&amp;#39;
##  8       1     8 &amp;quot;So come on baby, squeeze me tight&amp;quot;                 Good Lovin&amp;#39;
##  9       1     9 &amp;quot;Don&amp;#39;t you want your daddy to be alright?&amp;quot;          Good Lovin&amp;#39;
## 10       1    10 &amp;quot;I said baby, now it&amp;#39;s for sure&amp;quot;                    Good Lovin&amp;#39;
## # … with 294 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great those look good. Now to merge the data sets, we are going to use a command called &lt;code&gt;rbind&lt;/code&gt;. The single requirement is that both data frames have the same column names since it needs to know what columns go with what (but notice that having the same column names implies that the columns do not have to be in the same order!)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_albums &amp;lt;- rbind(american_beauty_tagged, shakedown_street_tagged)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you simply do the following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_albums&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 663 x 5
##    album           track_title track_n  line lyric                              
##    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                              
##  1 ***American Be… Box of Rain       1     1 Look out of any window             
##  2 ***American Be… Box of Rain       1     2 Any morning, any evening, any day  
##  3 ***American Be… Box of Rain       1     3 Maybe the sun is shining           
##  4 ***American Be… Box of Rain       1     4 Birds are winging or rain is falli…
##  5 ***American Be… Box of Rain       1     5 What do you want me to do          
##  6 ***American Be… Box of Rain       1     6 To do for you to see you through?  
##  7 ***American Be… Box of Rain       1     7 For this is all a dream we dreamed 
##  8 ***American Be… Box of Rain       1     8 One afternoon, long ago            
##  9 ***American Be… Box of Rain       1     9 Walk out of any doorway            
## 10 ***American Be… Box of Rain       1    10 Feel your way, feel your way like …
## # … with 653 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that you can’t actually observe both data frames were &lt;em&gt;stacked&lt;/em&gt;, or &lt;em&gt;bound.&lt;/em&gt; To do this, or at least get an indicator that the binding worked, you can find the &lt;em&gt;unique&lt;/em&gt; values in a column by doing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(all_albums$album)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;***American Beauty***&amp;quot;  &amp;quot;***Shakedown Street***&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sure enough, it looks like both albums are there.&lt;/p&gt;
&lt;div id=&#34;exploring-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring Data&lt;/h2&gt;
&lt;p&gt;OK now to do some exploratory data analysis (EDA) which have to be displayed in a useless format on par with &lt;em&gt;pie charts&lt;/em&gt; - that is &lt;em&gt;wordclouds&lt;/em&gt;. Let’s tidy the data sets.&lt;/p&gt;
&lt;p&gt;First we are going to tokenize the lyrics into a tidy dataframe&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics&amp;lt;- all_albums %&amp;gt;% 
  group_by(album) %&amp;gt;% 
  unnest_tokens(word, lyric) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here we are
1. using the all_albums data set we just created;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;grouping by album name which means all operations are done within the context of each album; and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;tokenizing the data set which basically is a fancy way of saying we’re splitting up lyrics into words within each album.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Take a look at what this data frame now looks like by using&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
## # Groups:   album [1]
##   album                 track_title track_n  line word  
##   &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; 
## 1 ***American Beauty*** Box of Rain       1     1 look  
## 2 ***American Beauty*** Box of Rain       1     1 out   
## 3 ***American Beauty*** Box of Rain       1     1 of    
## 4 ***American Beauty*** Box of Rain       1     1 any   
## 5 ***American Beauty*** Box of Rain       1     1 window
## 6 ***American Beauty*** Box of Rain       1     2 any&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or simply use &lt;code&gt;View(tidy_lyrics)&lt;/code&gt; to get a full view in a separate tab.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lemmatize-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lemmatize Words&lt;/h2&gt;
&lt;p&gt;For grammatical reasons, lyrics are going to use different forms of a word known as &lt;a href=&#34;https://essentialsoflinguistics.pressbooks.com/chapter/6-5-unsure-how-to-format-that-derivational-morphology/&#34; target=&#34;_blank&#34;&gt;morphological derivations&lt;/a&gt; such as drink, drinks, and drinking to name a few. Additionally, there are families of derivationally related words with similar meanings known as differentiated inflections such as democracy, democratic, and democratization. In many situations including lyrics, it is useful to use the basic term for each of these variants. For this we have &lt;strong&gt;stemming&lt;/strong&gt; and &lt;strong&gt;lemmatization&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stemming&lt;/strong&gt; refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lemmatization&lt;/strong&gt; refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;textstem&lt;/code&gt; package can do both, but for reasons associated with robustness, we’ll use lemmatization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics$word &amp;lt;- lemmatize_words(tidy_lyrics$word)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command (right) above takes the column &lt;code&gt;word&lt;/code&gt; in &lt;code&gt;tidy_lyrics&lt;/code&gt;, lemmatizes it, and then replaces the column &lt;code&gt;word&lt;/code&gt; in &lt;code&gt;tidy_lyrics&lt;/code&gt; (left) with the results. Take a look using &lt;code&gt;View(tidy_lyrics)&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-stop-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using Stop Words&lt;/h2&gt;
&lt;p&gt;Now that we have a list in a tidy format, let’s do something with it. First let’s remove all of the stop words. Recall that these are terms that are commonly used (e.g. the, an, in, etc) and for this type of analysis, are noise and basically useless. To do this, we’re going to use some logic! The approach that is the most efficient besides making someone else do it is to utilize &lt;em&gt;joins&lt;/em&gt;. If you want to know more about joins in R - which you definitely should get familiar with as it will make merging of data sets so much easier in your life - check out &lt;a href=&#34;https://stat545.com/bit001_dplyr-cheatsheet.html&#34; target=&#34;_blank&#34;&gt;Dr. Jenny Bryant’s Stat545 course&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our case, we’ll be using an &lt;code&gt;anti_join&lt;/code&gt; which basically gives you an output by finding the rows of the first table cannot that cannot be matched in the second table. But before that, let’s pull out the stop words from the &lt;code&gt;tidytext&lt;/code&gt; package by doing&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;stop_words&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then we’ll remove those words from our tidy data frame using an &lt;code&gt;anti_join&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics_nsw &amp;lt;- tidy_lyrics %&amp;gt;%
  ungroup() %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;word&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok so let’s take another moment to get our bearings straight. Currently we are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;using the &lt;code&gt;tidy_lyrics&lt;/code&gt; data set that we created;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ungrouping the data set; and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;removing all of the stop words from that data frame.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Now you can take a look by using &lt;code&gt;View(tidy_lyrics_nsw)&lt;/code&gt; or just look at the top six using the following syntax&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics_nsw %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   album                 track_title track_n  line word   
##   &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  
## 1 ***American Beauty*** Box of Rain       1     1 window 
## 2 ***American Beauty*** Box of Rain       1     2 morning
## 3 ***American Beauty*** Box of Rain       1     2 day    
## 4 ***American Beauty*** Box of Rain       1     3 sun    
## 5 ***American Beauty*** Box of Rain       1     3 shine  
## 6 ***American Beauty*** Box of Rain       1     4 bird&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;p&gt;Not seeing what you want? How about the bottom six then?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics_nsw %&amp;gt;%
  tail()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   album                  track_title        track_n  line word   
##   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  
## 1 ***Shakedown Street*** Stagger Lee (Live)      14    29 lee    
## 2 ***Shakedown Street*** Stagger Lee (Live)      14    30 song   
## 3 ***Shakedown Street*** Stagger Lee (Live)      14    30 delia  
## 4 ***Shakedown Street*** Stagger Lee (Live)      14    30 sing   
## 5 ***Shakedown Street*** Stagger Lee (Live)      14    30 stagger
## 6 ***Shakedown Street*** Stagger Lee (Live)      14    30 lee&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can remove additional words manually but for the purposes of this walkthrough, let’s not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;descriptive-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Descriptive Statistics&lt;/h2&gt;
&lt;p&gt;Until recently, the quantitative approach to analyze open text was in the use of word clouds. For all of my griping, this isn’t necessarily a flawed approach since it will indicate if some words are more important than others but they are &lt;em&gt;descriptive&lt;/em&gt; at best. Historically, the issue has been related to certain scholars making overarching claims, or &lt;em&gt;inferences&lt;/em&gt; from what are essentially visual representations of frequency counts which you should know by now is nonsense. However, with the advent of machine learning as a methodological tool, we now use inferential statistics to derive usable outcomes though there are still some people who stick by frequencies. In any case, let’s calculate some frequencies to get an idea of how our data looks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;frequencies_lyrics &amp;lt;- tidy_lyrics_nsw %&amp;gt;%
  group_by(album) %&amp;gt;%
  count(word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then plot them on a wordcloud. At this point, if you are unfamiliar or uncomfortable with &lt;code&gt;ggplot&lt;/code&gt;, I suggest going through or reviewing the first &lt;code&gt;ggplot&lt;/code&gt; section on &lt;a href=&#34;https://www.datacamp.com/&#34; target=&#34;_blank&#34;&gt;datacamp&lt;/a&gt;. With that said, there will also be a brief explanation below of what’s used below the plot.&lt;/p&gt;
&lt;p&gt;OK if you’re ready, run the following but be forewarned, it may take some time!&lt;/p&gt;
&lt;div id=&#34;wordclouds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wordclouds&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(99)
ggplot(frequencies_lyrics, aes(label = word, size = n, color = album)) +
  geom_text_wordcloud() +
  scale_radius(range = c(0, 20)) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_text_wordcloud).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/wordcloud-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So what’s going on above? Well let’s break it down:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;ggplot(frequencies_lyrics, aes(label = word, size = n, color = album))&lt;/code&gt; tells R to use the &lt;code&gt;ggplot2&lt;/code&gt; package by looking at the data set &lt;code&gt;frequencies_lyrics&lt;/code&gt; with the aesthetics (&lt;code&gt;aes&lt;/code&gt;) of labeling the data by the word, sizing each by its corresponding frequency (&lt;code&gt;n&lt;/code&gt;), and color by album title (&lt;code&gt;album&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;geom_text_wordcloud()&lt;/code&gt; tells R to use the &lt;code&gt;ggwordcloud&lt;/code&gt; package and that text will be used to represent the data and to put it in a spiral.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;scale_radius(range = c(0, 20))&lt;/code&gt; tells &lt;code&gt;ggplot&lt;/code&gt; that the smallest a data point can be is 0 points and the largest is 20 points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;theme_minimal&lt;/code&gt; is a &lt;code&gt;ggplot&lt;/code&gt; theme that removes all default(background, axes, etc.) information and just plots what you ask for.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may notice that I did not go over &lt;code&gt;set.seed&lt;/code&gt;. That is for another time because it deals with probabilities that are contingent on how data is analyzed and/or rendered by each package in R.&lt;/p&gt;
&lt;p&gt;OK great but that takes too long to render, its not very pretty nor does it do a great job of splitting the words apart - we do want to compare them after all! Let’s make it better:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(99)
frequencies_lyrics %&amp;gt;%
  filter(n &amp;gt; 1) %&amp;gt;%
  ggplot(aes(label = word, size = n, color = album)) +
  scale_color_manual(values = c(&amp;quot;#5bc0de&amp;quot;, &amp;quot;#5cb85c&amp;quot;)) +
  geom_text_wordcloud(rm_outside = TRUE, shape = &amp;quot;circle&amp;quot;) +
  scale_radius(range = c(4, 15)) +
  theme_minimal() +
  facet_grid(.~ album) +
  theme(panel.spacing = unit(0.5, &amp;quot;cm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_text_wordcloud).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Some words could not fit on page. They have been removed.
## Some words could not fit on page. They have been removed.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/wordcloud_custom_actual-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That looks better and we know what words come from what album. So what’s different here?
Well first of all, it is worth noting that not all of the syntax from above will be explained here simply due to the fact they are included for aesthetics and have nothing to do with the function creating the actual wordcloud. However, you are given the entire code so that the modified output is or will be at some point understandable. If you would like to know what this looks like without the extra aesthetics for comparison, run the following in your companion R script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(99)
frequencies_lyrics %&amp;gt;%
  filter(n &amp;gt; 1) %&amp;gt;%
ggplot(aes(label = word, size = n, color = album)) +
  scale_color_manual(values = c(&amp;quot;#5bc0de&amp;quot;, &amp;quot;#5cb85c&amp;quot;)) +
  geom_text_wordcloud(rm_outside = TRUE, shape = &amp;quot;circle&amp;quot;) +
  scale_radius(range = c(4, 15)) +
  theme_minimal() +
  facet_grid(album ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now back to the pending question that has been updated a bit: &lt;em&gt;What are the major differences here that aren’t related to the aesthetics?&lt;/em&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you call &lt;code&gt;frequencies_lyrics&lt;/code&gt;, you’ll notice that there are 684 rows implying that R has to plot 684 words. That takes some time! To reduce the time and before even calling &lt;code&gt;ggplot&lt;/code&gt;, let’s get rid of the “one offs”, or those terms that only appear once. Here we&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;called the original data set,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;filtered the frequency &lt;code&gt;n&lt;/code&gt; by telling R to only look at values greater than 1, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;then called &lt;code&gt;ggplot&lt;/code&gt;. One side note, notice that we don’t called the data set again in &lt;code&gt;ggplot&lt;/code&gt; since we already did that when filtering. The &lt;code&gt;%&amp;gt;%&lt;/code&gt; gives you the control to pass information through pipes. Much like anything Mario…pipes make everything better.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;I have manually assigned colors by using the &lt;code&gt;scale_color_manual&lt;/code&gt; command and inputting hexadecimal, or hex codes. These are one of two ways internet browsers determine what colors you see on screen (the other is called RBG) and most computer languages recognize them too. You can Google hex codes and find all sorts of examples. I suggest using this site which also includes user generated palettes, but obviously there is no mandate: &lt;a href=&#34;https://www.color-hex.com/&#34; target=&#34;_blank&#34;&gt;https://www.color-hex.com/&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(same as before)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To make some of the text appear readable, the range of &lt;code&gt;scale_radius&lt;/code&gt; was changed to reflect a minimum of 2 points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(same as before)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;facet_grid&lt;/code&gt; (and its counterpart &lt;code&gt;facet_wrap&lt;/code&gt; - not shown here) is a way of splitting up a visualization by a value in a column within your data frame. If you were confused as to why the album name was important and that we binded the original data frames, this is the reason! There are ways to customize a facet but we’ll get to that later.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;NOTE: If you see some blurred or overlapping text, just push the Zoom option right above the graphics window.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You can find more information, examples, and usable syntax/lines of code for &lt;code&gt;ggwordcloud&lt;/code&gt; by selecting going to this &lt;a href=&#34;https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html&#34; target=&#34;_blank&#34;&gt;vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bar-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bar Plots&lt;/h3&gt;
&lt;p&gt;One avenue to look at when comparing sets of open ended text is to compare terms, in that are common terms used? We can do this or &lt;a href=&#34;https://www.youtube.com/watch?v=AjPau5QYtYs&#34; target=&#34;_blank&#34;&gt;we can dance if we want to&lt;/a&gt;. Let’s say we don’t do the latter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;frequencies_lyrics %&amp;gt;%
  filter(n &amp;gt; 5) %&amp;gt;%
  group_by(album) %&amp;gt;% 
  count(word, sort = TRUE) %&amp;gt;%
  mutate(word = reorder(word, n)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(album = reorder(album, n)) %&amp;gt;% 
  ggplot(aes(word, n)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  facet_grid(~ album) +
  xlab(NULL) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/frequency_by_facet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a lot going on here! While some of these items may be repetitive, let’s break it down anyway:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;frequencies_lyrics&lt;/code&gt; data set has been called.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use &lt;code&gt;filter(n &amp;gt; 5)&lt;/code&gt; to filter out all of the word counts that are 5 or less. If you are using your own data set, this will vary.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since we are looking at albums, we should first pair those out by using &lt;code&gt;group_by(album)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using &lt;code&gt;count(word, sort = TRUE)&lt;/code&gt;, we count the number of words. Now you may be saying something like “Wait! didn’t we already do that?” Well we did but we counted them across both albums basically ignoring which word came from what album. This time, when we used &lt;code&gt;group_by(album)&lt;/code&gt;, the system now performs all operations by album. So words are now counted within each album.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you run the entire pipe chain up to this &lt;code&gt;count(word, sort = TRUE)&lt;/code&gt;, you may have noticed that the words are in order by counts. We can reorder the column &lt;code&gt;word&lt;/code&gt; by a particular word and then their respective counts &lt;code&gt;n&lt;/code&gt; by using &lt;code&gt;mutate(word = reorder(word, n))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you group something, that structure is maintained indefinitely. To remove it, we use the common &lt;code&gt;ungroup()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Similar to step 5, we can reorder the column &lt;code&gt;album&lt;/code&gt; by a particular album and then their respective counts &lt;code&gt;n&lt;/code&gt; by using &lt;code&gt;mutate(album = reorder(album, n))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, you may realize we are starting to plot. The great thing about the tidyverse universe is that you can do operations and plot in one fell swoop.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;ggplot(aes(word, n))&lt;/code&gt; tells R that we’re going to plot something using ggplot using the a &lt;code&gt;word&lt;/code&gt; on the &lt;em&gt;x&lt;/em&gt;-axis and its corresponding count &lt;code&gt;n&lt;/code&gt; on the y-axis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;geom_bar(stat = &#34;identity&#34;)&lt;/code&gt; indicates that we are going to use as bar plot. For the time being, we won’t discuss the &lt;code&gt;stat = identity&lt;/code&gt; part except to tell you that when a &lt;em&gt;y&lt;/em&gt; variable is declared, &lt;code&gt;geom_bar&lt;/code&gt; requires &lt;code&gt;stat = identity&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;facet_grid(~ albums)&lt;/code&gt; simply means we are going to facet, or ploy by albums.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;xlab(NULL)&lt;/code&gt; tells the system to ignore the label on the &lt;em&gt;x&lt;/em&gt;-axis which in this case is word.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Running the command up to this point yields vertical bar plots which are somewhat difficult to compare. They would probably be easier to interpret if the bars were horizontal. Well &lt;code&gt;coord_flip()&lt;/code&gt; does just that! This flips the &lt;em&gt;x&lt;/em&gt;- and &lt;em&gt;y&lt;/em&gt;-axes as well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will discuss themes at a later point but so you can simply read the vertical axis with the words, we are going to make the font size 8 (pixels) by stating &lt;code&gt;theme(axis.text.y = element_text(size = 8))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sentiment Analysis&lt;/h1&gt;
&lt;p&gt;Now we’ll investigate the various sentiments and emotions expressed in the lyrics by using three sentiment dictionaries or &lt;strong&gt;lexicons&lt;/strong&gt;, included with the &lt;code&gt;tidytext&lt;/code&gt; package: &lt;strong&gt;Bing&lt;/strong&gt;, &lt;strong&gt;NRC&lt;/strong&gt;, and &lt;strong&gt;AFINN&lt;/strong&gt;. I won’t cover what these are in great depth but in a nutshell&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
lexicon
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
description
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
variable type
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
sentiments
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
more information
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bing
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A backend of &lt;a href=&#34;https://www.bing.com/&#34; target=&#34;_blank&#34;&gt;Microsoft Bing&lt;/a&gt;), this lexicon may be used to assesses open text for its polarities in sentiments.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
categorical
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
positive, negative
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html&#34; target=&#34;_blank&#34;&gt;Bing&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NRC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
One of the most utilized and researched lexicons that provides information about emotional context.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
categorical
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
anger, anticipation, disgust, fear, joy, sadness, surprise, trust
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm&#34; target=&#34;_blank&#34;&gt;NRC&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
AFINN
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A lexicon used for measuring psychological valence by assigning a level of severity to a term.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
numerical
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-5,-4,-3,-2,-1,0,1,2,3,4,5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://github.com/fnielsen/afinn&#34; target=&#34;_blank&#34;&gt;AFINN&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can categorize the lyrics and then perform some text based transformations and manipulations to construct some visualizations.&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;tidy_lyrics_nsw&lt;/code&gt; data set that we created earlier, we will filter out any numbers in our data set and&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Get the &lt;strong&gt;Bing&lt;/strong&gt; lexicon with the get_sentiments() function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Join the &lt;strong&gt;Bing&lt;/strong&gt; lexicon to the tokenized data set, specify by = “word”&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_lyrics_bing &amp;lt;- tidy_lyrics_nsw %&amp;gt;% 
  filter(!grepl(&amp;#39;[0-9]&amp;#39;, word)) %&amp;gt;% 
  left_join(get_sentiments(&amp;quot;bing&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  group_by(album) %&amp;gt;%   
  mutate(sentiment = ifelse(is.na(sentiment), &amp;#39;neutral&amp;#39;, sentiment))

emotions_lyrics_bing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,605 x 6
## # Groups:   album [2]
##    album                 track_title track_n  line word    sentiment
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    
##  1 ***American Beauty*** Box of Rain       1     1 window  neutral  
##  2 ***American Beauty*** Box of Rain       1     2 morning neutral  
##  3 ***American Beauty*** Box of Rain       1     2 day     neutral  
##  4 ***American Beauty*** Box of Rain       1     3 sun     neutral  
##  5 ***American Beauty*** Box of Rain       1     3 shine   positive 
##  6 ***American Beauty*** Box of Rain       1     4 bird    neutral  
##  7 ***American Beauty*** Box of Rain       1     4 wing    neutral  
##  8 ***American Beauty*** Box of Rain       1     4 rain    neutral  
##  9 ***American Beauty*** Box of Rain       1     4 fall    negative 
## 10 ***American Beauty*** Box of Rain       1     4 heavy   neutral  
## # … with 1,595 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, there is a lot going on here! Let’s break it down line by line:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;tidy_lyrics_nsw&lt;/code&gt; data set has been called. We are using this rather than &lt;code&gt;tidy_lyrics&lt;/code&gt; because the stop words were removed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use &lt;code&gt;filter(!grepl(&#39;[0-9]&#39;, word))&lt;/code&gt; to filter out any numbers in our data set, namely in the &lt;code&gt;word&lt;/code&gt; column.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To get the &lt;strong&gt;Bing&lt;/strong&gt; lexicon and compare it to our list of words, we use &lt;code&gt;left_join(get_sentiments(&#34;bing&#34;), by = &#34;word&#34;)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Like before, we used &lt;code&gt;group_by(album)&lt;/code&gt; to perform all operations by album (or within each album if you prefer).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To perform most any operation on a data set when using &lt;code&gt;%&amp;gt;%&lt;/code&gt;, we use &lt;code&gt;mutate&lt;/code&gt;. In this case, we are using a logic statement to tell the program if it sees an entry with &lt;code&gt;NA&lt;/code&gt; in the sentiment column, change it to the term &lt;code&gt;neutral&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a total count, we delineate the sentiments and count how many are in each by album using &lt;code&gt;count(sentiment)&lt;/code&gt; which uses the &lt;code&gt;sentiment&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_lyrics_bing %&amp;gt;%
  count(sentiment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   album [2]
##   album                  sentiment     n
##   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 ***American Beauty***  negative     62
## 2 ***American Beauty***  neutral     703
## 3 ***American Beauty***  positive     62
## 4 ***Shakedown Street*** negative     76
## 5 ***Shakedown Street*** neutral     619
## 6 ***Shakedown Street*** positive     83&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;basic-text-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Text Analysis&lt;/h2&gt;
&lt;div id=&#34;most-common-positive-and-negative-words-in-a-bar-graph&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Common Positive and Negative Words in a Bar Graph&lt;/h3&gt;
&lt;p&gt;We can associate terms tagged with a negative sentiment with negative numbers to better visualize them. To accomplish this, we can create a logic statement called an if-else statement that assigns -n to a word with a negative sentiment and to everything else, it provides an n count. We’ll then order the words in these groups by the number of times they appear in the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_count &amp;lt;- emotions_lyrics_bing %&amp;gt;% 
  count(word, sentiment, sort = TRUE) 

word_count&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 693 x 4
## # Groups:   album [2]
##    album                  word     sentiment     n
##    &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
##  1 ***Shakedown Street*** love     positive     45
##  2 ***Shakedown Street*** lovin    neutral      25
##  3 ***Shakedown Street*** fire     neutral      24
##  4 ***Shakedown Street*** doctor   neutral      17
##  5 ***American Beauty***  home     neutral      16
##  6 ***Shakedown Street*** stagger  neutral      16
##  7 ***American Beauty***  friend   neutral      14
##  8 ***American Beauty***  time     neutral      14
##  9 ***Shakedown Street*** lee      neutral      14
## 10 ***Shakedown Street*** mountain neutral      14
## # … with 683 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above, we are going to&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;emotions_lyrics_bing&lt;/code&gt; data set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Count up the words by sentiment and then sort them by &lt;code&gt;count(word, sentiment, sort = TRUE)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_sentiments_bing &amp;lt;-  word_count %&amp;gt;% 
  filter(sentiment != &amp;#39;neutral&amp;#39;) %&amp;gt;% 
  group_by(sentiment) %&amp;gt;% 
  top_n(10, n) %&amp;gt;% 
  mutate(num = ifelse(sentiment == &amp;quot;negative&amp;quot;, -n, n)) %&amp;gt;%  
  select(-n) %&amp;gt;% 
  mutate(word = reorder(word, num)) %&amp;gt;% 
  ungroup() 

top_sentiments_bing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 21 x 4
##    album                  word    sentiment   num
##    &amp;lt;chr&amp;gt;                  &amp;lt;fct&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
##  1 ***Shakedown Street*** love    positive     45
##  2 ***American Beauty***  easy    positive     13
##  3 ***American Beauty***  sweet   positive      9
##  4 ***American Beauty***  devil   negative     -7
##  5 ***Shakedown Street*** dark    negative     -6
##  6 ***American Beauty***  fall    negative     -5
##  7 ***American Beauty***  lie     negative     -5
##  8 ***American Beauty***  numb    negative     -5
##  9 ***Shakedown Street*** dead    negative     -5
## 10 ***Shakedown Street*** miracle positive      5
## # … with 11 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s break it down line by line:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;word_count&lt;/code&gt; data set has been called.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use &lt;code&gt;filter(sentiment != &#39;neutral&#39;)&lt;/code&gt; to filter out any term in the &lt;code&gt;sentiment&lt;/code&gt; column that is &lt;code&gt;neutral&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To get the top 10 of words used in each album use &lt;code&gt;top_n(10, n)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We use a logic statement &lt;code&gt;mutate(num = ifelse(sentiment == &#34;negative&#34;, -n, n))&lt;/code&gt; to tell R that if it sees the term &lt;code&gt;neutral&lt;/code&gt; in the sentiment column, to change the the count associated with it to a negative in a new column called &lt;code&gt;num&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using &lt;code&gt;select(-n)&lt;/code&gt; tells the program that we want to get rid of the the column &lt;code&gt;n&lt;/code&gt;. You can think of the negative as a way to deselect a column.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We simply put the terms from greatest to least disregarding album by &lt;code&gt;mutate(word = reorder(word, num))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/top_sentiments_plot_true-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, alpha = 0.75) + 
  scale_fill_manual(guide = FALSE, values = c(&amp;quot;#d9534f&amp;quot;, &amp;quot;#428bca&amp;quot;)) +
  scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7)) + 
  labs(x = &amp;#39;&amp;#39;, y = &amp;quot;Number of Occurrences&amp;quot;,
       title = &amp;#39;Top Sentiments of Lyrics&amp;#39;,
       subtitle = &amp;#39;Most Common Positive and Negative Words&amp;#39;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 14 , face = &amp;quot;bold&amp;quot;),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1),
        panel.spacing = unit(0.5, &amp;quot;cm&amp;quot;)) +
   facet_wrap(album ~ ., scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now there a bunch of things going on in this bar plot too!&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Here we are using the &lt;code&gt;top_sentiments_bing&lt;/code&gt; data set but reordering the way its plotted by greatest to least and filling by positive or negative sentiment by &lt;code&gt;aes(reorder(word, num), num, fill = sentiment)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;geom_bar&lt;/code&gt; is a bar graph as we saw before, but here the transparency level is set to 75% by &lt;code&gt;alpha = 0.75&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The positive and negative colors are set to a specific blue (#d9534f) and red (#428bca), respectively by &lt;code&gt;scale_fill_manual(guide = FALSE, values = c(&#34;#d9534f&#34;, &#34;#428bca&#34;))&lt;/code&gt;. Additionally, the legend has been turned off as well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The y-axis is limited to plotting between -10 and 55 with an even number of breaks given by &lt;code&gt;scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;labs&lt;/code&gt; provides the opportunity to name the axes, title and subtitle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One of the default themes that can be used by &lt;code&gt;ggplot&lt;/code&gt; is &lt;code&gt;theme_bw()&lt;/code&gt; or a black and white theme.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The values along the x-axis can be aesthetically manipulated to be at a 45 degree angle in size 14 bold print right-adjusted by &lt;code&gt;axis.text.x = element_text(angle = 45, hjust = 1, size = 14, face = &#34;bold&#34;)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;most-common-positive-and-negative-words-in-a-wordcloud&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most Common Positive and Negative Words in a WordCloud&lt;/h3&gt;
&lt;p&gt;To compare the words within each album, we can initially use a comparison wordcloud. Now we can create this using &lt;code&gt;ggwordcloud&lt;/code&gt; as before, but if you just want a quick look, an older package named &lt;code&gt;wordcloud&lt;/code&gt; does a pretty good job and it has a built in function for comparisons.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the album &lt;strong&gt;&lt;em&gt;American Beauty&lt;/em&gt;&lt;/strong&gt; first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_lyrics_bing %&amp;gt;%
  filter(album == &amp;quot;***American Beauty***&amp;quot;) %&amp;gt;%
  filter(sentiment != &amp;quot;neutral&amp;quot;) %&amp;gt;% 
  count(word, sentiment, sort = TRUE) %&amp;gt;% 
  spread(sentiment, n, fill = 0L) %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  remove_rownames() %&amp;gt;% 
  column_to_rownames(&amp;quot;word&amp;quot;) %&amp;gt;% 
  select(-album) %&amp;gt;%
  comparison.cloud(colors = c(&amp;quot;#d9534f&amp;quot;, &amp;quot;#428bca&amp;quot;), title.size = 1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/am_beauty_cwc-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and then &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_lyrics_bing %&amp;gt;%
  filter(album == &amp;quot;***Shakedown Street***&amp;quot;) %&amp;gt;%
  filter(sentiment != &amp;quot;neutral&amp;quot;) %&amp;gt;% 
  count(word, sentiment, sort = TRUE) %&amp;gt;% 
  spread(sentiment, n, fill = 0L) %&amp;gt;% 
  as.data.frame() %&amp;gt;% 
  remove_rownames() %&amp;gt;% 
  column_to_rownames(&amp;quot;word&amp;quot;) %&amp;gt;% 
  select(-album) %&amp;gt;%
  comparison.cloud(colors = c(&amp;quot;#d9534f&amp;quot;, &amp;quot;#428bca&amp;quot;), title.size = 1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/shakedown_st_cwc-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Without being repetitive, the only command you haven’t seen is &lt;code&gt;spread&lt;/code&gt; which essentially takes a long data set and converts it into a wide one. Both &lt;code&gt;spread&lt;/code&gt; and its counterpart &lt;code&gt;gather&lt;/code&gt; are commands that are used often and worth your time getting to know a bit about, though the current incarnations of these known as &lt;code&gt;pivot_wider&lt;/code&gt; and &lt;code&gt;pivot_longer&lt;/code&gt;. If interested, take a look at the online version of the text &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html?q=pivot#pivoting&#34; target=&#34;_blank&#34;&gt;R for Data Science&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;positive-and-negative-words-in-a-line-graph&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Positive and Negative Words in a Line Graph&lt;/h3&gt;
&lt;p&gt;Now that we’ve looked at the most common words for either positive or negative sentiment, what proportion of these sentiments are present in within the entire lyrical data set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_neg_bing_album &amp;lt;- tidy_lyrics_nsw %&amp;gt;% 
  filter(!grepl(&amp;#39;[0-9]&amp;#39;, word)) %&amp;gt;% 
  left_join(get_sentiments(&amp;quot;bing&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  mutate(sentiment = ifelse(is.na(sentiment), &amp;#39;neutral&amp;#39;, sentiment)) %&amp;gt;%   
  group_by(album, sentiment) %&amp;gt;% 
  summarize(n = n()) %&amp;gt;% 
  mutate(percent = n / sum(n)) %&amp;gt;% 
  select(-n) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;album&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_neg_bing_album&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   album                  sentiment percent
##   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 ***American Beauty***  negative   0.0750
## 2 ***American Beauty***  neutral    0.850 
## 3 ***American Beauty***  positive   0.0750
## 4 ***Shakedown Street*** negative   0.0977
## 5 ***Shakedown Street*** neutral    0.796 
## 6 ***Shakedown Street*** positive   0.107&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_neg_bing_album %&amp;gt;% 
  filter(sentiment != &amp;quot;neutral&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = &amp;quot;Album&amp;quot;, y = &amp;quot;Emotion Words Count (as %)&amp;quot;) +
  scale_color_manual(values = c(positive = &amp;quot;#d9534f&amp;quot;, negative = &amp;quot;#428bca&amp;quot;)) +
  ggtitle(&amp;quot;Proportion of Positive and Negative Words by Album&amp;quot;, 
          subtitle = &amp;quot;Bing lexicon&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/albums_pos_neg_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well while this isn’t overtly interesting, it does provide some indication that while &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt; is slightly more positive, it is also more negative as well. Maybe we’ll get a better idea by looking at the individual songs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_neg_bing_track &amp;lt;- tidy_lyrics_nsw %&amp;gt;% 
  filter(!grepl(&amp;#39;[0-9]&amp;#39;, word)) %&amp;gt;% 
  left_join(get_sentiments(&amp;quot;bing&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  mutate(sentiment = ifelse(is.na(sentiment), &amp;#39;neutral&amp;#39;, sentiment)) %&amp;gt;%   
  group_by(album, track_title, track_n, sentiment) %&amp;gt;% 
  summarize(n = n()) %&amp;gt;% 
  mutate(percent = n / sum(n)) %&amp;gt;% 
  select(-n) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;album&amp;#39;, &amp;#39;track_title&amp;#39;, &amp;#39;track_n&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_neg_bing_track&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 5
## # Groups:   album, track_title, track_n [22]
##    album                 track_title       track_n sentiment percent
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;               &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 ***American Beauty*** Attics of My Life       9 negative   0.04  
##  2 ***American Beauty*** Attics of My Life       9 neutral    0.92  
##  3 ***American Beauty*** Attics of My Life       9 positive   0.04  
##  4 ***American Beauty*** Box of Rain             1 negative   0.0741
##  5 ***American Beauty*** Box of Rain             1 neutral    0.889 
##  6 ***American Beauty*** Box of Rain             1 positive   0.0370
##  7 ***American Beauty*** Brokedown Palace        7 negative   0.0392
##  8 ***American Beauty*** Brokedown Palace        7 neutral    0.892 
##  9 ***American Beauty*** Brokedown Palace        7 positive   0.0686
## 10 ***American Beauty*** Candyman                5 negative   0.0615
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_neg_bing_track %&amp;gt;% 
  filter(sentiment != &amp;quot;neutral&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, 
             group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = &amp;quot;Album&amp;quot;, y = &amp;quot;Sentiment Count (as %)&amp;quot;) +
  scale_color_manual(values = c(positive = &amp;quot;#d9534f&amp;quot;, negative = &amp;quot;#428bca&amp;quot;)) +
  ggtitle(&amp;quot;Proportion of Positive and Negative Words by Track&amp;quot;, 
          subtitle = &amp;quot;Bing lexicon&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = &amp;quot;bold&amp;quot;)) +
  facet_wrap(. ~ album, ncol = 1, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/track_pos_neg_plot-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Firstly, you may notice that &lt;code&gt;geom_line&lt;/code&gt;, &lt;code&gt;geom_point&lt;/code&gt;, and &lt;code&gt;reorder(track_title, track_n)&lt;/code&gt;. The first two are fairly obvious as they are parts of the visualization that render the dots and lines. In the latter, we arrange the tracks by their original order since many artists tend to itemize tracks in a preferred way. Sometimes this is intended to tell a story&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; while other times it is simply a personal or studio preference.&lt;/p&gt;
&lt;p&gt;Visually, the ebbs and flows of the sentiments within each album are relatively consistent and are reflections of each other as one would expect. In fact they both have an instance where the sentiments have a declining slop. The primary differential is the area between the positive and negative curves which is greater in the album &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt; than &lt;strong&gt;&lt;em&gt;American Beauty&lt;/em&gt;&lt;/strong&gt;. In fact, it is this disparity that is the main contribution to the earlier variant of this plot which looked at the albums as a whole.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;positive-and-negative-words-in-a-boxplot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Positive and Negative Words in a Boxplot&lt;/h3&gt;
&lt;p&gt;No we switch our attention to something that may be of greater interest, that is the &lt;code&gt;NRC&lt;/code&gt; lexicon. Recall this dictionary not only has positive and negative categories, it also has eight different emotional classifications: &lt;em&gt;Anger&lt;/em&gt;, &lt;em&gt;Anticipation&lt;/em&gt;, &lt;em&gt;Disgust&lt;/em&gt;, &lt;em&gt;Fear&lt;/em&gt;, &lt;em&gt;Joy&lt;/em&gt;, &lt;em&gt;Sadness&lt;/em&gt;, &lt;em&gt;Surprise&lt;/em&gt;, and &lt;em&gt;Trust&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this case we will disregard the positive and negative tags but please note that what &lt;code&gt;Bing&lt;/code&gt; classifies by either tag may not necessarily be the same as how &lt;strong&gt;NRC&lt;/strong&gt; groups them. In a nutshell, both &lt;code&gt;Bing&lt;/code&gt; and &lt;code&gt;NRC&lt;/code&gt; are different lexicons from two different sources so it would make sense that there would be a discrepancy between classifications.&lt;/p&gt;
&lt;p&gt;Using a similar approach as before, we first look at the differentials by album:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_album_nrc &amp;lt;- tidy_lyrics_nsw  %&amp;gt;% 
  left_join(get_sentiments(&amp;quot;nrc&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  filter(!(sentiment == &amp;quot;negative&amp;quot; | sentiment == &amp;quot;positive&amp;quot;)) %&amp;gt;% 
  mutate(sentiment = as.factor(sentiment)) %&amp;gt;% 
    group_by(album, sentiment) %&amp;gt;%
  summarize(n = n()) %&amp;gt;% 
  mutate(percent = n / sum(n)) %&amp;gt;%   
  select(-n) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;album&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_album_nrc &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 3
##    album                  sentiment    percent
##    &amp;lt;chr&amp;gt;                  &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 ***American Beauty***  anger         0.0611
##  2 ***American Beauty***  anticipation  0.195 
##  3 ***American Beauty***  disgust       0.0611
##  4 ***American Beauty***  fear          0.0656
##  5 ***American Beauty***  joy           0.208 
##  6 ***American Beauty***  sadness       0.143 
##  7 ***American Beauty***  surprise      0.0905
##  8 ***American Beauty***  trust         0.176 
##  9 ***Shakedown Street*** anger         0.0892
## 10 ***Shakedown Street*** anticipation  0.140 
## 11 ***Shakedown Street*** disgust       0.0255
## 12 ***Shakedown Street*** fear          0.178 
## 13 ***Shakedown Street*** joy           0.219 
## 14 ***Shakedown Street*** sadness       0.110 
## 15 ***Shakedown Street*** surprise      0.104 
## 16 ***Shakedown Street*** trust         0.134&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then the corresponding plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_album_nrc %&amp;gt;% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  scale_fill_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  ggtitle(&amp;quot;Distribution of Sentiments by Album&amp;quot;) +
  labs(x = &amp;quot;Sentiment&amp;quot;, y = &amp;quot;Percentage&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        legend.position = &amp;quot;none&amp;quot;,
        axis.text.x = element_text(size = 11, face = &amp;quot;bold&amp;quot;),
        axis.text.y = element_text(size = 11, face = &amp;quot;bold&amp;quot;)) +
  facet_wrap(. ~ album, ncol = 1, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/album_nrc_plot-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The lack of boxes isn’t surprising considering we only have one value for each sentiment per album. What is interesting is that the percent of terms associated with anger, anticipation, fear and trust are elevated in &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Now a bit of social commentary: These emotions tend to reflect the inner turmoil going on within the band where two of the staple members (Keith and Donna Jean Godchaux) left the band after it was reportedly found that Donna Jean was having an affair with another band member by the name of Bob Weir. Let’s see if this is reflected in the songs themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_tracks_nrc &amp;lt;- tidy_lyrics_nsw  %&amp;gt;% 
  left_join(get_sentiments(&amp;quot;nrc&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  filter(!(sentiment == &amp;quot;negative&amp;quot; | sentiment == &amp;quot;positive&amp;quot;)) %&amp;gt;% 
  mutate(sentiment = as.factor(sentiment)) %&amp;gt;% 
  group_by(album, track_title, track_n, sentiment) %&amp;gt;% 
  summarize(n = n()) %&amp;gt;% 
  mutate(percent = n / sum(n)) %&amp;gt;%   
  select(-n) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;album&amp;#39;, &amp;#39;track_title&amp;#39;, &amp;#39;track_n&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_tracks_nrc &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 149 x 5
##    album                 track_title       track_n sentiment    percent
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;               &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 ***American Beauty*** Attics of My Life       9 anger         0.0625
##  2 ***American Beauty*** Attics of My Life       9 anticipation  0.25  
##  3 ***American Beauty*** Attics of My Life       9 disgust       0.0625
##  4 ***American Beauty*** Attics of My Life       9 joy           0.125 
##  5 ***American Beauty*** Attics of My Life       9 sadness       0.188 
##  6 ***American Beauty*** Attics of My Life       9 surprise      0.0625
##  7 ***American Beauty*** Attics of My Life       9 trust         0.25  
##  8 ***American Beauty*** Box of Rain             1 anger         0.0345
##  9 ***American Beauty*** Box of Rain             1 anticipation  0.276 
## 10 ***American Beauty*** Box of Rain             1 fear          0.103 
## # … with 139 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then the corresponding plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_tracks_nrc  %&amp;gt;% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  scale_fill_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  ggtitle(&amp;quot;Distribution of Sentiments by Album Aggregated by Track&amp;quot;) +
  labs(x = &amp;quot;Detected Sentiments&amp;quot;, y = &amp;quot;Percentage&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        legend.position = &amp;quot;none&amp;quot;,
        axis.text.x = element_text(size = 11, face = &amp;quot;bold&amp;quot;),
        axis.text.y = element_text(size = 11, face = &amp;quot;bold&amp;quot;)) +
  facet_wrap(. ~ album, ncol = 1, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/tracks_nrc_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So when we look at the scores by track and then aggregate, we see the the original findings seem to be correct though there is great deal more variability in joy within &lt;strong&gt;&lt;em&gt;Shakedown Street&lt;/em&gt;&lt;/strong&gt; that wasn’t detected earlier (maybe it was the disco). Additionally there appears to be a greater association with sadness in &lt;strong&gt;&lt;em&gt;American Beauty&lt;/em&gt;&lt;/strong&gt; that was not found earlier as well. While you may not be familiar with the album, the new findings here are also not a bug surprise. The album is known for its polar track sequencing where an extremely upbeat song (reflected in joy) would be followed by one that is sorrowful (reflected almost equally by sadness).&lt;/p&gt;
&lt;p&gt;What if we wanted to see how the songs changed by track order? We can use a bump chart to visualize this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_tracks_nrc %&amp;gt;% 
  ggplot(aes(reorder(track_title, track_n), percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab(&amp;quot;Album&amp;quot;) + ylab(&amp;quot;Proportion of Sentiments&amp;quot;) +
  ggtitle(&amp;quot;Sentiments by Albums&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = &amp;quot;bold&amp;quot;)) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  facet_wrap(. ~ album, ncol = 1, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/tracks_agg_nrc_bump-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well that got messy. Even with the colors, it is hard to discern what is going on. This is a good instance where you can use &lt;code&gt;facet_grid&lt;/code&gt;. Until now, we have been using &lt;code&gt;facet_wrap&lt;/code&gt; to separate the &lt;code&gt;album&lt;/code&gt; variable into its distinct entries. However the functionality of &lt;code&gt;facet_wrap&lt;/code&gt; is limited and will return a symmetrical matrix of plots for the number of unique levels or factors of a given variable. &lt;code&gt;facet_grid&lt;/code&gt; serves another purpose by return facets equal to the levels or factors of a given variable. Think of it more this way -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;facet_wrap&lt;/code&gt; essentially splits your plot across the categories you want.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;facet_grid&lt;/code&gt; does a similar thing but instead of creating different plots it creates different grids and then plots each plot in the grids.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Still confused? Well let’s apply it and see.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_tracks_nrc %&amp;gt;% 
  ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab(&amp;quot;Album&amp;quot;) + ylab(&amp;quot;Proportion of Sentiments&amp;quot;) +
  ggtitle(&amp;quot;Individual Sentiments by Album&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 8, face = &amp;quot;bold&amp;quot;)) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  facet_grid(sentiment ~ album, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/tracks_ind_nrc_bump-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We only changed &lt;code&gt;facet_wrap(. ~ album, ncol = 1, scales = &#34;free_x&#34;)&lt;/code&gt; in the messy plot to &lt;code&gt;facet_grid(sentiment ~ album, scales = &#34;free_x&#34;)&lt;/code&gt; in the new one. Sure it could still use some work on the aesthetics side of things but the line graphs are now provided by type, or facet which makes a visual comparison much easier.&lt;/p&gt;
&lt;p&gt;With a few exceptions, you may notice that the sentiments share a common shape when comparing tracks in order. This consistency is not an outlier. One of the reasons you may enjoy a follow-up album by an artist you liked before is hypothesized to having a similar line of best fit. Don’t believe me? Try it yourself and hopefully you remember some basic terms from Algebra.&lt;/p&gt;
&lt;p&gt;First let’s define a binomial function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;binomial_smooth &amp;lt;- function(...) {
  geom_smooth(method = &amp;quot;glm&amp;quot;, method.args = list(family = &amp;quot;binomial&amp;quot;), ...)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then apply it as a third degree polynomial&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_tracks_nrc %&amp;gt;% 
  ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  binomial_smooth(formula = y ~ splines::ns(x, 3), color = &amp;quot;#000000&amp;quot;)   +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab(&amp;quot;Album&amp;quot;) + ylab(&amp;quot;Proportion of Sentiments&amp;quot;) +
  ggtitle(&amp;quot;Individual Sentiments by Album&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = &amp;quot;bold&amp;quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 8, face = &amp;quot;bold&amp;quot;)) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  facet_grid(sentiment ~ album, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/tracks_ind_nrc_bump_glm-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Change the number &lt;code&gt;3&lt;/code&gt; in &lt;code&gt;binomial_smooth(formula = y ~ splines::ns(x, 3), color = &#34;#000000&#34;)&lt;/code&gt; to amend the degree as you see fit. In any case, you may notice that the line of best fit is relatively consistent between the two albums when assessing sentiments in track order.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;viewing-across-all-lexicons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Viewing Across all Lexicons&lt;/h2&gt;
&lt;p&gt;Let’s compare the lexicons themselves on how many positive and negative words they each categorize.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;bing&amp;quot;) %&amp;gt;% 
  count(sentiment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   sentiment     n
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 negative   4781
## 2 positive   2005&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;nrc&amp;quot;) %&amp;gt;% 
  count(sentiment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    sentiment        n
##    &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt;
##  1 anger         1247
##  2 anticipation   839
##  3 disgust       1058
##  4 fear          1476
##  5 joy            689
##  6 negative      3324
##  7 positive      2312
##  8 sadness       1191
##  9 surprise       534
## 10 trust         1231&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In the &lt;code&gt;bing&lt;/code&gt; lexicon, there are 4781 negative and 2005 positive terms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;code&gt;nrc&lt;/code&gt; lexicon, there are 3324 negative and 2312 positive terms.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;afinn&amp;quot;) %&amp;gt;% 
  count(value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 11 x 2
##    value     n
##    &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1    -5    16
##  2    -4    43
##  3    -3   264
##  4    -2   966
##  5    -1   309
##  6     0     1
##  7     1   208
##  8     2   448
##  9     3   172
## 10     4    45
## 11     5     5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In the &lt;code&gt;AFINN&lt;/code&gt; lexicon, the measure with regards to the severity of a term being positive or negative, rather than just treating a word as one or the other. However, we can just treat the negative measures as negative sentiments and the positive measures as positive sentiments just to get a general idea of the total number. With that in mind, we can do the following:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sentiments(&amp;quot;afinn&amp;quot;) %&amp;gt;% 
  select(value) %&amp;gt;% 
  mutate(sentiment = if_else(value &amp;gt; 0, &amp;quot;positive&amp;quot;, &amp;quot;negative&amp;quot;, &amp;quot;NA&amp;quot;)) %&amp;gt;% 
  group_by(sentiment) %&amp;gt;% 
  summarize(sum = n()) %&amp;gt;%
  filter(sentiment == &amp;quot;positive&amp;quot; | sentiment == &amp;quot;negative&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   sentiment   sum
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 negative   1599
## 2 positive    878&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In the &lt;code&gt;AFINN&lt;/code&gt; lexicon, there are (generally) 1599 negative and 877 positive terms.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now taking a look at our data set, let’s see how our terms get tagged.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_lyrics_bing %&amp;gt;% 
  group_by(sentiment) %&amp;gt;% 
  summarize(sum = n()) %&amp;gt;%  
  filter(sentiment == &amp;quot;positive&amp;quot; | sentiment == &amp;quot;negative&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   sentiment   sum
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 negative    138
## 2 positive    145&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;bing&lt;/code&gt; tags 134 of our terms as negative and 143 as positive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy_lyrics_nsw %&amp;gt;%
  left_join(get_sentiments(&amp;quot;nrc&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  group_by(sentiment) %&amp;gt;% 
  summarize(sum = n()) %&amp;gt;%
  filter(sentiment == &amp;quot;positive&amp;quot; | sentiment == &amp;quot;negative&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   sentiment   sum
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 negative    145
## 2 positive    264&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, &lt;code&gt;nrc&lt;/code&gt; tags 140 of our terms as negative and 258 as positive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_lyrics_afinn &amp;lt;- tidy_lyrics_nsw  %&amp;gt;% 
  left_join(get_sentiments(&amp;quot;afinn&amp;quot;), by = &amp;quot;word&amp;quot;) %&amp;gt;% 
  filter(!grepl(&amp;#39;[0-9]&amp;#39;, word))

emotions_lyrics_afinn %&amp;gt;% 
  select(value) %&amp;gt;% 
  mutate(sentiment = if_else(value &amp;gt; 0, &amp;quot;positive&amp;quot;, &amp;quot;negative&amp;quot;, &amp;quot;NA&amp;quot;)) %&amp;gt;% 
  group_by(sentiment) %&amp;gt;% 
  summarize(sum = n()) %&amp;gt;%
  filter(sentiment == &amp;quot;positive&amp;quot; | sentiment == &amp;quot;negative&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   sentiment   sum
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 negative    116
## 2 positive    155&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally &lt;code&gt;AFINN&lt;/code&gt; (generally) tags 113 of our terms as negative and 153 as positive.&lt;/p&gt;
&lt;p&gt;So you can see that the sentiments are dependent on lexicons. Is this a bad thing? Well it depends on what you are using it for.&lt;/p&gt;
&lt;p&gt;Now let’s put all of the lexicons together and see how they compare. Note that there is a lot below which will not have a line-by-line explanation. At this point, try reading the code to figure out what is occurring. You may find this to be easier when using albums of artists that you enjoy. In any case, please ask questions if needed!&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;p&gt;First we calculate &lt;strong&gt;AFINN&lt;/strong&gt; sentiment scores and compare them to the list of terms we already have.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;afinn_scores &amp;lt;- emotions_lyrics_afinn %&amp;gt;% 
  replace_na(replace = list(value = 0)) %&amp;gt;%
  group_by(index = album, track_title) %&amp;gt;% 
  summarize(sentiment = sum(value)) %&amp;gt;% 
  mutate(lexicon = &amp;quot;AFINN&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;index&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then combine both the &lt;code&gt;bing&lt;/code&gt; and &lt;code&gt;nrc&lt;/code&gt; lexicons into one data frame and calculate the sentiment scores for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bing_nrc_scores &amp;lt;- bind_rows(
  tidy_lyrics_nsw %&amp;gt;% 
    inner_join(get_sentiments(&amp;quot;bing&amp;quot;)) %&amp;gt;% 
    mutate(lexicon = &amp;quot;Bing&amp;quot;),
  tidy_lyrics %&amp;gt;% 
    inner_join(get_sentiments(&amp;quot;nrc&amp;quot;) %&amp;gt;% 
                 filter(sentiment %in% c(&amp;quot;positive&amp;quot;, &amp;quot;negative&amp;quot;))) %&amp;gt;% 
    mutate(lexicon = &amp;quot;NRC&amp;quot;)) %&amp;gt;% 
  # from here we count the sentiments, spread on positive/negative, 
  # then create the final sentiment score:
  count(lexicon, index = album, track_title, sentiment) %&amp;gt;% 
  spread(sentiment, n, fill = 0) %&amp;gt;% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;word&amp;quot;
## Joining, by = &amp;quot;word&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we compile a list of all three lexicons using the &lt;code&gt;bind_rows&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_lexicons &amp;lt;- bind_rows(afinn_scores, bing_nrc_scores) %&amp;gt;%
  select(-negative, -positive)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Try &lt;code&gt;View(all_lexicons)&lt;/code&gt; if you want to view the resulting data frame. To make the final plot more interesting, let’s assign a palette using hex colors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lexicon_cols &amp;lt;- c(&amp;quot;AFINN&amp;quot; = &amp;quot;#ae5a41&amp;quot;, &amp;quot;NRC&amp;quot; = &amp;quot;#559e83&amp;quot;, &amp;quot;Bing&amp;quot; = &amp;quot;#1b85b8&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we plot all of the lexicons by album and score.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_lexicons %&amp;gt;% 
  ggplot(aes(track_title, sentiment, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~lexicon, ncol = 1, scales = &amp;quot;free_y&amp;quot;) +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle(&amp;quot;Comparison of Sentiments&amp;quot;, subtitle = &amp;quot;by track order&amp;quot;) +
  labs(x = &amp;quot;Index of All Songs&amp;quot;, y = &amp;quot;Sentiment Score&amp;quot;) +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_blank()) +
  facet_grid(lexicon ~ index, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/lesson/SentimentAnalysisIntro_files/figure-html/all_lexicons_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well that is a lot to take in but this is essentially one way statistics can be applied! We have taken open text, broken it down into its basic terms, filtered out common terms, and used natural language processing (NLP) including various lexicons to derive sentiments.&lt;/p&gt;
&lt;p&gt;This work is registered under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/3.0/us/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-SA 3.0 Creative Commons License&lt;/a&gt;. Any items including, but not limited to lyrics, logos, and references associated with the Grateful Dead are licensed under  Grateful Dead Productions unless otherwise noted. All rights reserved.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;like Dark Side of the Moon by Pink Floyd or maybe that reference just dated me and you are even more confused.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dplyr, Pipes, and the NFL</title>
      <link>/lesson/nflpipes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/lesson/nflpipes/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preparation&#34;&gt;Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#purpose&#34;&gt;Purpose&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objectives&#34;&gt;Objectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-tidyverse-package&#34;&gt;The Tidyverse Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidy-data&#34;&gt;Tidy Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basics&#34;&gt;Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-pipe-operator&#34;&gt;The Pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; Operator&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gapminder&#34;&gt;Gapminder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mutate-adds-new-variables&#34;&gt;&lt;code&gt;mutate()&lt;/code&gt; adds new variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-these-out&#34;&gt;Try These Out&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#group_by-operates-on-groups&#34;&gt;&lt;code&gt;group_by()&lt;/code&gt; operates on groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summarize-with-group_by&#34;&gt;&lt;code&gt;summarize()&lt;/code&gt; with &lt;code&gt;group_by()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#arrange-orders-columns&#34;&gt;&lt;code&gt;arrange()&lt;/code&gt; orders columns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nfl-data-set&#34;&gt;NFL Data Set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#task&#34;&gt;Task&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#excusive&#34;&gt;Logical Operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-science&#34;&gt;Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dont-get-bogged-down&#34;&gt;Don’t Get Bogged Down!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;p&gt;Download a &lt;a href=&#34;/scripts/NFLpipes.R&#34; target=&#34;_blank&#34;&gt;script&lt;/a&gt; file of just the R chunks used in this walkthrough.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;purpose&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;What are some common things you like to do with your data? Maybe remove rows or columns, do calculations and maybe add new columns? This is called &lt;strong&gt;data wrangling&lt;/strong&gt;. It’s not data management or data manipulation: you &lt;strong&gt;keep the raw data raw&lt;/strong&gt; and do these things programatically in R with the &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You are going to be introduced to data wrangling in R without using the base package, or “Base R.” The &lt;code&gt;tidyverse&lt;/code&gt; is a suite of packages that match a philosophy of data science developed by Hadley Wickham and the RStudio team. It is a more straight-forward way to learn R. I encourage you to take a look around the &lt;code&gt;tidyverse&lt;/code&gt; web page just to see everything it can do. You can fine it here: &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34;&gt;https://www.tidyverse.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now that you have had two weeks of utter frustration with “Base R”, which means, in R without using any additional packages (though we have used a bit of tidyverse), I will show you by comparison what code will look like in “Base R”. For some things, base-R is more straightforward and where tat is apparent, I will note it. Whenever we use a function that is from the &lt;code&gt;tidyverse&lt;/code&gt;, we will prefix it so you’ll know for sure.&lt;/p&gt;
&lt;div id=&#34;objectives&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Objectives&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;discuss tidying data&lt;/li&gt;
&lt;li&gt;read data from a &lt;code&gt;csv&lt;/code&gt; file into R&lt;/li&gt;
&lt;li&gt;explore &lt;code&gt;gapminder&lt;/code&gt; data with base-R functions&lt;/li&gt;
&lt;li&gt;wrangle &lt;code&gt;gapminder&lt;/code&gt; data with &lt;code&gt;dplyr&lt;/code&gt; from the &lt;code&gt;tidyverse&lt;/code&gt; family of functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages&lt;/h3&gt;
&lt;p&gt;Please load up the following packages&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.4     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
## ✓ readr   1.4.0     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter()     masks stats::filter()
## x dplyr::group_rows() masks kableExtra::group_rows()
## x dplyr::lag()        masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gapminder)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to download them if you receive an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)
install.packages(&amp;quot;gapminder&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tidyverse-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Tidyverse Package&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;tidyverse&lt;/code&gt; package is actually a family of packages that have been constructed to take all kinds of data sets in multiple formats and to &lt;strong&gt;tidy&lt;/strong&gt; them. Before we get into that concept, the typical path that we take when working with real world data sets is relatively simple…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…yet the process can be extremely complex and time consuming as described below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information &lt;a href=&#34;http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html&#34;&gt;(NYTimes, 2014)&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;tidyverse&lt;/code&gt; provides packages - given in &lt;span style=&#34; font-weight: bold;    color: #28453B !important;&#34;&gt;green&lt;/span&gt; - that address all of these notions, but we won’t even be able to scratch the surface of most of them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this walk through, we’ll be concentrating on the use of pipes using the &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidy Data&lt;/h2&gt;
&lt;p&gt;Let’s start off discussing &lt;strong&gt;tidy data&lt;/strong&gt; which has simple convention: put variables in the columns and observations in the rows - amazing right?&lt;/p&gt;
&lt;p&gt;There are three interrelated rules which make a data set tidy:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Each variable must have its own column.&lt;/li&gt;
&lt;li&gt;Each observation must have its own row.&lt;/li&gt;
&lt;li&gt;Each value must have its own cell.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are going to wrangle - yup that is a real term for messing with the structure of data - a tidy-ish data set (the &lt;strong&gt;Mutate&lt;/strong&gt; part of the cycle), and then come back to tidying messy data using &lt;code&gt;tidyr&lt;/code&gt; once we’ve gotten it into proper form.&lt;/p&gt;
&lt;p&gt;Conceptually, making data tidy first is really critical. Instead of building your analyses around whatever format your data are in, we’ll take deliberate steps to make your data tidy. When your data are in this format, you can use a growing assortment of powerful analytical and visualization tools instead of inventing home-grown ways to accommodate your data. This will save you time since you aren’t reinventing the wheel, and will make your work more clear and understandable to your collaborators. Additionally after struggling with Base R and its arduous process, pipes will probably be something you welcome!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basics&lt;/h2&gt;
&lt;p&gt;There are six functions in &lt;code&gt;dplyr&lt;/code&gt; that you will primarily use to wrangle data. Remember that variables are the column names while observations are the values within a column.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;&lt;code&gt;filter()&lt;/code&gt;&lt;/strong&gt; command which let’s you pick observations by their values.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R5.png&#34; style=&#34;width:75.0%&#34; /&gt;&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;&lt;code&gt;select()&lt;/code&gt;&lt;/strong&gt; command which let’s you pick variables by their names.&lt;br /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R6.png&#34; style=&#34;width:75.0%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;&lt;code&gt;mutate()&lt;/code&gt;&lt;/strong&gt; command which let’s you create new variables with functions of existing variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R7.png&#34; style=&#34;width:75.0%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;&lt;code&gt;summarise()&lt;/code&gt;&lt;/strong&gt; command which let’s you collapse multiple values to a single summary.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R8.png&#34; style=&#34;width:75.0%&#34; /&gt;&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;&lt;code&gt;group_by()&lt;/code&gt;&lt;/strong&gt; command which let’s you perform operations with respect to a variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/img/R/R9.png&#34; style=&#34;width:75.0%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;&lt;code&gt;arrange()&lt;/code&gt;&lt;/strong&gt; command which let’s you reorder the rows of a data frame.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(nope there isn’t a picture for this)
&lt;br&gt;
Should you memorize these? NO! NEVER! DON’T DO IT! That’s what the internet is for! As with any data set, the main objective is to logically think about what you can do to achieve a goal. The commands are simply paths you could take to get there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pipe-operator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; Operator&lt;/h2&gt;
&lt;p&gt;Pipes are a logical operator from the &lt;code&gt;magrittr&lt;/code&gt; package that allows you to pass logic down a chain. Find that confusing? Let’s try to explain it another way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;I %&amp;gt;% 
  woke up %&amp;gt;%
  showered %&amp;gt;%
  got dressed %&amp;gt;%
  ate breakfast %&amp;gt;%
  showed up for work&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here every act is dependent on all of the previous acts. This essentially signifies what pipes do, in that you can fit multiple commands in a row without having to do them one by one as in Base R. Pipes are given by the &lt;code&gt;%&amp;gt;%&lt;/code&gt; symbol. In RStudio, the keyboard shortcut for a pipe is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cmd + Shift + M (Mac or Linux)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ctrl + Shift + M (Windows)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;gapminder&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gapminder&lt;/h3&gt;
&lt;p&gt;In this walk through, we’ll be using &lt;a href=&#34;http://www.gapminder.org/world&#34;&gt;Gapminder data&lt;/a&gt;, which represents the health and wealth of nations. It was pioneered by &lt;a href=&#34;https://www.ted.com/speakers/hans_rosling&#34;&gt;Hans Rosling&lt;/a&gt;, who is famous for describing the prosperity of nations over time through famines, wars and other historic events with an interactive data visualization in his &lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34;&gt;2006 TED Talk: The best stats you’ve ever seen&lt;/a&gt; which you can access by selecting the image on the next page.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gapminder.org/world&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;/img/R/R1.png&#34; alt=&#34;Gapminder Motion Chart&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let’s take a look at the gapminder data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(gapminder)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   country     continent  year lifeExp      pop gdpPercap
##   &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Afghanistan Asia       1952    28.8  8425333      779.
## 2 Afghanistan Asia       1957    30.3  9240934      821.
## 3 Afghanistan Asia       1962    32.0 10267083      853.
## 4 Afghanistan Asia       1967    34.0 11537966      836.
## 5 Afghanistan Asia       1972    36.1 13079460      740.
## 6 Afghanistan Asia       1977    38.4 14880372      786.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now using pipes, we could have done&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   country     continent  year lifeExp      pop gdpPercap
##   &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Afghanistan Asia       1952    28.8  8425333      779.
## 2 Afghanistan Asia       1957    30.3  9240934      821.
## 3 Afghanistan Asia       1962    32.0 10267083      853.
## 4 Afghanistan Asia       1967    34.0 11537966      836.
## 5 Afghanistan Asia       1972    36.1 13079460      740.
## 6 Afghanistan Asia       1977    38.4 14880372      786.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are some others using the &lt;code&gt;filter&lt;/code&gt; and &lt;code&gt;select&lt;/code&gt; commands with multiple steps:&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;div id=&#34;no-pipes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;No Pipes&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapusa_filter &amp;lt;- filter(gapminder, country == &amp;quot;United States&amp;quot;)

gapusa_filter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 6
##    country       continent  year lifeExp       pop gdpPercap
##    &amp;lt;fct&amp;gt;         &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 United States Americas   1952    68.4 157553000    13990.
##  2 United States Americas   1957    69.5 171984000    14847.
##  3 United States Americas   1962    70.2 186538000    16173.
##  4 United States Americas   1967    70.8 198712000    19530.
##  5 United States Americas   1972    71.3 209896000    21806.
##  6 United States Americas   1977    73.4 220239000    24073.
##  7 United States Americas   1982    74.6 232187835    25010.
##  8 United States Americas   1987    75.0 242803533    29884.
##  9 United States Americas   1992    76.1 256894189    32004.
## 10 United States Americas   1997    76.8 272911760    35767.
## 11 United States Americas   2002    77.3 287675526    39097.
## 12 United States Americas   2007    78.2 301139947    42952.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapusa_select &amp;lt;- select(gapusa_filter, -continent, -lifeExp)

gapusa_select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 4
##    country        year       pop gdpPercap
##    &amp;lt;fct&amp;gt;         &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 United States  1952 157553000    13990.
##  2 United States  1957 171984000    14847.
##  3 United States  1962 186538000    16173.
##  4 United States  1967 198712000    19530.
##  5 United States  1972 209896000    21806.
##  6 United States  1977 220239000    24073.
##  7 United States  1982 232187835    25010.
##  8 United States  1987 242803533    29884.
##  9 United States  1992 256894189    32004.
## 10 United States  1997 272911760    35767.
## 11 United States  2002 287675526    39097.
## 12 United States  2007 301139947    42952.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;with-pipes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;With Pipes&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapusa &amp;lt;- gapminder %&amp;gt;% 
  filter(country == &amp;quot;United States&amp;quot;) %&amp;gt;%
  select(-continent, -lifeExp) 

gapusa&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 4
##    country        year       pop gdpPercap
##    &amp;lt;fct&amp;gt;         &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 United States  1952 157553000    13990.
##  2 United States  1957 171984000    14847.
##  3 United States  1962 186538000    16173.
##  4 United States  1967 198712000    19530.
##  5 United States  1972 209896000    21806.
##  6 United States  1977 220239000    24073.
##  7 United States  1982 232187835    25010.
##  8 United States  1987 242803533    29884.
##  9 United States  1992 256894189    32004.
## 10 United States  1997 272911760    35767.
## 11 United States  2002 287675526    39097.
## 12 United States  2007 301139947    42952.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using multiple lines you can actually read this like a story and there aren’t temporary variables that get confusing. This reads like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“start with the &lt;code&gt;gapminder&lt;/code&gt; data,
and then filter for the United States,
and lastly drop the variables continent and lifeExp.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mutate-adds-new-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;mutate()&lt;/code&gt; adds new variables&lt;/h3&gt;
&lt;p&gt;Let’s say we needed to add an index column so we know which order these data came in. Let’s not make a new variable, let’s add a column to our gapminder data frame. How do we do that? With the &lt;code&gt;mutate()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Imagine we want to know each country’s annual GDP. We can multiply &lt;code&gt;pop&lt;/code&gt; by &lt;code&gt;gdpPercap&lt;/code&gt; to create a new column named &lt;code&gt;gdp&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  mutate(gdp = pop * gdpPercap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,704 x 7
##    country     continent  year lifeExp      pop gdpPercap          gdp
##    &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.
##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.
##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.
##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.
##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.
##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.
##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.
##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.
##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.
## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.
## # … with 1,694 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;try-these-out&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Try These Out&lt;/h3&gt;
&lt;div id=&#34;on-your-own-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;On Your Own 1&lt;/h4&gt;
&lt;p&gt;Try to figure this out by yourself using pipes and only the &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;mutate&lt;/code&gt;, and &lt;code&gt;select&lt;/code&gt; commands. Compare your syntax with the syntax below. Remember! There are multiple ways to go about finding the outcome.&lt;/p&gt;
&lt;center&gt;
&lt;em&gt;Calculate the population in thousands for all European countries in the year 2007 and add it as a new column.&lt;/em&gt;
&lt;/center&gt;
&lt;br&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Possible solution&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(continent == &amp;quot;Europe&amp;quot;,
         year == 2007) %&amp;gt;%
  mutate(pop_thousands = pop/1000) %&amp;gt;%
  select(country, year, pop_thousands) #this cleans up the dataframe but isn&amp;#39;t necessary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 x 3
##    country                 year pop_thousands
##    &amp;lt;fct&amp;gt;                  &amp;lt;int&amp;gt;         &amp;lt;dbl&amp;gt;
##  1 Albania                 2007         3601.
##  2 Austria                 2007         8200.
##  3 Belgium                 2007        10392.
##  4 Bosnia and Herzegovina  2007         4552.
##  5 Bulgaria                2007         7323.
##  6 Croatia                 2007         4493.
##  7 Czech Republic          2007        10229.
##  8 Denmark                 2007         5468.
##  9 Finland                 2007         5238.
## 10 France                  2007        61084.
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;
If you got it, that’s great! However, it’s absolutely fine if you did not. The best way to learn is to practice (and possibly yell at your computer a few times if that helps).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;group_by-operates-on-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;group_by()&lt;/code&gt; operates on groups&lt;/h3&gt;
&lt;p&gt;What if we wanted to know the total population on each continent in 2002? Answering this question requires a &lt;strong&gt;grouping variable&lt;/strong&gt;. By using &lt;code&gt;group_by()&lt;/code&gt; we can set our grouping variable to &lt;code&gt;continent&lt;/code&gt; and create a new column called &lt;code&gt;cont_pop&lt;/code&gt; that will add up all country populations by their associated continents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year == 2002) %&amp;gt;%
  group_by(continent) %&amp;gt;% 
  mutate(cont_pop = sum(pop))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 142 x 7
## # Groups:   continent [5]
##    country     continent  year lifeExp       pop gdpPercap   cont_pop
##    &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 Afghanistan Asia       2002    42.1  25268405      727. 3601802203
##  2 Albania     Europe     2002    75.7   3508512     4604.  578223869
##  3 Algeria     Africa     2002    71.0  31287142     5288.  833723916
##  4 Angola      Africa     2002    41.0  10866106     2773.  833723916
##  5 Argentina   Americas   2002    74.3  38331121     8798.  849772762
##  6 Australia   Oceania    2002    80.4  19546792    30688.   23454829
##  7 Austria     Europe     2002    79.0   8148312    32418.  578223869
##  8 Bahrain     Asia       2002    74.8    656397    23404. 3601802203
##  9 Bangladesh  Asia       2002    62.0 135656790     1136. 3601802203
## 10 Belgium     Europe     2002    78.3  10311970    30486.  578223869
## # … with 132 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sure this great but what if we don’t care about the other columns and only want each continent and their population in 2002? That leads us to the next function:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summarize-with-group_by&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;summarize()&lt;/code&gt; with &lt;code&gt;group_by()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We want to operate on a group, but actually collapse or distill the output from that group. The &lt;code&gt;summarize()&lt;/code&gt; function will do that for us.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  group_by(continent) %&amp;gt;%
  summarize(cont_pop = sum(pop)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   continent    cont_pop
##   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Africa     6187585961
## 2 Americas   7351438499
## 3 Asia      30507333901
## 4 Europe     6181115304
## 5 Oceania     212992136&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;summarize()&lt;/code&gt; will actually only keep the columns that are grouped_by or summarized. So if we wanted to keep other columns, we’d have to do have a few more steps. &lt;code&gt;ungroup()&lt;/code&gt; removes the grouping and it’s good to get in the habit of using it after a &lt;code&gt;group_by()&lt;/code&gt; because R remembers! We can use more than one grouping variable. Let’s get total populations by &lt;strong&gt;continent&lt;/strong&gt; and &lt;strong&gt;year&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  group_by(continent, year) %&amp;gt;%
  summarize(cont_pop = sum(as.numeric(pop))) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;continent&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 x 3
##    continent  year  cont_pop
##    &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Africa     1952 237640501
##  2 Africa     1957 264837738
##  3 Africa     1962 296516865
##  4 Africa     1967 335289489
##  5 Africa     1972 379879541
##  6 Africa     1977 433061021
##  7 Africa     1982 499348587
##  8 Africa     1987 574834110
##  9 Africa     1992 659081517
## 10 Africa     1997 743832984
## # … with 50 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;arrange-orders-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;arrange()&lt;/code&gt; orders columns&lt;/h3&gt;
&lt;p&gt;This is ordered alphabetically, which is helpful in certain circumstances. But let’s say we wanted to order it in ascending order for &lt;code&gt;year&lt;/code&gt;. The dplyr function to do that is &lt;code&gt;arrange()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  group_by(continent, year) %&amp;gt;%
  summarize(cont_pop = sum(as.numeric(pop))) %&amp;gt;%
  arrange(year) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;continent&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 60 x 3
##    continent  year   cont_pop
##    &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 Africa     1952  237640501
##  2 Americas   1952  345152446
##  3 Asia       1952 1395357351
##  4 Europe     1952  418120846
##  5 Oceania    1952   10686006
##  6 Africa     1957  264837738
##  7 Americas   1957  386953916
##  8 Asia       1957 1562780599
##  9 Europe     1957  437890351
## 10 Oceania    1957   11941976
## # … with 50 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;on-your-own-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;On Your Own 2&lt;/h4&gt;
&lt;p&gt;Try to figure this out by yourself using pipes and some if not all of the commands introduced above. Compare your syntax with the script &lt;strong&gt;On Your Own (Pipes and the NFL) Syntax Set.R&lt;/strong&gt;. Remember! There are multiple ways to go about finding the outcome.&lt;/p&gt;
&lt;center&gt;
&lt;em&gt;Now what is the maximum GDP per continent across all years?&lt;/em&gt;
&lt;/center&gt;
&lt;br&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Possible solution&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  mutate(gdp = pop * gdpPercap) %&amp;gt;%
  group_by(continent) %&amp;gt;%
  mutate(max_gdp = max(gdp)) %&amp;gt;%
  filter(gdp == max_gdp) %&amp;gt;%
  select(-country)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 7
## # Groups:   continent [5]
##   continent  year lifeExp        pop gdpPercap     gdp max_gdp
##   &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Oceania    2007    81.2   20434176    34435. 7.04e11 7.04e11
## 2 Asia       2007    73.0 1318683096     4959. 6.54e12 6.54e12
## 3 Africa     2007    71.3   80264543     5581. 4.48e11 4.48e11
## 4 Europe     2007    79.4   82400996    32170. 2.65e12 2.65e12
## 5 Americas   2007    78.2  301139947    42952. 1.29e13 1.29e13&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;nfl-data-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NFL Data Set&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Grab the data set&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:
&lt;a href=&#34;/data/R/2014-average-ticket-price.csv&#34;&gt;&lt;code&gt;2014-average-ticket-price&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Save it in the same place as your script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Load it up in R&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nfl2014 &amp;lt;- read_csv(&amp;quot;2014-average-ticket-price.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Event = col_character(),
##   Division = col_character(),
##   `Avg TP, $` = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make a good habit, let’s check the file’s characteristics. This is a good habit and can alleviate some pains down the line. It is recommended you run these simple checks on any data set.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check that your set is in a data frame format and get information about your columns and their types (character, numeric, or factor) by running&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(nfl2014)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [108 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ Event    : chr [1:108] &amp;quot;Baltimore Ravens at Pittsburgh Steelers Tickets on 02-Nov-2014 (9037819)&amp;quot; &amp;quot;Pittsburgh Steelers at Baltimore Ravens Tickets on 11-Sep-2014 (9037835)&amp;quot; &amp;quot;Cleveland Browns at Pittsburgh Steelers Tickets on 07-Sep-2014 (9037806)&amp;quot; &amp;quot;Cincinnati Bengals at Pittsburgh Steelers Tickets on 28-Dec-2014 (9037828)&amp;quot; ...
##  $ Division : chr [1:108] &amp;quot;AFC North&amp;quot; &amp;quot;AFC North&amp;quot; &amp;quot;AFC North&amp;quot; &amp;quot;AFC North&amp;quot; ...
##  $ Avg TP, $: num [1:108] 202 199 196 164 148 137 135 102 89 83 ...
##  - attr(*, &amp;quot;spec&amp;quot;)=
##   .. cols(
##   ..   Event = col_character(),
##   ..   Division = col_character(),
##   ..   `Avg TP, $` = col_double()
##   .. )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can do&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(nfl2014)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 108
## Columns: 3
## $ Event       &amp;lt;chr&amp;gt; &amp;quot;Baltimore Ravens at Pittsburgh Steelers Tickets on 02-No…
## $ Division    &amp;lt;chr&amp;gt; &amp;quot;AFC North&amp;quot;, &amp;quot;AFC North&amp;quot;, &amp;quot;AFC North&amp;quot;, &amp;quot;AFC North&amp;quot;, &amp;quot;AFC …
## $ `Avg TP, $` &amp;lt;dbl&amp;gt; 202, 199, 196, 164, 148, 137, 135, 102, 89, 83, 83, 81, 2…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we do have a data frame with 108 rows and three columns as well as two columns that are made up of characters (i.e. letters) and one that is numeric, or made of numbers. These columns are actually called &lt;strong&gt;vectors&lt;/strong&gt;. Please keep this mind as we’ll be referring to the columns as vectors from now on.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check that its a tibble, which is essentially a data frame but tweaked to make it easier to wrangle.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is_tibble(nfl2014)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The differences between the two are worth a brief discussion in a data science course, but not here. Some older functions don’t work with tibbles. If you encounter one of these functions, use the command &lt;code&gt;as.data.frame()&lt;/code&gt; to turn a tibble back to a data.frame like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nfl2014_dfonly &amp;lt;- as.data.frame(nfl2014)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anyway, if you test for both of those and they pass, you will be able to use all of the commands listed above.&lt;/p&gt;
&lt;p&gt;We’ll tweak the second question on the task list a bit to say: “What was the highest and lowest average ticket price for all 2014-2015 NFL games by division?” For this session, we’ll disregard the histogram and answer the question that was asked for on the original.&lt;/p&gt;
&lt;p&gt;To tackle this, let’s think about it logically. Originally you had to&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;inspect the data frame which I called &lt;code&gt;nfl2014&lt;/code&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;remove the columns with &lt;em&gt;NA&lt;/em&gt;,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;create eight different data frames, one for each division, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;derive the mean for each.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;put the values in order.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is a lot of steps! Let’s try using pipes to do this instead:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nfl2014 %&amp;gt;%
  select(Division, `Avg TP, $`) %&amp;gt;%
  group_by(Division) %&amp;gt;%
  na.omit() %&amp;gt;%
  summarize(Mean_by_Division = mean(`Avg TP, $`, na.rm = TRUE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  arrange(desc(Mean_by_Division))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##   Division  Mean_by_Division
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;
## 1 NFC North            179. 
## 2 NFC East             173. 
## 3 NFC West             165. 
## 4 AFC North            135. 
## 5 AFC East             127. 
## 6 AFC West             125. 
## 7 NFC South             95.4
## 8 AFC South             83.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well that was MUCH EASIER! Now what happened? Let’s go through it line by line:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;I called the data frame &lt;code&gt;nfl2014&lt;/code&gt;. This was the name given to the csv file when it was brought in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I selected the two columns I want to know more about. In this case, I can’t compute anything by &lt;code&gt;Division&lt;/code&gt; if I don’t have the numerical data that explains ticket prices &lt;code&gt;Avg TP, $&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since I need the mean by each &lt;code&gt;Division&lt;/code&gt;, grouping by that vector will allow me to find perform any and all operations by each unique value. In this case we have eight divisions, so well have outcomes for each.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;NA&lt;/em&gt;s are a special type of value in R. New users tend to disregard them as blanks or sometimes zeros. However, the name itself &lt;em&gt;NA&lt;/em&gt; means not available aka a missing value. When you see this, it means that R has recognized that a value should be there but its not and depending on the command, it may bring up an error or worst case scenario, an analysis runs and your output is skewed. The command &lt;code&gt;na.omit()&lt;/code&gt; will remove &lt;strong&gt;any row where an&lt;/strong&gt; &lt;em&gt;NA&lt;/em&gt; &lt;strong&gt;is found&lt;/strong&gt;. Please note this differs from removing only rows with &lt;em&gt;NA&lt;/em&gt;s. The distinction, while minute, can have lasting ramifications.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;summarize&lt;/code&gt; tells R that I’m going to reduce the data set using some summary technique. In this case the mean is needed so I constructed a new column named &lt;code&gt;Mean_by_Division&lt;/code&gt; which took the &lt;code&gt;mean&lt;/code&gt; of `&lt;code&gt;Avg TP, $&lt;/code&gt;. Note that since we are using pipes, we do not have to tell R about the name of the data frame. It has been understood since step 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a good rule of thumb, it is always good to &lt;code&gt;ungroup()&lt;/code&gt; your data because the grouping is information that R retains. R remembers!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Arrange the &lt;code&gt;Mean_by_Division&lt;/code&gt; vector in decreasing order. The default is increasing.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;on-your-own-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;On Your Own 3&lt;/h4&gt;
&lt;center&gt;
&lt;em&gt;Now try doing the same as the above except only for AFC teams.&lt;/em&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
There are a lot of ways to accomplish this, but I would suggest filtering with a string. Take a look at the link and see if you can figure out the command to use.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.exploratory.io/filter-with-text-data-952df792c2ba&#34;&gt;Filter with Text data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Of course, a solution is below. Again you are learning how to do this so take some time and for many of you, it may be oddly satisfying to find the output you want after being frustrated.
&lt;br&gt;&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Possible solution&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nfl2014 %&amp;gt;%
select(Division, `Avg TP, $`) %&amp;gt;%
  filter(str_detect(Division, &amp;quot;AFC&amp;quot;)) %&amp;gt;%
  na.omit() %&amp;gt;%
  summarize(AFC_mean = mean(`Avg TP, $`, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   AFC_mean
##      &amp;lt;dbl&amp;gt;
## 1     117.&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;task&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Task&lt;/h2&gt;
&lt;p&gt;For this task first download the &lt;a href=&#34;/data/R/drinks2010andBeyond.csv&#34;&gt;&lt;code&gt;drinks2010andBeyond&lt;/code&gt;&lt;/a&gt; data set&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. This is an amended version of the average serving sizes per person by country as reported by the World Health Organization (WHO), Global Information System on Alcohol and Health (GISAH) since 2010. If you are interested, please visit the &lt;a href=&#34;https://apps.who.int/gho/data/node.main.GISAH?lang=en&#34;&gt;WHO GISAH site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a typical submission&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, you must include&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Your name and task in the top of the script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Separated solutions in proper numerical order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Your code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An answer to the question.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Remember you can leave text that R will ignore by putting a hashtag # in front of it.&lt;/p&gt;
&lt;p&gt;As an example, if I were to submit the altered item 2 from above, my script would read:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Abhik Roy
# EDP 613
# Drinks Task

# 1 blah blah blah


# 2
nfl2014 %&amp;gt;%
  select(Division, `Avg TP, $`) %&amp;gt;%
  group_by(Division) %&amp;gt;%
  na.omit() %&amp;gt;%
  summarize(Mean_by_Division = mean(`Avg TP, $`, na.rm = TRUE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  arrange(desc(Mean_by_Division))

# According to the output, the highest average price paid was $170.00 per ticket paid by
# fans the NFC North whereas the lowest average price ticket was paid by attendees who  
# went to games in the AFC South at $83.30.


# 3 blah blah blah&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now please answer the following questions. Please note that all consumption measurements are in liters:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;First step&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;We have to bring in the data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drinks &amp;lt;- read_csv(&amp;quot;drinks2010andBeyond.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Country = col_character(),
##   `Beverage Types` = col_character(),
##   `2016` = col_double(),
##   `2015` = col_double(),
##   `2014` = col_double(),
##   `2013` = col_double(),
##   `2012` = col_double(),
##   `2011` = col_double(),
##   `2010` = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;
OK now we can move on with the questions! &lt;br&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which country or countries had the highest average consumption of wine and beer in each year?&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Possible solution&lt;/summary&gt;
&lt;p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drinks %&amp;gt;%
  filter(`Beverage Types` == &amp;quot;Beer&amp;quot; | `Beverage Types` == &amp;quot;Wine&amp;quot;) %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  group_by(year, Country) %&amp;gt;%
  summarize(avg = mean(measure)) %&amp;gt;%
  top_n(n=1, avg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;year&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 3
## # Groups:   year [7]
##   year  Country             avg
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;
## 1 2010  Portugal           5.42
## 2 2011  Croatia            5.40
## 3 2012  Equatorial Guinea  5.82
## 4 2013  Equatorial Guinea  5.08
## 5 2014  Austria            5.25
## 6 2015  Slovenia           5.32
## 7 2016  Czechia            4.84&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Breakdown line-by-line:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We call the data frame &lt;code&gt;drinks&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We filter the column &lt;code&gt;Beverage Types&lt;/code&gt; by “Beer” OR “Wine”.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;gather()&lt;/code&gt; command is used to go from a wide data set to a long one while ignoring the application of the command on the columns &lt;code&gt;Country&lt;/code&gt;, and &lt;code&gt;Beverage Types&lt;/code&gt;. To see what occurs, first run the first two lines by highlighting them and ending before the &lt;code&gt;%&amp;gt;%&lt;/code&gt; (pipe) on the second line:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;graphics/partialpipes2lines.png&#34; alt=&#34;Highlight of first two lines of the code.&#34; /&gt;
That simply runs through steps 1 and 2 above. Now run the first three lines in the same way ending prior to the &lt;code&gt;%&amp;gt;%&lt;/code&gt; (pipe) in the third line&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graphics/partialpipes3lines.png&#34; alt=&#34;Highlight of first two lines of the code.&#34; /&gt;
You can find a multitude of resources for &lt;code&gt;gather()&lt;/code&gt; and its complement &lt;code&gt;spread()&lt;/code&gt; but my advice is simply to play around with the &lt;code&gt;key&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt;, and columns you don’t want to make long. However if you like some reference material while you explore these commands, try &lt;a href=&#34;%22https://garrettgman.github.io/tidying/%22&#34;&gt;Data Science for R&lt;/a&gt;. If the link is not work, the address is &lt;a href=&#34;https://garrettgman.github.io/tidying/&#34; class=&#34;uri&#34;&gt;https://garrettgman.github.io/tidying/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our case all of the data is now in relation to the columns &lt;code&gt;Country&lt;/code&gt; and &lt;code&gt;Beverage Types&lt;/code&gt; which gives us the remaining columns (the &lt;code&gt;key&lt;/code&gt;) as data points that have values (the &lt;code&gt;value&lt;/code&gt;) which I have named &lt;code&gt;year&lt;/code&gt; (via &lt;code&gt;key = &#34;year&#34;&lt;/code&gt;) and &lt;code&gt;measure&lt;/code&gt; (via &lt;code&gt;value = &#34;measure&#34;&lt;/code&gt;). Those that do not have associated values become &lt;code&gt;NA&lt;/code&gt;s (for example the country of &lt;code&gt;Afghanistan&lt;/code&gt; did not have any values for &lt;code&gt;Beer&lt;/code&gt; or &lt;code&gt;Wine&lt;/code&gt; in &lt;code&gt;2016&lt;/code&gt;). Please take some time to investigate the &lt;code&gt;gather()&lt;/code&gt; command as R works will with long data sets and not so well with wide ones!&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;na.omit()&lt;/code&gt; is a command you have seen before and it simply sees if there is an &lt;code&gt;NA&lt;/code&gt; in a row and then deleted said row. It performs this recursively throughout your entire data frame.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;group_by()&lt;/code&gt; command notifies R that all operations must be performed with respect to the columns indicated which in this case implies that &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the &lt;code&gt;summarize()&lt;/code&gt; command used here to find the mean of the column &lt;code&gt;measure&lt;/code&gt; is performed by &lt;code&gt;year&lt;/code&gt; and &lt;code&gt;Country&lt;/code&gt;. We assign the results to a new column &lt;code&gt;avg&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally the &lt;code&gt;top_n()&lt;/code&gt; which gives us the top value of the average &lt;code&gt;avg&lt;/code&gt; column.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Again to see how this layering works, I suggest that you run this line by line to observe the results. This is the benefit of the &lt;code&gt;tidyverse&lt;/code&gt; approach.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which country or countries had the highest average consumption of spirits in each year?&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Possible solution&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;The process here is exactly the same as the one described in item 1 except the &lt;code&gt;Beverage Types&lt;/code&gt; are &lt;code&gt;&#34;Spirits&#34;&lt;/code&gt; rather than &lt;code&gt;&#34;Beer&#34;&lt;/code&gt; or &lt;code&gt;&#34;Wine&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drinks %&amp;gt;%
  filter(`Beverage Types` == &amp;quot;Spirits&amp;quot;) %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  group_by(year, Country) %&amp;gt;%
  summarize(avg = mean(measure)) %&amp;gt;%
  top_n(n=1, avg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;year&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 3
## # Groups:   year [7]
##   year  Country   avg
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 2010  Estonia  7.53
## 2 2011  Estonia  8.18
## 3 2012  Estonia  8.53
## 4 2013  Estonia  8.93
## 5 2014  Estonia  8.7 
## 6 2015  Estonia  8.37
## 7 2016  Estonia  7.72&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which country had the highest average consumption of all alcohol across time?&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Possible solution&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;Again, the process here is exactly the same as the one described in items 1 and 2 except the &lt;code&gt;Beverage Types&lt;/code&gt; are &lt;code&gt;&#34;All types&#34;&lt;/code&gt; rather than any individual type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drinks %&amp;gt;%
  filter(`Beverage Types` == &amp;quot;All types&amp;quot;)  %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  group_by(year, Country) %&amp;gt;%
  summarize(avg = mean(measure)) %&amp;gt;%
  top_n(n=1, avg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;year&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 3
## # Groups:   year [7]
##   year  Country   avg
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 2010  Estonia  15.0
## 2 2011  Estonia  16.3
## 3 2012  Estonia  17.0
## 4 2013  Estonia  17.8
## 5 2014  Estonia  17.3
## 6 2015  Estonia  16.6
## 7 2016  Estonia  15.4&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Based on each years average consumption of beer across all counties, how far does the United States deviate from the mean, if at all? Is the mean an appropriate measure of the average here? Why or why not?&lt;br&gt;&lt;br&gt;
This one was fairly difficult. A &lt;em&gt;deviation from the mean&lt;/em&gt; essentially indicates how a single measure differs from an established value of a mean. In our case, how the mean of the United States differs from that of the mean derived from the rest of the countries. Your first thought may be to find the difference between both the means by subtracting them. This would be fine if both had the same standard deviation, but they do not.&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Possible solution&lt;/summary&gt;
&lt;p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# USA only
drinks %&amp;gt;%
  filter(`Beverage Types` == &amp;quot;Beer&amp;quot; &amp;amp; Country == &amp;quot;United States of America&amp;quot;) %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  summarize(avg = mean(measure), stanDev = sd(measure)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##     avg stanDev
##   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1  4.22  0.0699&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remaining countries
drinks %&amp;gt;%
  filter(`Beverage Types` == &amp;quot;Beer&amp;quot; &amp;amp; Country != &amp;quot;United States of America&amp;quot;) %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  summarize(avg = mean(measure), stanDev = sd(measure)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##     avg stanDev
##   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1  2.06    1.80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that our &lt;code&gt;filter()&lt;/code&gt; command again uses an AND command rather than an OR and rather than filtering using one column, we use two (&lt;code&gt;Beverage Types&lt;/code&gt; AND &lt;code&gt;Country&lt;/code&gt;) but in the latter we we want to exclude the United States of America by using &lt;code&gt;Country != &#34;United States of America&#34;&lt;/code&gt;. So interpreting this, we can say that we’re looking for figures for all Beer consumption around the world except for the United States of America.&lt;/p&gt;
&lt;p&gt;So what can we do? At this point not much! In the future we should be able to address this issue by using a statistical test known as a &lt;em&gt;t&lt;/em&gt;-test but we need to satisfy some initial prerequisites first. We will explore this in R5 but for now, we’ll leave it here.&lt;/p&gt;
On a side note, it is absolutely fine if you subtracted the means! That would be the most logical choice and you also did not have much of a choice at the current time. Consider that the issue of subtracting means with differing standard deviations is that the spread differs between each and while a mean is a mean, they’re not measures of the mean on the same footing - ergo all things are not equal.
&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Since we measure beer in the United States in ounces (for some reason) rather than in liters, convert the 2016 measures into ounces using the conversion 1 Liter = 33.814022 oz.&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Possible solution&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;One of the nicer aspects of using &lt;code&gt;%&amp;gt;%&lt;/code&gt; is in the fact you can selectively perform operations and general augmentations on the columns of your choice without ever having to worry about how any other columns are affected. This is actually accomplished using the command &lt;code&gt;mutate()&lt;/code&gt; which in a nutshell allows for column wise operations.&lt;/p&gt;
&lt;p&gt;In this task, we are informed that the column &lt;code&gt;2016&lt;/code&gt; is given in liters. However since we and a handful of other countries have chosen to use a measurement that are nowadays based off the metric system (yes that’s true..for example an inch is officially defined as 2.54 centimeters) and not the metric system itself (base 10 seems pretty easy but what do I know), we have to convert on order to report. In this case we are given the easy to remember 1 Liter = 33.814022 (fluid) ounces. To convert this, we use the &lt;code&gt;mutate()&lt;/code&gt; command by
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drinks %&amp;gt;%
  mutate(`2016` = 33.814022 * `2016`) %&amp;gt;%
  rename(`2016 (using garbage measurements)` = `2016`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 954 x 9
##    Country `Beverage Types` `2016 (using ga… `2015` `2014` `2013` `2012` `2011`
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;                       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 Afghan… All types                   NA      0.02   0.03   0.03   0.04   0.04
##  2 Afghan… Beer                        NA      0.01   0.01   0.01   0.01   0.01
##  3 Afghan… Wine                        NA      0      0      0      0      0   
##  4 Afghan… Spirits                     NA      0.02   0.02   0.02   0.03   0.03
##  5 Afghan… Other alcoholic…            NA      0      0      0      0      0   
##  6 Albania All types                  171.     4.77   4.81   5.06   5.43   5.65
##  7 Albania Beer                        58.8    1.57   1.58   1.82   1.81   1.88
##  8 Albania Wine                        45.3    1.17   1.12   1.06   1.3    1.27
##  9 Albania Spirits                     64.2    1.94   2.02   2.09   2.22   2.4 
## 10 Albania Other alcoholic…             3.04   0.08   0.09   0.09   0.1    0.1 
## # … with 944 more rows, and 1 more variable: `2010` &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;with the last line being dedicated to renaming the column that was just converted from &lt;code&gt;2016&lt;/code&gt; to &lt;code&gt;2016 (using garbage measurements)&lt;/code&gt;. Notice that the &lt;code&gt;rename()&lt;/code&gt; command requires that the new name of a column precede the current name separated by and &lt;code&gt;=&lt;/code&gt;. I realize this appears to be counterintuative and there is a reason for it, but unless you love discussing how R compiles code, I’ll leave it be.&lt;/p&gt;
&lt;p&gt;On a side note, we only converted the column &lt;code&gt;2016&lt;/code&gt;. What if we wanted to convert every column? Well we’d be using a lot of &lt;code&gt;%&amp;gt;%&lt;/code&gt; and wasted energy. This is one of many instances where &lt;code&gt;gather()&lt;/code&gt; is a great command to know. Switching from a wide data frame to a long one allows you to convert all years at once. We can do this by&lt;/p&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drinks %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  mutate(`measure` = 33.814022 * `measure`) %&amp;gt;%
  rename(`Measures (in ounces because that&amp;#39;s apparently easy)` = `measure`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6,016 x 4
##    Country `Beverage Types`       year  `Measures (in ounces because that&amp;#39;s app…
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;                  &amp;lt;chr&amp;gt;                                    &amp;lt;dbl&amp;gt;
##  1 Albania All types              2016                                    171.  
##  2 Albania Beer                   2016                                     58.8 
##  3 Albania Wine                   2016                                     45.3 
##  4 Albania Spirits                2016                                     64.2 
##  5 Albania Other alcoholic bever… 2016                                      3.04
##  6 Algeria All types              2016                                     18.9 
##  7 Algeria Beer                   2016                                     10.8 
##  8 Algeria Wine                   2016                                      4.73
##  9 Algeria Spirits                2016                                      3.38
## 10 Algeria Other alcoholic bever… 2016                                      0   
## # … with 6,006 more rows&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unlike the piping we used earlier, we aren’t filtering for anything (though we could if we wanted by putting the filters back in). The rest is pretty much a combination of previous column-wise wrangling. Try running the first three lines. You will once again notice that the &lt;code&gt;measure&lt;/code&gt; column is the associated liter count by year. With that, we mainly need to use &lt;code&gt;mutate()&lt;/code&gt; on the &lt;code&gt;measure&lt;/code&gt; column to convert it.&lt;/p&gt;
&lt;p&gt;Learning both &lt;code&gt;gather()&lt;/code&gt; and to some degree &lt;code&gt;spread()&lt;/code&gt; which is not covered in this walk-through will save you a great deal of time. It is worth taking the time now to explore what you can do with it.&lt;/p&gt;
&lt;/details&gt;
&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Explore the data set a bit and play around with pipes. Provide one outcome you derived not already asked about that you (hopefully) found interesting.&lt;/li&gt;
&lt;/ol&gt;
&lt;details&gt;
&lt;summary&gt;Possible solution&lt;/summary&gt;
&lt;p&gt;
&lt;p&gt;Of course answers will vary here but we can see the values for each country by year&lt;/p&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coalesce_by_column &amp;lt;- function(df) {
  return(dplyr::coalesce(!!! as.list(df)))
}

drinks %&amp;gt;%
  gather(key = &amp;quot;year&amp;quot;, value = &amp;quot;measure&amp;quot;, -Country, -`Beverage Types`) %&amp;gt;%
  na.omit() %&amp;gt;%
  rowid_to_column(&amp;quot;Country ID&amp;quot;) %&amp;gt;%
  spread(`Beverage Types`, value = `measure`) %&amp;gt;%
  select(-`Country ID`) %&amp;gt;%
  group_by(Country, year) %&amp;gt;%
  summarise_all(coalesce_by_column) %&amp;gt;%
  na_if(0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,202 x 7
## # Groups:   Country [190]
##    Country     year  `All types`  Beer `Other alcoholic beverages` Spirits  Wine
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;                       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Afghanistan 2010         0.03  0.01                       NA       0.02 NA   
##  2 Afghanistan 2011         0.04  0.01                       NA       0.03 NA   
##  3 Afghanistan 2012         0.04  0.01                       NA       0.03 NA   
##  4 Afghanistan 2013         0.03  0.01                       NA       0.02 NA   
##  5 Afghanistan 2014         0.03  0.01                       NA       0.02 NA   
##  6 Afghanistan 2015         0.02  0.01                       NA       0.02 NA   
##  7 Albania     2010         5.53  1.72                        0.12    2.39  1.3 
##  8 Albania     2011         5.65  1.88                        0.1     2.4   1.27
##  9 Albania     2012         5.43  1.81                        0.1     2.22  1.3 
## 10 Albania     2013         5.06  1.82                        0.09    2.09  1.06
## # … with 1,192 more rows&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where &lt;code&gt;NA&lt;/code&gt; represents values not reported.&lt;/p&gt;
&lt;p&gt;On a side note, there is an easier way to run &lt;code&gt;spread()&lt;/code&gt; and it is called &lt;a href=&#34;https://dcl-wrangle.stanford.edu/pivot_basic.html#wider&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pivot_wider()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;
&lt;/details&gt;

&lt;/div&gt;
&lt;div id=&#34;excusive&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logical Operators&lt;/h2&gt;
&lt;center&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
R syntax
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Idea
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Logic
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
!
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
NOT
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
!a
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
&amp;amp;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
vector-based AND (value-wise)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a &amp;amp; b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
&amp;amp;&amp;amp;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
value-based AND (single value)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a &amp;amp;&amp;amp; b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
|
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
vector-based inclusive OR (value-wise)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a | b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
||
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
value-based inclusive OR (single value)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a || b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
xor
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
vector-based exclusive OR (value-wise)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
xor(a,b)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
&amp;lt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
LESS than
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a &amp;lt; b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
&amp;gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
GREATER than
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a &amp;gt; b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
==
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
exactly EQUALS
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a == b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
&amp;lt;=
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
LESS than or EQUAL to
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a &amp;lt;= b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
&amp;gt;=
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
GREATER than or EQUAL to
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a &amp;gt;= b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
!=
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
NOT EQUAL to
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a != b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
%in%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
included in?
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
a %in% b
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
isTRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
test if something is TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
isTRUE(a)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;The use of &lt;code&gt;%&amp;gt;%&lt;/code&gt; will get easier. Some people think of what steps they use when using a spreadsheet software (e.g. Microsoft Excel ©) when learning the logic of R. In fact, many of the formulas and syntax used in spreadsheet softwares parallel those in R. If you are used to Excel for example, try checking out &lt;a href=&#34;%22https://www.rforexcelusers.com/&#34;&gt;R for Excel Users&lt;/a&gt;. If the link is broken for some reason, head over to &lt;a href=&#34;https://www.rforexcelusers.com/&#34; class=&#34;uri&#34;&gt;https://www.rforexcelusers.com/&lt;/a&gt;. Please note that I do not receive any compensation in any form for recommending this site nor do I take responsibility for its content. For assistance regarding the site, please contact the authors&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Science&lt;/h2&gt;
&lt;p&gt;Data science uses the statistics you will learn in this course and to some degree, in the follow-up course to tackle real life data. According to Glassdoor, the average salary for a data scientist is 113,436 USD (range: 95,000 USD - 250,000 USD; &lt;a href=&#34;https://www.burtchworks.com/wp-content/uploads/2018/05/Burtch-Works-Study_DS-2018.pdf&#34; target=&#34;_blank&#34;&gt;Burtch-Works, 2018&lt;/a&gt;). If you are interested in learning more about Data Science, a course is tentatively scheduled for the Fall 2021 term in addition to Data Visualization in Spring 2021. Both of these courses run very differently from the statistics courses and are focused on exploring data sets of interest to students. If interested, please let me know or stay tuned.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;A great deal of material presented here was adapted from two online texts: &lt;a href=&#34;%22https://ohi-science.org/data-science-training/&#34; target=&#34;_blank&#34;&gt;Introduction to Open Data Science&lt;/a&gt; and &lt;a href=&#34;%22https://r4ds.had.co.nz/%22&#34; target=&#34;_blank&#34;&gt;R for Data Science&lt;/a&gt;. If interested in how R is used in data science, try taking a look.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dont-get-bogged-down&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Don’t Get Bogged Down!&lt;/h2&gt;
&lt;p&gt;Just get a feel for the items above. There is a whole bunch of commands and ridiculous syntax above but that’s not the point. Concentrate on knowing what to do (e.g. “Is this best represented by a bar plot?”) rather than how to do it (“The &lt;code&gt;ggplot&lt;/code&gt; command is…”). You can ALWAYS search for the syntax associated with whatever in R but if you don’t how to conceptually solve a problem, software won’t help. Remember computers are stupid!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;From the FiveThirtyEight &lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/nfl-ticket-prices&#34; target=&#34;_blank&#34;&gt;NFL Ticket Prices Github Page&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;If you are getting an error, remember to point R to the right location by &lt;code&gt;Session &amp;gt; Working Directory &amp;gt; To Source File Location&lt;/code&gt; in the menu.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;From the FiveThirtyEight &lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/alcohol-consumption&#34; target=&#34;_blank&#34;&gt;Alcohol Consumption Github Page&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This is just for practice so you do not have to turn anything in!&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Note that &lt;strong&gt;OR&lt;/strong&gt; here means what is known as an &lt;strong&gt;inclusive or&lt;/strong&gt;…aka &lt;em&gt;it this or that or both&lt;/em&gt;. Using this idea, the second line search reads like &lt;code&gt;Beverage Types&lt;/code&gt; for “Beer” OR “Wine” OR both. The other type of OR is called an &lt;strong&gt;exclusive or&lt;/strong&gt; ..aka &lt;em&gt;it this or that but NOT both&lt;/em&gt;. If you ever have to use it, it is given by the command &lt;code&gt;xor()&lt;/code&gt;. Included on the &lt;a href=&#34;#exclusive&#34;&gt;last page of this document&lt;/a&gt; are some standard logical operators for your reference.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
