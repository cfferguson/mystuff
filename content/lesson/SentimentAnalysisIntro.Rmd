---
title: "Week 15: Something Fun - An Introduction to Sentiment Analysis Using (My Favorite Band): The Grateful Dead"
output:
  blogdown::html_page:
    toc: true
menu:
  lesson:
    parent: Walkthroughs
    weight: 4
type: docs
weight: 3
fig_crop: false 
latex-engine: xelatex
header-includes:
  - \usepackage{textcomp}
urlcolor: blue
---
\begin{center}
\includegraphics[width=3cm]{syf_highres.png}
\end{center}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(global.par = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(patchwork)
library(knitr)
library(kableExtra)
```

# Introduction

## Preparation

Download a [script](/scripts/SentimentAnalysisIntro.R){target="_blank"} file of just the R chunks used in this walkthrough.

## Sentiment Analysis
Without getting into the specifics of machine learning, a **sentiment analysis** is a quantitative process of determining whether a piece of writing is positive, negative or neutral. It entails using a mix of statistics (you know...that old chestnut), [natural language processing (NLP)](https://www.sas.com/en_us/insights/analytics/what-is-natural-language-processing-nlp.html){target="_blank"}, and [machine learning](https://www.ibm.com/cloud/learn/machine-learning){target="_blank"} to identify and extract subjective information from text files, for instance, a reviewerâ€™s feelings, thoughts, judgments, or assessments within open text. If you want to know more about this method, Qualtrics provides a nice readable [writeup](https://www.qualtrics.com/experience-management/research/sentiment-analysis/){target="_blank"} about it.  

## Learning by Doing
We are essentially going to learn about a sentiment analysis using music...well music that I like anyway. In this session we'll be looking at some Grateful Dead lyrics from two albums. 

With that said, let's load up some libraries. If you don't have some of these (and you most likely don't), remember to download them first using `Tools > Install Packages`.

```{r echo=FALSE, eval = TRUE}
par(mar=c(3,3,3,0)) #it's important to have that in a separate chunk
``` 

```{r library, echo = TRUE, message = FALSE}
# We are using the tidyverse family which is essentially the gold standard for 
# data wrangling:
library(tidyverse) 
library(scales)

# text mining and annoying wordclouds based on the tidyverse family
library(tidytext)
library(ggwordcloud)
library(textstem)
library(textdata)
library(wordcloud)

# getting lyrics
library(genius) # Allows you to download lyrics for an entire album in a tidy format from
# genius.com

# the aesthetics
library(viridis)
library(RColorBrewer)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()
```

## Extra Information

More information about 
* the `tidyverse` family of packages are introduced by selecting [https://www.tidyverse.org/](https://www.tidyverse.org/){target="_blank"}.

* genius.com can be found here [https://genius.com/](https://genius.com/){target="_blank"}. 

* the genius package can be found via this link
[https://github.com/josiahparry/genius](https://github.com/josiahparry/genius){target="_blank"}.

## Getting Data

Grab the lyrics for ***American Beauty*** (1971) which was more about rock and roll and ***Shakedown Street*** (1978) which was influenced by disco (ugh yes...disco). Were going to see if the sentiments associated within each album changed over time. On a side note, feel free to grab albums and lyrics that you like instead but the example will be based on the two albums so I would suggest going through it with these first and then doing your own. We'll compare the two using a sentiment analysis.

```{r American_Beauty_gather}
american_beauty <- genius_album(artist = "The Grateful Dead", album = "***American Beauty***")
```

To see the results, just type in the following:
```{r}
american_beauty
```

Now let's do the same for ***Shakedown Street***
```{r Shakedown_Street_gather}
shakedown_street <- genius_album(artist = "The Grateful Dead", album = "***Shakedown Street***")
```

And the results look like:
```{r}
shakedown_street
```

To find out the internal structure of one of your data sets, run this:
```{r Shakedown_Street_structure}
str(shakedown_street)
```

This gives you a good bit of information about the structure of your data set.
* The class of the data are in multiple formats: `tbl_df` (tabular data frame), `tbl` (table), and `data.frame` (normal data frame). This implies that you can use common commands to wrangle the data within this variable.

* You have four (4) internal variables and 304 lines in this data frame (think an excel document which has 4 columns and 304 rows).

* Notice that the track_title (track title) and `lyric` (lyrics) columns are character vectors while `track_n` (track number) and `line` (line number) are integer vectors. These are important because what you can do with any vector is mostly dependent on its type. 

# Data Wrangling
If we're going to compare the two albums, we should probably put them in the same data frame. But to make sure we can distinguish each, we're going to have to tag them by album name. To do this, we need to add a column. The package `dplyr` let's you mess around with the data without destroying the data frame format. It also lets you deal with pipes (`%>%`) which is a fantastic thing. During the dark days of R before pipes, you had to go back and rerun any R code that you made changes to which meant you had to keep track of everything. In any case, when using pipes, to do (nearly) anything to the data frame, you must use the command `mutate`.

Remember we are adding a column that denoted the album name:
```{r American_Beauty_scope}
american_beauty_tagged <- american_beauty %>% 
  mutate(album = "***American Beauty***") %>%
  select(album, track_title, track_n, line, lyric)
```

<br>
<hr>
<br>

Ok so let's take a moment to get our bearings straight. Currently we are

1. using the `american_beauty` data set

2. adding a column named album and then populating it with the text ***American Beauty*** (in quotes); and

3. rearranging the columns. By default, any new columns are put at the end of a data frame. Just to satisfy my obsessive nature, we'll rearrange them in order of scope by putting the album name in front.

<br>
<hr>
<br>

Good? Ok then let's take a look at the data set now:
```{r}
american_beauty
```

Sure enough, there's the column with the album name. So now let's do the same for the other album

```{r Shakedown_Street_scope}
shakedown_street_tagged <- shakedown_street %>% 
  mutate(album = "***Shakedown Street***") %>%
  select(album, track_title, track_n, line, lyric)
```

and verify:
```{r}
shakedown_street
```

Great those look good. Now to merge the data sets, we are going to use a command called `rbind`. The single requirement is that both data frames have the same column names since it needs to know what columns go with what (but notice that having the same column names implies that the columns do not have to be in the same order!)

```{r binded_albums}
all_albums <- rbind(american_beauty_tagged, shakedown_street_tagged)
```

If you simply do the following
```{r}
all_albums
```

Notice that you can't actually observe both data frames were *stacked*, or *bound.* To do this, or at least get an indicator that the binding worked, you can find the *unique* values in a column by doing

```{r}
unique(all_albums$album)
```

Sure enough, it looks like both albums are there.

## Exploring Data
OK now to do some exploratory data analysis (EDA) which have to be displayed in a useless format on par with *pie charts* - that is *wordclouds*. Let's tidy the data sets.

First we are going to tokenize the lyrics into a tidy dataframe
```{r American_Beauty_tidy}
tidy_lyrics<- all_albums %>% 
  group_by(album) %>% 
  unnest_tokens(word, lyric) 
```

So here we are 
1. using the all_albums data set we just created;

2. grouping by album name which means all operations are done within the context of each album; and

3. tokenizing the data set which basically is a fancy way of saying we're splitting up lyrics into words within each album.

Take a look at what this data frame now looks like by using 
```{r}
tidy_lyrics %>%
  head()
```

Or simply use `View(tidy_lyrics)` to get a full view in a separate tab.

## Lemmatize Words
For grammatical reasons, lyrics are going to use different forms of a word known as [morphological derivations](https://essentialsoflinguistics.pressbooks.com/chapter/6-5-unsure-how-to-format-that-derivational-morphology/){target="_blank"} such as drink, drinks, and drinking to name a few. Additionally, there are families of derivationally related words with similar meanings known as differentiated inflections such as democracy, democratic, and democratization. In many situations including lyrics, it is useful to use the basic term for each of these variants. For this we have **stemming** and **lemmatization**. 

  * **Stemming** refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.
  
  * **Lemmatization** refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.

The `textstem` package can do both, but for reasons associated with robustness, we'll use lemmatization.

```{r lemma_words}
tidy_lyrics$word <- lemmatize_words(tidy_lyrics$word)
```

The command (right) above takes the column `word` in `tidy_lyrics`, lemmatizes it, and then replaces the column `word` in `tidy_lyrics` (left) with the results. Take a look using `View(tidy_lyrics)`.

## Using Stop Words
Now that we have a list in a tidy format, let's do something with it. First let's remove all of the stop words. Recall that these are terms that are commonly used (e.g. the, an, in, etc) and for this type of analysis, are noise and basically useless. To do this, we're going to use some logic! The approach that is the most efficient besides making someone else do it is to utilize *joins*. If you want to know more about joins in R - which you definitely should get familiar with as it will make merging of data sets so much easier in your life - check out [Dr. Jenny Bryant's Stat545 course](https://stat545.com/bit001_dplyr-cheatsheet.html){target="_blank"}.

In our case, we'll be using an `anti_join` which basically gives you an output by finding the rows of the first table cannot that cannot be matched in the second table. But before that, let's pull out the stop words from the `tidytext` package by doing

```{r tidytext_stopwords}
data("stop_words")
```

and then we'll remove those words from our tidy data frame using an `anti_join`

```{r join_stopwords}
tidy_lyrics_nsw <- tidy_lyrics %>%
  ungroup() %>%
  anti_join(stop_words)
```

<br>
<hr>
<br>

Ok so let's take another moment to get our bearings straight. Currently we are

1. using the `tidy_lyrics` data set that we created; 

2. ungrouping the data set; and

3. removing all of the stop words from that data frame.

<br>
<hr>
<br>

Now you can take a look by using `View(tidy_lyrics_nsw)` or just look at the top six using the following syntax

```{r}
tidy_lyrics_nsw %>%
  head()
```

\newpage
Not seeing what you want? How about the bottom six then?
```{r}
tidy_lyrics_nsw %>%
  tail()
```

We can remove additional words manually but for the purposes of this walkthrough, let's not. 

## Descriptive Statistics

Until recently, the quantitative approach to analyze open text was in the use of word clouds. For all of my griping, this isn't necessarily a flawed approach since it will indicate if some words are more important than others but they are *descriptive* at best. Historically, the issue has been related to certain scholars making overarching claims, or *inferences* from what are essentially visual representations of frequency counts which you should know by now is nonsense. However, with the advent of machine learning as a methodological tool, we now use inferential statistics to derive usable outcomes though there are still some people who stick by frequencies. In any case, let's calculate some frequencies to get an idea of how our data looks. 

```{r frequencies}
frequencies_lyrics <- tidy_lyrics_nsw %>%
  group_by(album) %>%
  count(word, sort = TRUE)
```

and then plot them on a wordcloud. At this point, if you are unfamiliar or uncomfortable with `ggplot`, I  suggest going through or reviewing the first `ggplot` section on [datacamp](https://www.datacamp.com/){target="_blank"}. With that said, there will also be a brief explanation below of what's used below the plot. 

OK if you're ready, run the following but be forewarned, it may take some time!

### Wordclouds

```{r wordcloud}
set.seed(99)
ggplot(frequencies_lyrics, aes(label = word, size = n, color = album)) +
  geom_text_wordcloud() +
  scale_radius(range = c(0, 20)) +
  theme_minimal()
```

So what's going on above? Well let's break it down:

1. `ggplot(frequencies_lyrics, aes(label = word, size = n, color = album))` tells R to use the `ggplot2` package by looking at the data set `frequencies_lyrics` with the aesthetics (`aes`) of labeling the data by the word, sizing each by its corresponding frequency (`n`), and color by album title (`album`).

2. `geom_text_wordcloud()` tells R to use the `ggwordcloud` package and that text will be used to represent the data and to put it in a spiral.

3. `scale_radius(range = c(0, 20))` tells `ggplot` that the smallest a data point can be is 0 points and the largest is 20 points.

4. `theme_minimal` is a `ggplot` theme that removes all default(background, axes, etc.) information and just plots what you ask for.

You may notice that I did not go over `set.seed`. That is for another time because it deals with probabilities that are contingent on how data is analyzed and/or rendered by each package in R.

OK great but that takes too long to render, its not very pretty nor does it do a great job of splitting the words apart - we do want to compare them after all! Let's make it better:


```{r wordcloud_custom_actual, echo=TRUE, eval = TRUE}
set.seed(99)
frequencies_lyrics %>%
  filter(n > 1) %>%
  ggplot(aes(label = word, size = n, color = album)) +
  scale_color_manual(values = c("#5bc0de", "#5cb85c")) +
  geom_text_wordcloud(rm_outside = TRUE, shape = "circle") +
  scale_radius(range = c(4, 15)) +
  theme_minimal() +
  facet_grid(.~ album) +
  theme(panel.spacing = unit(0.5, "cm"))
```

That looks better and we know what words come from what album. So what's different here?
Well first of all, it is worth noting that not all of the syntax from above will be explained here simply due to the fact they are included for aesthetics and have nothing to do with the function creating the actual wordcloud. However, you are given the entire code so that the modified output is or will be at some point understandable. If you would like to know what this looks like without the extra aesthetics for comparison, run the following in your companion R script:

```{r wordcloud_naked, echo=TRUE, eval = FALSE}
set.seed(99)
frequencies_lyrics %>%
  filter(n > 1) %>%
ggplot(aes(label = word, size = n, color = album)) +
  scale_color_manual(values = c("#5bc0de", "#5cb85c")) +
  geom_text_wordcloud(rm_outside = TRUE, shape = "circle") +
  scale_radius(range = c(4, 15)) +
  theme_minimal() +
  facet_grid(album ~ .)
```

Now back to the pending question that has been updated a bit: *What are the major differences here that aren't related to the aesthetics?*

1. If you call `frequencies_lyrics`, you'll notice that there are 684 rows implying that R has to plot 684 words. That takes some time! To reduce the time and before even calling `ggplot`, let's get rid of the "one offs", or those terms that only appear once. Here we 

  * called the original data set, 
  
  * filtered the frequency `n` by telling R to only look at values greater than 1, and
  
  * then called `ggplot`. One side note, notice that we don't called the data set again in `ggplot` since we already did that when filtering. The `%>%` gives you the control to pass information through pipes. Much like anything Mario...pipes make everything better.

2. I have manually assigned colors by using the `scale_color_manual` command and inputting hexadecimal, or hex codes. These are one of two ways internet browsers determine what colors you see on screen (the other is called RBG) and most computer languages recognize them too. You can Google hex codes and find all sorts of examples. I suggest using this site which also includes user generated palettes, but obviously there is no mandate: [https://www.color-hex.com/](https://www.color-hex.com/){target="_blank"}.

3. (same as before)

4. To make some of the text appear readable, the range of `scale_radius` was changed to reflect a minimum of 2 points.

5. (same as before)

6. `facet_grid` (and its counterpart `facet_wrap` - not shown here) is a way of splitting up a visualization by a value in a column within your data frame. If you were confused as to why the album name was important and that we binded the original data frames, this is the reason! There are ways to customize a facet but we'll get to that later.

*NOTE: If you see some blurred or overlapping text, just push the Zoom option right above the graphics window.*

You can find more information, examples, and usable syntax/lines of code for `ggwordcloud` by selecting going to this [vignette](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html){target="_blank"}.


### Bar Plots

One avenue to look at when comparing sets of open ended text is to compare terms, in that are common terms used? We can do this or [we can dance if we want to](https://www.youtube.com/watch?v=AjPau5QYtYs){target="_blank"}. Let's say we don't do the latter:

```{r frequency_by_facet, echo=TRUE, eval = TRUE}
frequencies_lyrics %>%
  filter(n > 5) %>%
  group_by(album) %>% 
  count(word, sort = TRUE) %>%
  mutate(word = reorder(word, n)) %>% 
  ungroup() %>% 
  mutate(album = reorder(album, n)) %>% 
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity") +
  facet_grid(~ album) +
  xlab(NULL) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8))
```

There is a lot going on here! While some of these items may be repetitive, let's break it down anyway:

1. The `frequencies_lyrics` data set has been called.

2. We use `filter(n > 5)` to filter out all of the word counts that are 5 or less. If you are using your own data set, this will vary.

3. Since we are looking at albums, we should first pair those out by using ` group_by(album)`.

4. Using `count(word, sort = TRUE)`, we count the number of words. Now you may be saying something like "Wait! didn't we already do that?" Well we did but we counted them across both albums basically ignoring which word came from what album. This time, when we used `group_by(album)`, the system now performs all operations by album. So words are now counted within each album.

5. If you run the entire pipe chain up to this `count(word, sort = TRUE)`, you may have noticed that the words are in order by counts. We can reorder the column `word` by a particular word and then their respective counts `n` by using `mutate(word = reorder(word, n))`. 

6. When you group something, that structure is maintained indefinitely. To remove it, we use the common `ungroup()`.

7. Similar to step 5, we can reorder the column `album` by a particular album and then their respective counts `n` by using `mutate(album = reorder(album, n))`. 

At this point, you may realize we are starting to plot. The great thing about the tidyverse universe is that you can do operations and plot in one fell swoop.

8. `ggplot(aes(word, n))` tells R that we're going to plot something using ggplot using the a `word` on the *x*-axis and its corresponding count `n` on the y-axis.

9. `geom_bar(stat = "identity")` indicates that we are going to use as bar plot. For the time being, we won't discuss the `stat = identity` part except to tell you that when a *y* variable is declared, `geom_bar` requires `stat = identity`.

10. `facet_grid(~ albums)` simply means we are going to facet, or ploy by albums.

11. `xlab(NULL)` tells the system to ignore the label on the *x*-axis which in this case is word.

12. Running the command up to this point yields vertical bar plots which are somewhat difficult to compare. They would probably be easier to interpret if the bars were horizontal. Well `coord_flip()` does just that! This flips the *x*- and *y*-axes as well.

13. We will discuss themes at a later point but so you can simply read the vertical axis with the words, we are going to make the font size 8 (pixels) by stating `theme(axis.text.y = element_text(size = 8))`. 

# Sentiment Analysis

Now we'll investigate the various sentiments and emotions expressed in the lyrics by using three sentiment dictionaries or **lexicons**,  included with the `tidytext` package: **Bing**, **NRC**, and **AFINN**. I won't cover what these are in great depth but in a nutshell

```{r, echo=FALSE}
lexicons <- tibble(
  
  lexicon = c("Bing", 
            "NRC", 
            "AFINN"),
  
  description = c("A backend of [Microsoft Bing](https://www.bing.com/){target='_blank'}), this lexicon may be used to assesses open text for its polarities in sentiments.", 
                  "One of the most utilized and researched lexicons that provides information about emotional context.",
                  "A lexicon used for measuring psychological valence by assigning a level of severity to a term."),
  
  `variable type` = c("categorical",
                      "categorical",
                      "numerical"),
  
  sentiments = c("positive, negative", 
                 "anger, anticipation, disgust, fear, joy, sadness, surprise, trust", 
               "-5,-4,-3,-2,-1,0,1,2,3,4,5"),
  
  `more information` = c("[Bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html){target='_blank'}",
    "[NRC](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm){target='_blank'}",
    "[AFINN](https://github.com/fnielsen/afinn){target='_blank'}")
  
) 

lexicons %>%
  kable(align = 'lllc',
        booktabs = TRUE, 
        linesep = "") %>%
  kable_styling(full_width = FALSE) 
```

We can categorize the lyrics and then perform some text based transformations and manipulations to construct some visualizations.

Using the `tidy_lyrics_nsw` data set that we created earlier, we will filter out any numbers in our data set and 

1. Get the **Bing** lexicon with the get_sentiments() function.

2. Join the **Bing** lexicon to the tokenized data set, specify by = "word"

```{r removing_numbers, echo=TRUE, eval = TRUE}
emotions_lyrics_bing <- tidy_lyrics_nsw %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  group_by(album) %>%   
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment))

emotions_lyrics_bing
```


Again, there is a lot going on here! Let's break it down line by line:

1. The `tidy_lyrics_nsw` data set has been called. We are using this rather than `tidy_lyrics`  because the stop words were removed.

2. We use `filter(!grepl('[0-9]', word))` to filter out any numbers in our data set, namely in the `word` column. 

3. To get the **Bing** lexicon and compare it to our list of words, we use `left_join(get_sentiments("bing"), by = "word")`.

4. Like before, we used `group_by(album)` to  perform all operations by album (or within each album if you prefer). 

5. To perform most any operation on a data set when using `%>%`, we use `mutate`. In this case, we are using a logic statement to tell the program if it sees an entry with `NA` in the sentiment column, change it to the term `neutral`.

For a total count,  we delineate the sentiments and count how many are in each by album using `count(sentiment)` which uses the `sentiment` column.

```{r counts_by_term, echo=TRUE, eval = TRUE}
emotions_lyrics_bing %>%
  count(sentiment)
```

## Basic Text Analysis

### Most Common Positive and Negative Words in a Bar Graph

We can associate terms tagged with a negative sentiment with negative numbers to better visualize them. To accomplish this, we can create a logic statement called an if-else statement that assigns -n to a word with a negative sentiment and to everything else, it provides an n count. Weâ€™ll then order the words in these groups by the number of times they appear in the plot.

```{r tokenizing, echo=TRUE, eval = TRUE}
word_count <- emotions_lyrics_bing %>% 
  count(word, sentiment, sort = TRUE) 

word_count
```

In the above, we are going to

1. Use the `emotions_lyrics_bing` data set.

2. Count up the words by sentiment and then sort them by `count(word, sentiment, sort = TRUE)`


```{r filter_top_sentiments, echo=TRUE, eval = TRUE}
top_sentiments_bing <-  word_count %>% 
  filter(sentiment != 'neutral') %>% 
  group_by(sentiment) %>% 
  top_n(10, n) %>% 
  mutate(num = ifelse(sentiment == "negative", -n, n)) %>%  
  select(-n) %>% 
  mutate(word = reorder(word, num)) %>% 
  ungroup() 

top_sentiments_bing
```

Let's break it down line by line:

1. The `word_count` data set has been called. 

2. We use `filter(sentiment != 'neutral')` to filter out any term in the `sentiment` column that is `neutral`. 

3. To get the top 10 of words used in each album use `top_n(10, n)`.

4. We use a logic statement `mutate(num = ifelse(sentiment == "negative", -n, n))`  to tell R that if it sees the term `neutral` in the sentiment column, to change the the count associated with it to a negative in a new column called `num`.

5. Using `select(-n)` tells the program that we want to get rid of the the column `n`. You can think of the negative as a way to deselect a column.

6. We simply put the terms from greatest to least disregarding album by `mutate(word = reorder(word, num))`.

```{r top_sentiments_plot_true, echo=FALSE, eval = TRUE}
ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = FALSE, values = c("#d9534f", "#428bca")) +
  scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7)) + 
  labs(x = '', y = "Number of Occurrences",
       title = 'Top Sentiments of Lyrics',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1),
        panel.spacing = unit(0.5, "cm")) +
  facet_wrap(album ~ ., scales = "free_x")
```

```{r top_sentiments_plot_false, echo=TRUE, eval = FALSE}
ggplot(top_sentiments_bing, aes(reorder(word, num), num, fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = FALSE, values = c("#d9534f", "#428bca")) +
  scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7)) + 
  labs(x = '', y = "Number of Occurrences",
       title = 'Top Sentiments of Lyrics',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 14 , face = "bold"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1),
        panel.spacing = unit(0.5, "cm")) +
   facet_wrap(album ~ ., scales = "free_x")
```


Now there a bunch of things going on in this bar plot too! 

1. Here we are using the `top_sentiments_bing` data set but reordering the way its plotted by greatest to least and filling by positive or negative sentiment by `aes(reorder(word, num), num, fill = sentiment)`.

2. `geom_bar` is a bar graph as we saw before, but here the transparency level is set to 75% by `alpha = 0.75`.

3. The positive and negative colors are set to a specific blue (#d9534f) and red (#428bca), respectively  by `scale_fill_manual(guide = FALSE, values = c("#d9534f", "#428bca"))`. Additionally, the legend has been turned off as well.

4. The y-axis is limited to plotting between -10 and 55 with an even number of breaks given by `scale_y_continuous(limits = c(-10, 55), breaks = pretty_breaks(7))`.

5. `labs` provides the opportunity to name the axes, title and subtitle. 

6. One of the default themes that can be used by `ggplot` is `theme_bw()` or a black and white theme.

7. The values along the x-axis can be aesthetically manipulated to be at a 45 degree angle in size 14 bold print right-adjusted by `axis.text.x = element_text(angle = 45, hjust = 1, size = 14, face = "bold")`.


### Most Common Positive and Negative Words in a WordCloud

To compare the words within each album, we can initially use a comparison wordcloud. Now we can create this using `ggwordcloud` as before, but if you just want a quick look, an older package named `wordcloud` does a pretty good job and it has a built in function for comparisons.

Let's take a look at the album ***American Beauty*** first:
```{r am_beauty_cwc, echo=TRUE, eval = TRUE}
emotions_lyrics_bing %>%
  filter(album == "***American Beauty***") %>%
  filter(sentiment != "neutral") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  spread(sentiment, n, fill = 0L) %>% 
  as.data.frame() %>% 
  remove_rownames() %>% 
  column_to_rownames("word") %>% 
  select(-album) %>%
  comparison.cloud(colors = c("#d9534f", "#428bca"), title.size = 1.5)
```

and then ***Shakedown Street***:
```{r shakedown_st_cwc, echo=TRUE, eval = TRUE}
emotions_lyrics_bing %>%
  filter(album == "***Shakedown Street***") %>%
  filter(sentiment != "neutral") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  spread(sentiment, n, fill = 0L) %>% 
  as.data.frame() %>% 
  remove_rownames() %>% 
  column_to_rownames("word") %>% 
  select(-album) %>%
  comparison.cloud(colors = c("#d9534f", "#428bca"), title.size = 1.5)
```

Without being repetitive, the only command you haven't seen is `spread` which essentially takes a long data set and converts it into a wide one. Both `spread` and its counterpart `gather` are commands that are used often and worth your time getting to know a bit about, though the current incarnations of these known as `pivot_wider` and `pivot_longer`. If interested, take a look at the online version of the text [R for Data Science](https://r4ds.had.co.nz/tidy-data.html?q=pivot#pivoting){target="_blank"}.


### Positive and Negative Words in a Line Graph

Now that weâ€™ve looked at the most common words for either positive or negative sentiment, what proportion of these sentiments are present in within the entire lyrical data set?

```{r albums_pos_neg, echo=TRUE, eval = TRUE}
pos_neg_bing_album <- tidy_lyrics_nsw %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  group_by(album, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>% 
  select(-n) %>% 
  ungroup() 

pos_neg_bing_album
```

```{r albums_pos_neg_plot, echo=TRUE, eval = TRUE}
pos_neg_bing_album %>% 
  filter(sentiment != "neutral") %>% 
  ggplot(aes(x = album, y = percent, color = sentiment, group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = "Album", y = "Emotion Words Count (as %)") +
  scale_color_manual(values = c(positive = "#d9534f", negative = "#428bca")) +
  ggtitle("Proportion of Positive and Negative Words by Album", 
          subtitle = "Bing lexicon") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold"))
```

Well while this isn't overtly interesting, it does provide some indication that while ***Shakedown Street*** is slightly more positive, it is also more negative as well. Maybe we'll get a better idea  by looking at the individual songs:

```{r track_pos_neg, echo=TRUE, eval = TRUE}
pos_neg_bing_track <- tidy_lyrics_nsw %>% 
  filter(!grepl('[0-9]', word)) %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  mutate(sentiment = ifelse(is.na(sentiment), 'neutral', sentiment)) %>%   
  group_by(album, track_title, track_n, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>% 
  select(-n) 

pos_neg_bing_track
```

```{r track_pos_neg_plot, echo=TRUE, eval = TRUE, fig.height= 10, fig.width=12}
pos_neg_bing_track %>% 
  filter(sentiment != "neutral") %>% 
  ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, 
             group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  labs(x = "Album", y = "Sentiment Count (as %)") +
  scale_color_manual(values = c(positive = "#d9534f", negative = "#428bca")) +
  ggtitle("Proportion of Positive and Negative Words by Track", 
          subtitle = "Bing lexicon") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  facet_wrap(. ~ album, ncol = 1, scales = "free_x")
```

Firstly, you may notice that `geom_line`, `geom_point`, and `reorder(track_title, track_n)`. The first two are fairly obvious as they are parts of the visualization that render the dots and lines. In the latter, we arrange the tracks by their original order since many artists tend to itemize tracks in a preferred way. Sometimes this is intended to tell a story^[like Dark Side of the Moon by Pink Floyd or maybe that reference just dated me and you are even more confused.] while other times it is simply a personal or studio preference. 

Visually, the ebbs and flows of the sentiments within each album are relatively consistent and are reflections of each other as one would expect. In fact they both have an instance where the sentiments have a declining slop. The primary differential is the area between the positive and negative curves which is greater in the album ***Shakedown Street*** than ***American Beauty***. In fact, it is this disparity that is the main contribution to the earlier variant of this plot which looked at the albums as a whole.

### Positive and Negative Words in a Boxplot

No we switch our attention to something that may be of greater interest, that is the `NRC` lexicon. Recall this dictionary not only has positive and negative categories, it also has eight different emotional classifications: *Anger*, *Anticipation*, *Disgust*, *Fear*, *Joy*, *Sadness*, *Surprise*, and *Trust*. 

In this case we will disregard the positive and negative tags but please note that what `Bing` classifies by either tag may not necessarily be the same as how **NRC** groups them. In a nutshell, both `Bing` and `NRC` are different lexicons from two different sources so it would make sense that there would be a discrepancy between classifications.

Using a similar approach as before, we first look at the differentials by album: 

```{r album_nrc, echo=TRUE, eval = TRUE}
emotions_album_nrc <- tidy_lyrics_nsw  %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
    group_by(album, sentiment) %>%
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>%   
  select(-n) %>% 
  ungroup() 

emotions_album_nrc 
```

and then the corresponding plot.

```{r album_nrc_plot, echo=TRUE, eval = TRUE, fig.height= 14, fig.width=12}
emotions_album_nrc %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  scale_fill_brewer(palette = "Spectral") +
  ggtitle("Distribution of Sentiments by Album") +
  labs(x = "Sentiment", y = "Percentage") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        legend.position = "none",
        axis.text.x = element_text(size = 11, face = "bold"),
        axis.text.y = element_text(size = 11, face = "bold")) +
  facet_wrap(. ~ album, ncol = 1, scales = "free")
```

The lack of boxes isn't surprising considering we only have one value for each sentiment per album. What is interesting is that the percent of terms associated with anger, anticipation, fear and trust are elevated in ***Shakedown Street***. 

Now a bit of social commentary: These emotions tend to reflect the inner turmoil going on within the band where two of the staple members (Keith and Donna Jean Godchaux) left the band after it was reportedly found that Donna Jean was having an affair with another band member by the name of Bob Weir. Let's see if this is reflected in the songs themselves.

```{r tracks_nrc, echo=TRUE, eval = TRUE}
emotions_tracks_nrc <- tidy_lyrics_nsw  %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(album, track_title, track_n, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>%   
  select(-n) %>% 
  ungroup() 

emotions_tracks_nrc 
```

and then the corresponding plot.

```{r tracks_nrc_plot, echo=TRUE, eval = TRUE}
emotions_tracks_nrc  %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  scale_fill_brewer(palette = "Spectral") +
  ggtitle("Distribution of Sentiments by Album Aggregated by Track") +
  labs(x = "Detected Sentiments", y = "Percentage") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        legend.position = "none",
        axis.text.x = element_text(size = 11, face = "bold"),
        axis.text.y = element_text(size = 11, face = "bold")) +
  facet_wrap(. ~ album, ncol = 1, scales = "free_x")
```

So when we look at the scores by track and then aggregate, we see the the original findings seem to be correct though there is great deal more variability in joy within ***Shakedown Street*** that wasn't detected earlier (maybe it was the disco). Additionally there appears to be a greater association with sadness in ***American Beauty*** that was not found earlier as well. While you may not be familiar with the album, the new findings here are also not a bug surprise. The album is known for its polar track sequencing where an extremely upbeat song (reflected in joy) would be followed by one that is sorrowful (reflected almost equally by sadness).

What if we wanted to see how the songs changed by track order? We can use a bump chart to visualize this

```{r tracks_agg_nrc_bump, echo=TRUE, eval = TRUE, fig.height= 10, fig.width=12}
emotions_tracks_nrc %>% 
  ggplot(aes(reorder(track_title, track_n), percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Sentiments") +
  ggtitle("Sentiments by Albums") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold")) +
  scale_color_brewer(palette = "Spectral") +
  facet_wrap(. ~ album, ncol = 1, scales = "free_x")
```

Well that got messy. Even with the colors, it is hard to discern what is going on. This is a good instance where you can use `facet_grid`. Until now, we have been using `facet_wrap` to separate the `album` variable into its distinct entries. However the functionality of `facet_wrap` is limited and will return a symmetrical matrix of plots for the number of unique levels or factors of a given variable. `facet_grid` serves another purpose by return facets equal to the levels or factors of a given variable. Think of it more this way - 

* `facet_wrap` essentially splits your plot across the categories you want.

*  `facet_grid` does a similar thing but instead of creating different plots it creates different grids and then plots each plot in the grids.

Still confused? Well let's apply it and see.

```{r tracks_ind_nrc_bump, echo=TRUE, eval = TRUE, fig.height= 10, fig.width=12}
emotions_tracks_nrc %>% 
  ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Sentiments") +
  ggtitle("Individual Sentiments by Album") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 8, face = "bold")) +
  scale_color_brewer(palette = "Spectral") +
  facet_grid(sentiment ~ album, scales = "free_x")
```

We only changed `facet_wrap(. ~ album, ncol = 1, scales = "free_x")` in the messy plot to `facet_grid(sentiment ~ album, scales = "free_x")` in the new one. Sure it could still use some work on the aesthetics side of things but the line graphs are now provided by type, or facet which makes a visual comparison much easier.

With a few exceptions, you may notice that the sentiments share a common shape when comparing tracks in order. This consistency is not an outlier. One of the reasons you may enjoy a follow-up album by an artist you liked before is hypothesized to having a similar line of best fit. Don't believe me? Try it yourself and hopefully you remember some basic terms from Algebra.

First let's define a binomial function

```{r binomial, echo=TRUE, eval = TRUE}
binomial_smooth <- function(...) {
  geom_smooth(method = "glm", method.args = list(family = "binomial"), ...)
}
```

and then apply it as a third degree polynomial

```{r tracks_ind_nrc_bump_glm, echo=TRUE, eval = TRUE, warning=FALSE, fig.height= 10, fig.width=12}
emotions_tracks_nrc %>% 
  ggplot(aes(x = reorder(track_title, track_n), y = percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1.5) +
  geom_point(size = 3.5) +
  binomial_smooth(formula = y ~ splines::ns(x, 3), color = "#000000")   +
  scale_y_continuous(breaks = pretty_breaks(5), labels = percent_format()) +
  xlab("Album") + ylab("Proportion of Sentiments") +
  ggtitle("Individual Sentiments by Album") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        axis.text.x = element_text(angle = 45, hjust = 1, size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 8, face = "bold")) +
  scale_color_brewer(palette = "Spectral") +
  facet_grid(sentiment ~ album, scales = "free_x")
```

Change the number `3` in `binomial_smooth(formula = y ~ splines::ns(x, 3), color = "#000000")` to amend the degree as you see fit. In any case, you may notice that the line of best fit is relatively consistent between the two albums when assessing sentiments in track order.

## Viewing Across all Lexicons
Letâ€™s compare the lexicons themselves on how many positive and negative words they each categorize.

```{r bing_lexicon, echo=TRUE, eval = TRUE}
get_sentiments("bing") %>% 
  count(sentiment)
```

```{r nrc_lexicon, echo=TRUE, eval = TRUE}
get_sentiments("nrc") %>% 
  count(sentiment)
```

* In the `bing` lexicon, there are 4781 negative and 2005 positive terms.

* In the `nrc` lexicon, there are 3324 negative and 2312 positive terms.

```{r afinn_lexicon_num, echo=TRUE, eval = TRUE}
get_sentiments("afinn") %>% 
  count(value)
```

* In the `AFINN` lexicon, the measure with regards to the severity of a term being positive or negative, rather than just treating a word as one or the other. However, we can just treat the negative measures as negative sentiments and the positive measures as positive sentiments just to get a general idea of the total number. With that in mind, we can do the following:

```{r afinn_lexicon, echo=TRUE, eval = TRUE}
get_sentiments("afinn") %>% 
  select(value) %>% 
  mutate(sentiment = if_else(value > 0, "positive", "negative", "NA")) %>% 
  group_by(sentiment) %>% 
  summarize(sum = n()) %>%
  filter(sentiment == "positive" | sentiment == "negative")
```

* In the `AFINN` lexicon, there are (generally) 1599 negative and 877 positive terms.

Now taking a look at our data set, let's see how our terms get tagged.

```{r bing_terms, echo=TRUE, eval = TRUE}
emotions_lyrics_bing %>% 
  group_by(sentiment) %>% 
  summarize(sum = n()) %>%  
  filter(sentiment == "positive" | sentiment == "negative")
```

`bing` tags 134 of our terms as negative and 143 as positive.

```{r nrc_terms, echo=TRUE, eval = TRUE}
tidy_lyrics_nsw %>%
  left_join(get_sentiments("nrc"), by = "word") %>% 
  group_by(sentiment) %>% 
  summarize(sum = n()) %>%
  filter(sentiment == "positive" | sentiment == "negative")
```

However, `nrc` tags 140 of our terms as negative and 258 as positive.

```{r afinn_terms, echo=TRUE, eval = TRUE}
emotions_lyrics_afinn <- tidy_lyrics_nsw  %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  filter(!grepl('[0-9]', word))

emotions_lyrics_afinn %>% 
  select(value) %>% 
  mutate(sentiment = if_else(value > 0, "positive", "negative", "NA")) %>% 
  group_by(sentiment) %>% 
  summarize(sum = n()) %>%
  filter(sentiment == "positive" | sentiment == "negative")
```

Finally `AFINN` (generally) tags 113 of our terms as negative and 153 as positive.

So you can see that the sentiments are dependent on  lexicons. Is this a bad thing? Well it depends on what you are using it for. 

Now let's put all of the lexicons together and see how they compare. Note that there is a lot below which will not have a line-by-line explanation. At this point, try reading the code to figure out what is occurring. You may find this to be easier when using albums of artists that you enjoy. In any case, please ask questions if needed!

\newpage
First we calculate **AFINN** sentiment scores and compare them to the list of terms we already have.

```{r all_lexicons_afinn, echo=TRUE, eval = TRUE}
afinn_scores <- emotions_lyrics_afinn %>% 
  replace_na(replace = list(value = 0)) %>%
  group_by(index = album, track_title) %>% 
  summarize(sentiment = sum(value)) %>% 
  mutate(lexicon = "AFINN")
```

We then combine both the `bing` and `nrc` lexicons  into one data frame and calculate the sentiment scores for each.

```{r all_lexicons_bing_nrc, echo=TRUE, eval = TRUE}
bing_nrc_scores <- bind_rows(
  tidy_lyrics_nsw %>% 
    inner_join(get_sentiments("bing")) %>% 
    mutate(lexicon = "Bing"),
  tidy_lyrics %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", "negative"))) %>% 
    mutate(lexicon = "NRC")) %>% 
  # from here we count the sentiments, spread on positive/negative, 
  # then create the final sentiment score:
  count(lexicon, index = album, track_title, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

```

Finally we compile a list of all three lexicons using the `bind_rows` command.

```{r all_lexicons_bind, echo=TRUE, eval = TRUE}
all_lexicons <- bind_rows(afinn_scores, bing_nrc_scores) %>%
  select(-negative, -positive)
```

Try `View(all_lexicons)` if you want to view the resulting data frame. To make the final plot more interesting, let's assign a palette using hex colors.

```{r palette, echo=TRUE, eval = TRUE}
lexicon_cols <- c("AFINN" = "#ae5a41", "NRC" = "#559e83", "Bing" = "#1b85b8")
```

Finally we plot all of the lexicons by album and score.

```{r all_lexicons_plot, echo=TRUE, eval = TRUE}
all_lexicons %>% 
  ggplot(aes(track_title, sentiment, fill = lexicon)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~lexicon, ncol = 1, scales = "free_y") +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle("Comparison of Sentiments", subtitle = "by track order") +
  labs(x = "Index of All Songs", y = "Sentiment Score") +
  theme_bw() +
  theme(plot.title = element_text(hjust=0.5),
        plot.subtitle = element_text(hjust=0.5),
        axis.text.x = element_blank()) +
  facet_grid(lexicon ~ index, scales = "free_x")
```

Well that is a lot to take in but this is essentially one way statistics can be applied! We have taken open text, broken it down into its basic terms, filtered out common terms, and used natural language processing (NLP) including various lexicons to derive sentiments.

This work is registered under the [CC BY-NC-SA 3.0 Creative Commons License](https://creativecommons.org/licenses/by-nc-sa/3.0/us/){target="_blank"}. Any items including, but not limited to lyrics, logos, and references associated with the Grateful Dead are licensed under \textcopyright\ Grateful Dead Productions unless otherwise noted. All rights reserved. 
