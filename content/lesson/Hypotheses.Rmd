---
title: "The t-tests"
linktitle: "Week 10: The t-tests"
output:
  blogdown::html_page:
    toc: true
menu:
  lesson:
    parent: Walkthroughs
    weight: 2
type: docs
weight: 1
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(htmltools)
library(knitr)
library(kableExtra)
```

## Preparation

Download a [script](/scripts/Hypotheses.R){target="_blank"} file of just the R chunks used in this walkthrough.

## Purpose

One of the most common tests in statistics, the *t*-test, is used to determine whether the means of two groups are equal to each other. The assumption for the test is that both groups are sampled from normal distributions with equal variances. The null hypothesis is that the two means are equal, and the alternative is that they are not. It is known that under the null hypothesis, we can calculate a *t*-statistic that will follow a *t*-distribution with $n_1+n_2 - 2$ degrees of freedom. There is also a widely used modification of the *t*-test, known as Welch’s *t*-test that adjusts the number of degrees of freedom when the variances are thought not to be equal to each other. This tutorial covers the basics of performing *t*-tests in `R`.

### Objectives

This guide serves as an introduction to performing *t*-tests to compare two groups. Here's what we'll

* introduce the `midwest` example data set 
* use the `patchwork` package
* associate the *t*-test in R by using the overarching command `t.test()`
* provide some familiarity with the variants of the *t*-test:
  - **One-sample *t*-tests**: Compare the sample mean with a known value, when the variance of the population is unknown
  - **Two-sample *t*-tests**: Compare the means of two groups under the assumption that both samples are random, independent, and normally distributed with unknown but equal variances
  - **Paired *t*-tests**: Compare the means of two sets of paired samples, taken from two populations with unknown variance


### Packages

There is one package we'll be using that you probably do not have: the `patchwork` package which is a tidy update to the base R `grid` and `gridExtra` packages. If you do not have it yet, make sure to install it first using

```{r, eval=FALSE}
install.packages('patchwork')
```

Now please load up the following packages

```{r}
library(tidyverse)
library(patchwork)
```

## The `t.test()` Command

The `t.test()` function can be used to perform both one and two sample *t*-tests on vectors of data. The function contains a variety of arguments and is called as follows:

```{r ttest_definition, eval=FALSE}
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), mu = 0, 
       paired = FALSE, var.equal = FALSE, conf.level = 0.95)
```

Each of the letters - aka arguments - inside of the command are specifically defined:

```{r echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, purl=FALSE}

t.test_opts <- tibble(
  
`Options that you can use in t.test()` = c("`x`",
                                           "`y`",
                                           "`mu`",
                                           "`alternative`",
                                           "`paired`",
                                           "`var.equa`l",
                                           "`conf.level`"), 

`What it is...` = c("a *numeric vector* from a data set",
                    "an optional *numeric vector* from a data set",
                    "a number indicating the true value of the mean",
                    "preference on type of test you wish to run",
                    "preference on whether you wish to perform a paired *t*-test",
                    "indicates whether or not to assume equal variances when performing a two-sample *t*-test", 
                    "the confidence level of the reported confidence interval")

)

```

<center>
```{r message=FALSE, warning=FALSE, eval = TRUE, echo = FALSE, purl=FALSE}
kable(t.test_opts, 
      escape = FALSE,
      align = 'cl') %>%
  kable_styling(full_width = FALSE) %>%
  column_spec(1, width = "20em") %>%
  column_spec(2, width = "30em") 
```
</center>

Below are a few things to note about the `t.test()` command before we move on that can drastically affect your outcomes if you are not aware of them. 

- If the variable `y` is 
  - *excluded*, `t.test()` will run as a **one-sample *t*-test**
  - *included*, `t.test()` will run as a **two-sample *t*-test**
  
- The default `t.test()` command will run as a two-sided *t*-test. However you can perform an alternative hypothesis by changing the `alternative` argument to:
  -  "`greater`", or
  -  "`less`".
For example 

```{r example1, eval=FALSE}
 t.test(x, alternative = "greater", mu = 47)
```

performs a one-sample *t*-test on the data contained in x where the 
  - null hypothesis is $\mu = 47$, and the
  - alternative hypothesis is $\mu > 47$.
  
* The `paired` argument has a default setting of `FALSE` indicating that a paired *t*-test should not be run. If you wish to run a paired *t*-test, this can be done by changing the setting to `TRUE`.

* The `var.equals` argument has a default setting of `FALSE` indicating unequal variances ans applies the Welsch approximation to the degrees of freedom. If you wish to have equal variances, this can be done by changing the setting to `TRUE`.

* The `conf.level` argument is set to 95%, or where $\alpha = 0.05$. The confidence interval is determined by
  - $\mu$ for the one-sample *t*-test, and 
  - $\mu_1-\mu_2$ for the one-sample *t*-test. 

On a final note, the `wilcox.test()` option provides the same basic functionality and arguments, but with the idea that we **do not want to assume the data to follow a normal distribution**. This is often the case for samples but at this time, we will assume that the data that is normally distributed. Understanding the `wilcox.test()` function is beyond the scope of this course.

## It's All About the Assumptions

Before we can move forward in the exploration of the *t*-test, we must first make sure that any data that we use adheres to the test assumptions.

* **Assumption #1: Random sampling**: Data is used from random sampling.
* **Assumption #2: Independent observations**: Observations are independent from one another.
* **Assumption #3: Normality**: Observations are from a normally distributed population.
* **Assumption #4: Homogeneity**: If more than one population is sampled from, then the populations have equal variances (also known as ***homogeneity of variances***)

## Example

### Midwest Data Set from ggplot2

The `ggplot2` package has some test data built in, though you may not find them that exciting which is the reason we tend to use real-world data that is more connected to the social sciences and education. However, in this walkthrough, we will be using one of the `ggplot2` data sets named `midwest`. This data set contains  contains county-level data for 5 states: IL, IN, MI, OH, & WI derived from the 2010 U.S. Census. I'll leave the level of excitement up to you. When you lead up `tidyverse`, or more specifically `ggplot2`, the `midwest` data sets is also loaded automatically. Let's take a look at it:

```{r}
head(midwest)
```

### One-sample *t*-test

The one-sample *t*-test compares a sample’s mean with a known value, when the variance of the population is unknown. Consider we want to assess the percent of college educated adults in the midwest and compare it to a certain value. For example, let’s assume the nation-wide average of college educated adults is approximately [37% (Bachelor’s degree or higher)]("https://www.census.gov/data/tables/2018/demo/education-attainment/cps-detailed-tables.html") and we want to see if the midwest mean is significantly different than the national average; in particular we want to test if the midwest average is less than the national average.

Let's first take a look at how the data looks (important!) and some descriptive statistics:

```{r}
head(midwest$percollege, 10)

summary(midwest$percollege)
```

and then plot it to see the distribution:

```{r}
p1 <- ggplot(midwest, aes(x = percollege)) + 
        geom_histogram(aes(fill = ..count..), color = "#ccccaa") +
        scale_fill_gradient("Frequency", low = "#5cb85c", high = "#428bca") +
        theme_minimal() +
        theme(legend.position = "bottom",
              legend.direction = "horizontal")



p2 <- ggplot(midwest, aes(x = percollege)) + 
        geom_histogram(aes(fill = ..count..), color = "#ccccaa") +
        scale_fill_gradient("Frequency", low = "#5cb85c", high = "#428bca") +
        scale_x_log10() +
        theme_minimal() +
        theme(legend.position = "bottom",
              legend.direction = "horizontal")


p1 + p2
```


To test if the midwest average is less than the national average, we'll perform three tests. First we test with a normal *t*-test without any distribution transformations. The results below show a *p*-value < .001 supporting the alternative hypothesis that “the true mean is less than 37%.”

```{r}
t.test(midwest$percollege, mu = 37, alternative = "less")
```

Alternatively, due to the non-normality concerns we can perform this test in another way to ensure our results are not being biased due to assumption violations. We can perform the *t*-test and transform our data. The results support our initial conclusion that the percent of college educated adults in the midwest is statistically less than the nationwide average.

```{r}
t.test(log(midwest$percollege), mu = log(37), alternative = "less")
```


### Two-sample *t*-test

Now let’s say we want to compare the differences between the average percent of college educated adults in Ohio versus Michigan. Here, we want to perform a two-sample *t*-test. So let's first do some data wrangling...

```{r}
ohio_mi <- midwest %>%
        filter(state == "OH" | state == "MI") %>%
        select(state, percollege)
```

...and find some descriptive statistics

* First for Ohio:

```{r}
# Ohio summary stats
summary(ohio_mi %>% 
          filter(state == "OH") %>% 
          .$percollege)
```

* and then for Michigan:

```{r}
# Ohio summary stats
summary(ohio_mi %>% 
          filter(state == "MI") %>% 
          .$percollege)
```

We can see Ohio appears to have slightly less college educated adults than Michigan but the graphic doesn’t tell us if it is statistically significant or not.

```{r}
ggplot(ohio_mi, aes(x = state, y = percollege, fill = state)) +
        geom_boxplot(alpha = 0.5) +
        scale_fill_manual(values = c("#00274C", "#BB0000")) +
        theme_minimal() 
```

We can also once again see similar skewness within the sample distributions by running the following.

```{r}
p3 <- ggplot(ohio_mi, aes(x = percollege)) +
        geom_histogram(aes(fill = ..count..), color = "#ccccaa") +
        scale_fill_gradient("Frequency", low = "#5cb85c", high = "#428bca") +
        facet_wrap(~ state) +
        theme_minimal() +
        theme(legend.position = "bottom",
              legend.direction = "horizontal")
    

p4 <- ggplot(ohio_mi, aes(x = percollege)) +
        geom_histogram(aes(fill = ..count..), color = "#ccccaa") +
        scale_fill_gradient("Frequency", low = "#5cb85c", high = "#428bca") +
        facet_wrap(~ state) + 
        scale_x_log10() +
        theme_minimal() +
        theme(legend.position = "bottom",
              legend.direction = "horizontal")

p3 + p4
```

Similar to the plots in the one-sample example, to test if the Ohio and Michigan averages differ we'll perform two tests. Also, note that we am searching for any differences between the means rather than if one is specifically less than or greater than the other. First we test with a normal *t*-test without any distribution transformations. The results below show a *p*-value < 0.01 supporting the alternative hypothesis that “true difference in means is not equal to 0”; essentially it states there is a statistical difference between the two means.

```{r}
t.test(percollege ~ state, data = ohio_mi)
```

Again as an alternative, due to the non-normality concerns we can perform this test in another way to ensure our results are not being biased due to assumption violations. We can perform the *t*-test and transform our data. The results support our initial conclusion that the percent of college educated adults in Ohio is statistically different than the percent in Michigan.

```{r}
t.test(log(percollege) ~ state, data = ohio_mi)
```

### The Paired *t*-test

To illustrate the paired *t*-test we'll use the built-in `sleep` data set.

Please load up the following packages

```{r}
sleep
```

```{r}
ggplot(sleep, aes(group, extra, fill = group)) +
        geom_boxplot(alpha = 0.5) +
        scale_fill_manual(values = c("#00274C", "#BB0000")) +
        theme_minimal() 
```

In this case we are assessing if there is a statistically significant effect of a particular drug on sleep (increase in hours of sleep compared to control) for 10 patients. Please see `?sleep` for more details on the variables. 

We want to see if the mean values for the extra variable differs between group 1 and group 2. Here, we perform the `t.test` as in the previous sections but just add the `paired = TRUE` argument:

```{r}
t.test(extra ~ group, data = sleep, paired = TRUE)
```

In this example it appears that the drug does have an effect as the $p$-value = 0.0028 suggesting that the drug increases sleep on average by 1.58 hours.

The above looks at exact matching. If you are getting an error, try running which will only run on values that can be matched. To use the command below, make sure both comparison variables are numeric.


```{r} 
sleep <- sleep %>%
  mutate(group = as.numeric(group))

t.test(sleep$extra, sleep$group, paired = TRUE)
```
